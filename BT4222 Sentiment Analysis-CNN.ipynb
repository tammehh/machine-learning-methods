{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19yu1LkeRdfxQOIZnoskp994PjEH9CRQz","timestamp":1691994814059}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Note:\n","- This notebook file may contain methods or algorithms that are NOT covered by the teaching content of BT4222 and hence will not be assessed in your midterm exam.\n","- It serves to increase your exposure in depth and breath to the practical methods in addressing the specific project topic. We believe it will be helpful for your current project and also your future internship endeavors."],"metadata":{"id":"JYzbB3x3lnwB"}},{"cell_type":"markdown","source":["# **Import Library**"],"metadata":{"id":"UXYWxqcLTrsT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"STdFlfuXL6Xz"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR"]},{"cell_type":"markdown","source":[],"metadata":{"id":"2UDlABr3mnEz"}},{"cell_type":"markdown","source":["# **Define Network Structure**\n","Here, I use a 2-layer CNN for classification, the network structure is as followed:\n","\n","```\n","conv1.weight     torch.Size([32, 1, 3])\n","conv1.bias       torch.Size([32])\n","Bn1.weight       torch.Size([32])\n","Bn1.bias         torch.Size([32])\n","Bn1.running_mean         torch.Size([32])\n","Bn1.running_var          torch.Size([32])\n","Bn1.num_batches_tracked          torch.Size([])\n","conv2.weight     torch.Size([32, 32, 3])\n","conv2.bias       torch.Size([32])\n","Bn2.weight       torch.Size([32])\n","Bn2.bias         torch.Size([32])\n","Bn2.running_mean         torch.Size([32])\n","Bn2.running_var          torch.Size([32])\n","Bn2.num_batches_tracked          torch.Size([])\n","fc1.weight       torch.Size([5, 416])\n","c1.bias         torch.Size([5])\n","```\n","\n","self.conv1 = nn.Conv1d(1, 32, 3, 1, 1, bias=True): The first layer is a 1D convolutional layer that takes an input with 1 channel and outputs 32 channels. The kernel size is 3, stride is 1 and padding is 1. If the input size to this layer is (batch_size, 1, L), the output size would be (batch_size, 32, L), given the padding is set to 1 to preserve the length of data. (L is 50 and batch_size is 640 here)\n","\n","self.Bn1 = nn.BatchNorm1d(32): The next layer is a 1D batch normalization layer. Batch normalization helps to stabilize the learning process and reduces the number of training steps required. It achieves this by normalizing the output of the previous layer.\n","\n","self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2): This is an average pooling layer. Pooling is used to reduce the spatial dimensions of the data while preserving the most important features. It applies a 1D sliding window of size 2 (the kernel size) over the input, taking strides of 2, and returns the average value in each window.\n","\n","self.fc1 = nn.Linear(32\\*12, 5, bias=True): This is a fully connected (linear) layer that takes a 1D tensor with a length of 32*12 (the number of channels from the previous layer multiplied by the length of the data after two pooling operations) and outputs a tensor of length 5.\n","\n","x = torch.flatten(x, 1): Before the output is passed to the fully connected layer, it needs to be flattened from a 3D tensor to a 2D tensor. This is because a fully connected layer expects inputs to be of shape (batch_size, num_features). Here, the flatten operation takes every 1D tensor in the batch (all dimensions except the first dimension), and flattens it into a single dimension.\n","\n"],"metadata":{"id":"-98wEEiETkWO"}},{"cell_type":"code","source":["class Net(nn.Module):  # Defines a new neural network architecture as a class that inherits from the PyTorch base class nn.Module.\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv1d(1, 32, 3, 1,1, bias=True)\n","        # Define the first 1D convolution layer. Takes 1 input channel, outputs 32 channels, kernel size is 3, stride is 1, padding is 1.\n","        self.Bn1 = nn.BatchNorm1d(32)\n","        # Apply Batch Normalization to the output of the first convolutional layer.\n","        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n","        # Apply 1D Average Pooling after the first Batch Normalization. The kernel size and stride are 2.\n","\n","        self.conv2 = nn.Conv1d(32, 32, 3, 1,1, bias=True)\n","        self.Bn2 = nn.BatchNorm1d(32)\n","        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n","\n","        self.fc1 = nn.Linear(32*12, 5, bias=True)\n","        # Define a linear layer (fully connected layer). It takes 32*12 inputs and outputs 5 nodes.\n","\n","\n","    def forward(self, x):\n","        x = F.relu(self.Bn1(self.conv1(x)))\n","        # Pass the input through the first convolutional layer, then Batch Normalization, and then apply ReLU activation.\n","        x = self.pool1(x)\n","        # Apply Average Pooling to the output of the previous step.\n","        x = F.relu(self.Bn2(self.conv2(x)))\n","        x = self.pool2(x)\n","        x = torch.flatten(x, 1)\n","        # Flatten the output from the previous step. This is necessary because fully connected layers expect a 1D input.\n","        x = self.fc1(x)\n","        # Pass the flattened output through the fully connected layer. This is the output of the network.\n","        return x"],"metadata":{"id":"VcaxvGyjMLef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Training and Testing**"],"metadata":{"id":"y2tLMOBDYdT7"}},{"cell_type":"code","source":["def train(args, model, device, train_loader, optimizer, epoch):\n","    model.train()  # Set the model to training mode\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):  # Loop over each batch from the training set\n","        data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n","\n","        target = target-1  # Adjust the target values (Moving 1-5 to 0-4  for easy training)\n","        target = target.long()  # Make sure that target data is long type (necessary for loss function)\n","\n","        optimizer.zero_grad()  # Clear gradients from the previous training step\n","        output = model(data)  # Run forward pass (model predictions)\n","\n","        loss = F.cross_entropy(output, target)  # Calculate the loss between the output and target\n","        loss.backward()  # Perform backpropagation (calculate gradients of loss w.r.t. parameters)\n","        optimizer.step()  # Update the model parameters\n","\n","        if batch_idx % args.log_interval == 0:  # Print log info for specified interval\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.item()))\n","\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()  # Set the model to evaluation mode\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n","        for data, target in test_loader:  # Loop over each batch from the testing set\n","            data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n","            target = target-1  # Adjust the target values\n","            output = model(data)  # Run forward pass (model predictions)\n","            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability as the predicted output\n","            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n","\n","    test_loss /= len(test_loader.dataset)  # Calculate the average loss\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n","    return correct  # Return the number of correctly classified samples\n"],"metadata":{"id":"KRYHsopyMYYU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Hyperparameter**\n","\n","1. epochs: The number of times the entire dataset is passed forward and backward through the neural network.\n","\n","2. lr: Learning rate, which determines the step size at each iteration while moving towards a minimum in the loss function.\n","\n","3. use_cuda: A boolean flag indicating whether to use CUDA (NVIDIA's parallel computing platform and API) for computations. This would be set to True if you want to utilize GPU acceleration.\n","\n","4. gamma: Typically used in learning rate scheduling. It's a factor by which the learning rate is reduced at certain intervals or when certain conditions are met.\n","\n","5. log_interval: The interval in terms of batches during training.\n","\n","6. seed: A seed value for random number generators to ensure reproducibility of results.\n","\n","For simple networks and small datasets, we typically set the learning rate to 1, the number of epochs to 10 and gamma to 0.7 for model training."],"metadata":{"id":"vafhcsv9Yiql"}},{"cell_type":"code","source":["class Args:\n","  epochs = 10\n","  lr = 1.0\n","  use_cuda=False\n","  gamma = 0.7\n","  log_interval = 10\n","  seed = 1\n","\n","args = Args()"],"metadata":{"id":"-wlKYHWEMdwi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Load Data**"],"metadata":{"id":"HpgnI9XQR8x5"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","import gdown\n","\n","file_id = '1CCIfElCaURQbuYvHZiL445UQIRzmmuM7'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'train_vectors.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1bwkg7XdmH6Mkp_tkAakCbxJMWNAXJU43'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'train_labels.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1fprUkqC9Qb-y1eDRZt0gA4-4gS941TUo'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'test_vectors.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1VwOqpW7DZPhqAGDrreVwhtzCB2lUc_LD'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'test_labels.pt'\n","gdown.download(url, output, quiet=False)\n","\n","#!wget --no-check-certificate 'https://drive.google.com/file/d/1CCIfElCaURQbuYvHZiL445UQIRzmmuM7/view?usp=share_link' -O train_vectors.pt\n","#!wget --no-check-certificate 'https://drive.google.com/file/d/1bwkg7XdmH6Mkp_tkAakCbxJMWNAXJU43/view?usp=share_link' -O train_labels.pt\n","#!wget --no-check-certificate 'https://drive.google.com/file/d/1fprUkqC9Qb-y1eDRZt0gA4-4gS941TUo/view?usp=sharing' -O test_vectors.pt\n","#!wget --no-check-certificate 'https://drive.google.com/file/d/1VwOqpW7DZPhqAGDrreVwhtzCB2lUc_LD/view?usp=sharing' -O test_labels.pt\n","#!ls\n","train_vectors = torch.load('train_vectors.pt')\n","train_labels = torch.load('train_labels.pt')\n","test_vectors = torch.load('test_vectors.pt')\n","test_labels = torch.load('test_labels.pt')\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"niXJRCXUR9Uh","executionInfo":{"status":"ok","timestamp":1691928798966,"user_tz":-480,"elapsed":8710,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}},"outputId":"909188f4-b964-486e-99d5-1ed481dc92d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1CCIfElCaURQbuYvHZiL445UQIRzmmuM7\n","To: /content/train_vectors.pt\n","100%|██████████| 80.0M/80.0M [00:00<00:00, 129MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1bwkg7XdmH6Mkp_tkAakCbxJMWNAXJU43\n","To: /content/train_labels.pt\n","100%|██████████| 3.20M/3.20M [00:00<00:00, 60.9MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1fprUkqC9Qb-y1eDRZt0gA4-4gS941TUo\n","To: /content/test_vectors.pt\n","100%|██████████| 20.0M/20.0M [00:00<00:00, 160MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1VwOqpW7DZPhqAGDrreVwhtzCB2lUc_LD\n","To: /content/test_labels.pt\n","100%|██████████| 801k/801k [00:00<00:00, 23.8MB/s]\n"]}]},{"cell_type":"markdown","source":["# **Start training and testing**"],"metadata":{"id":"x_jwrzWbbENR"}},{"cell_type":"code","source":["torch.manual_seed(args.seed)\n","\n","device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n","model = Net().to(device)\n","\n","for param_tensor in model.state_dict():\n","        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n","\n","#Form training and testing dataset\n","optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n","\n","train_dataset = torch.utils.data.TensorDataset(train_vectors, train_labels)\n","test_dataset = torch.utils.data.TensorDataset(test_vectors, test_labels)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=640, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=640, shuffle=False)\n","scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n","\n","#Model training\n","ACC = 0\n","for epoch in range(1, args.epochs + 1):\n","    train(args, model, device, train_loader, optimizer, epoch)\n","    ACC_ = test(model, device, test_loader)\n","    if ACC_>ACC or ACC_ == ACC:\n","        ACC = ACC_\n","        torch.save(model.state_dict(), \"Baseline_CNN.pt\")\n","\n","    scheduler.step()\n","\n","print(ACC)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPYD-LvcMi6w","executionInfo":{"status":"ok","timestamp":1691929091390,"user_tz":-480,"elapsed":277756,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}},"outputId":"6c624bc3-5b4c-44f7-eaed-9da47ef45ebb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["conv1.weight \t torch.Size([32, 1, 3])\n","conv1.bias \t torch.Size([32])\n","Bn1.weight \t torch.Size([32])\n","Bn1.bias \t torch.Size([32])\n","Bn1.running_mean \t torch.Size([32])\n","Bn1.running_var \t torch.Size([32])\n","Bn1.num_batches_tracked \t torch.Size([])\n","conv2.weight \t torch.Size([32, 32, 3])\n","conv2.bias \t torch.Size([32])\n","Bn2.weight \t torch.Size([32])\n","Bn2.bias \t torch.Size([32])\n","Bn2.running_mean \t torch.Size([32])\n","Bn2.running_var \t torch.Size([32])\n","Bn2.num_batches_tracked \t torch.Size([])\n","fc1.weight \t torch.Size([5, 384])\n","fc1.bias \t torch.Size([5])\n","Train Epoch: 1 [0/400000 (0%)]\tLoss: 1.664010\n","Train Epoch: 1 [6400/400000 (2%)]\tLoss: 1.361696\n","Train Epoch: 1 [12800/400000 (3%)]\tLoss: 1.372544\n","Train Epoch: 1 [19200/400000 (5%)]\tLoss: 1.182817\n","Train Epoch: 1 [25600/400000 (6%)]\tLoss: 1.171335\n","Train Epoch: 1 [32000/400000 (8%)]\tLoss: 1.146980\n","Train Epoch: 1 [38400/400000 (10%)]\tLoss: 1.131668\n","Train Epoch: 1 [44800/400000 (11%)]\tLoss: 1.133337\n","Train Epoch: 1 [51200/400000 (13%)]\tLoss: 1.065842\n","Train Epoch: 1 [57600/400000 (14%)]\tLoss: 1.069619\n","Train Epoch: 1 [64000/400000 (16%)]\tLoss: 1.154215\n","Train Epoch: 1 [70400/400000 (18%)]\tLoss: 1.099082\n","Train Epoch: 1 [76800/400000 (19%)]\tLoss: 1.086073\n","Train Epoch: 1 [83200/400000 (21%)]\tLoss: 1.116779\n","Train Epoch: 1 [89600/400000 (22%)]\tLoss: 1.153953\n","Train Epoch: 1 [96000/400000 (24%)]\tLoss: 1.124683\n","Train Epoch: 1 [102400/400000 (26%)]\tLoss: 1.070978\n","Train Epoch: 1 [108800/400000 (27%)]\tLoss: 1.101784\n","Train Epoch: 1 [115200/400000 (29%)]\tLoss: 1.090420\n","Train Epoch: 1 [121600/400000 (30%)]\tLoss: 1.066588\n","Train Epoch: 1 [128000/400000 (32%)]\tLoss: 1.128659\n","Train Epoch: 1 [134400/400000 (34%)]\tLoss: 1.074670\n","Train Epoch: 1 [140800/400000 (35%)]\tLoss: 1.191886\n","Train Epoch: 1 [147200/400000 (37%)]\tLoss: 1.105590\n","Train Epoch: 1 [153600/400000 (38%)]\tLoss: 1.031272\n","Train Epoch: 1 [160000/400000 (40%)]\tLoss: 1.094434\n","Train Epoch: 1 [166400/400000 (42%)]\tLoss: 1.092682\n","Train Epoch: 1 [172800/400000 (43%)]\tLoss: 1.066228\n","Train Epoch: 1 [179200/400000 (45%)]\tLoss: 1.081735\n","Train Epoch: 1 [185600/400000 (46%)]\tLoss: 1.056796\n","Train Epoch: 1 [192000/400000 (48%)]\tLoss: 1.025703\n","Train Epoch: 1 [198400/400000 (50%)]\tLoss: 1.087441\n","Train Epoch: 1 [204800/400000 (51%)]\tLoss: 1.055069\n","Train Epoch: 1 [211200/400000 (53%)]\tLoss: 1.031539\n","Train Epoch: 1 [217600/400000 (54%)]\tLoss: 1.042466\n","Train Epoch: 1 [224000/400000 (56%)]\tLoss: 1.057386\n","Train Epoch: 1 [230400/400000 (58%)]\tLoss: 1.062252\n","Train Epoch: 1 [236800/400000 (59%)]\tLoss: 1.046017\n","Train Epoch: 1 [243200/400000 (61%)]\tLoss: 1.059823\n","Train Epoch: 1 [249600/400000 (62%)]\tLoss: 1.070259\n","Train Epoch: 1 [256000/400000 (64%)]\tLoss: 1.015115\n","Train Epoch: 1 [262400/400000 (66%)]\tLoss: 1.067074\n","Train Epoch: 1 [268800/400000 (67%)]\tLoss: 1.061929\n","Train Epoch: 1 [275200/400000 (69%)]\tLoss: 1.035679\n","Train Epoch: 1 [281600/400000 (70%)]\tLoss: 1.060150\n","Train Epoch: 1 [288000/400000 (72%)]\tLoss: 1.078358\n","Train Epoch: 1 [294400/400000 (74%)]\tLoss: 1.071715\n","Train Epoch: 1 [300800/400000 (75%)]\tLoss: 1.085500\n","Train Epoch: 1 [307200/400000 (77%)]\tLoss: 0.988889\n","Train Epoch: 1 [313600/400000 (78%)]\tLoss: 1.028391\n","Train Epoch: 1 [320000/400000 (80%)]\tLoss: 1.006587\n","Train Epoch: 1 [326400/400000 (82%)]\tLoss: 1.055663\n","Train Epoch: 1 [332800/400000 (83%)]\tLoss: 1.085371\n","Train Epoch: 1 [339200/400000 (85%)]\tLoss: 1.015361\n","Train Epoch: 1 [345600/400000 (86%)]\tLoss: 1.073710\n","Train Epoch: 1 [352000/400000 (88%)]\tLoss: 1.026689\n","Train Epoch: 1 [358400/400000 (90%)]\tLoss: 1.025512\n","Train Epoch: 1 [364800/400000 (91%)]\tLoss: 1.072967\n","Train Epoch: 1 [371200/400000 (93%)]\tLoss: 1.022684\n","Train Epoch: 1 [377600/400000 (94%)]\tLoss: 0.981438\n","Train Epoch: 1 [384000/400000 (96%)]\tLoss: 1.028285\n","Train Epoch: 1 [390400/400000 (98%)]\tLoss: 1.063414\n","Train Epoch: 1 [396800/400000 (99%)]\tLoss: 1.069363\n","\n","Test set: Average loss: 0.0000, Accuracy: 53967/100000 (54%)\n","\n","Train Epoch: 2 [0/400000 (0%)]\tLoss: 1.007697\n","Train Epoch: 2 [6400/400000 (2%)]\tLoss: 1.006799\n","Train Epoch: 2 [12800/400000 (3%)]\tLoss: 1.043565\n","Train Epoch: 2 [19200/400000 (5%)]\tLoss: 1.023236\n","Train Epoch: 2 [25600/400000 (6%)]\tLoss: 1.000306\n","Train Epoch: 2 [32000/400000 (8%)]\tLoss: 0.980473\n","Train Epoch: 2 [38400/400000 (10%)]\tLoss: 1.006366\n","Train Epoch: 2 [44800/400000 (11%)]\tLoss: 1.047564\n","Train Epoch: 2 [51200/400000 (13%)]\tLoss: 1.061089\n","Train Epoch: 2 [57600/400000 (14%)]\tLoss: 1.030249\n","Train Epoch: 2 [64000/400000 (16%)]\tLoss: 1.077757\n","Train Epoch: 2 [70400/400000 (18%)]\tLoss: 1.034251\n","Train Epoch: 2 [76800/400000 (19%)]\tLoss: 1.023355\n","Train Epoch: 2 [83200/400000 (21%)]\tLoss: 1.052612\n","Train Epoch: 2 [89600/400000 (22%)]\tLoss: 1.082825\n","Train Epoch: 2 [96000/400000 (24%)]\tLoss: 1.070276\n","Train Epoch: 2 [102400/400000 (26%)]\tLoss: 1.031401\n","Train Epoch: 2 [108800/400000 (27%)]\tLoss: 1.026395\n","Train Epoch: 2 [115200/400000 (29%)]\tLoss: 1.102723\n","Train Epoch: 2 [121600/400000 (30%)]\tLoss: 1.088477\n","Train Epoch: 2 [128000/400000 (32%)]\tLoss: 1.061442\n","Train Epoch: 2 [134400/400000 (34%)]\tLoss: 1.049543\n","Train Epoch: 2 [140800/400000 (35%)]\tLoss: 0.996724\n","Train Epoch: 2 [147200/400000 (37%)]\tLoss: 1.039309\n","Train Epoch: 2 [153600/400000 (38%)]\tLoss: 1.035155\n","Train Epoch: 2 [160000/400000 (40%)]\tLoss: 1.038830\n","Train Epoch: 2 [166400/400000 (42%)]\tLoss: 0.985101\n","Train Epoch: 2 [172800/400000 (43%)]\tLoss: 0.971066\n","Train Epoch: 2 [179200/400000 (45%)]\tLoss: 0.996067\n","Train Epoch: 2 [185600/400000 (46%)]\tLoss: 1.046885\n","Train Epoch: 2 [192000/400000 (48%)]\tLoss: 1.058209\n","Train Epoch: 2 [198400/400000 (50%)]\tLoss: 1.001943\n","Train Epoch: 2 [204800/400000 (51%)]\tLoss: 1.020472\n","Train Epoch: 2 [211200/400000 (53%)]\tLoss: 1.033522\n","Train Epoch: 2 [217600/400000 (54%)]\tLoss: 1.051482\n","Train Epoch: 2 [224000/400000 (56%)]\tLoss: 1.104412\n","Train Epoch: 2 [230400/400000 (58%)]\tLoss: 1.054100\n","Train Epoch: 2 [236800/400000 (59%)]\tLoss: 1.081797\n","Train Epoch: 2 [243200/400000 (61%)]\tLoss: 1.026042\n","Train Epoch: 2 [249600/400000 (62%)]\tLoss: 1.007011\n","Train Epoch: 2 [256000/400000 (64%)]\tLoss: 1.047885\n","Train Epoch: 2 [262400/400000 (66%)]\tLoss: 1.017333\n","Train Epoch: 2 [268800/400000 (67%)]\tLoss: 1.023278\n","Train Epoch: 2 [275200/400000 (69%)]\tLoss: 0.997989\n","Train Epoch: 2 [281600/400000 (70%)]\tLoss: 1.040092\n","Train Epoch: 2 [288000/400000 (72%)]\tLoss: 0.996072\n","Train Epoch: 2 [294400/400000 (74%)]\tLoss: 0.979222\n","Train Epoch: 2 [300800/400000 (75%)]\tLoss: 1.020261\n","Train Epoch: 2 [307200/400000 (77%)]\tLoss: 1.011829\n","Train Epoch: 2 [313600/400000 (78%)]\tLoss: 1.050594\n","Train Epoch: 2 [320000/400000 (80%)]\tLoss: 1.043131\n","Train Epoch: 2 [326400/400000 (82%)]\tLoss: 0.974316\n","Train Epoch: 2 [332800/400000 (83%)]\tLoss: 1.111611\n","Train Epoch: 2 [339200/400000 (85%)]\tLoss: 1.051583\n","Train Epoch: 2 [345600/400000 (86%)]\tLoss: 1.023272\n","Train Epoch: 2 [352000/400000 (88%)]\tLoss: 1.026669\n","Train Epoch: 2 [358400/400000 (90%)]\tLoss: 1.013373\n","Train Epoch: 2 [364800/400000 (91%)]\tLoss: 0.983844\n","Train Epoch: 2 [371200/400000 (93%)]\tLoss: 1.020993\n","Train Epoch: 2 [377600/400000 (94%)]\tLoss: 1.053512\n","Train Epoch: 2 [384000/400000 (96%)]\tLoss: 1.005327\n","Train Epoch: 2 [390400/400000 (98%)]\tLoss: 1.021291\n","Train Epoch: 2 [396800/400000 (99%)]\tLoss: 1.019805\n","\n","Test set: Average loss: 0.0000, Accuracy: 54738/100000 (55%)\n","\n","Train Epoch: 3 [0/400000 (0%)]\tLoss: 0.998029\n","Train Epoch: 3 [6400/400000 (2%)]\tLoss: 1.025927\n","Train Epoch: 3 [12800/400000 (3%)]\tLoss: 1.024191\n","Train Epoch: 3 [19200/400000 (5%)]\tLoss: 1.074172\n","Train Epoch: 3 [25600/400000 (6%)]\tLoss: 1.004358\n","Train Epoch: 3 [32000/400000 (8%)]\tLoss: 1.025081\n","Train Epoch: 3 [38400/400000 (10%)]\tLoss: 1.019659\n","Train Epoch: 3 [44800/400000 (11%)]\tLoss: 1.061776\n","Train Epoch: 3 [51200/400000 (13%)]\tLoss: 1.030373\n","Train Epoch: 3 [57600/400000 (14%)]\tLoss: 1.054349\n","Train Epoch: 3 [64000/400000 (16%)]\tLoss: 0.987467\n","Train Epoch: 3 [70400/400000 (18%)]\tLoss: 1.019876\n","Train Epoch: 3 [76800/400000 (19%)]\tLoss: 1.076861\n","Train Epoch: 3 [83200/400000 (21%)]\tLoss: 1.036916\n","Train Epoch: 3 [89600/400000 (22%)]\tLoss: 1.044866\n","Train Epoch: 3 [96000/400000 (24%)]\tLoss: 1.045206\n","Train Epoch: 3 [102400/400000 (26%)]\tLoss: 1.052068\n","Train Epoch: 3 [108800/400000 (27%)]\tLoss: 0.990436\n","Train Epoch: 3 [115200/400000 (29%)]\tLoss: 1.026162\n","Train Epoch: 3 [121600/400000 (30%)]\tLoss: 0.999593\n","Train Epoch: 3 [128000/400000 (32%)]\tLoss: 1.032002\n","Train Epoch: 3 [134400/400000 (34%)]\tLoss: 0.995697\n","Train Epoch: 3 [140800/400000 (35%)]\tLoss: 1.059823\n","Train Epoch: 3 [147200/400000 (37%)]\tLoss: 1.001422\n","Train Epoch: 3 [153600/400000 (38%)]\tLoss: 0.961485\n","Train Epoch: 3 [160000/400000 (40%)]\tLoss: 1.000808\n","Train Epoch: 3 [166400/400000 (42%)]\tLoss: 1.003194\n","Train Epoch: 3 [172800/400000 (43%)]\tLoss: 0.952662\n","Train Epoch: 3 [179200/400000 (45%)]\tLoss: 1.008498\n","Train Epoch: 3 [185600/400000 (46%)]\tLoss: 1.004012\n","Train Epoch: 3 [192000/400000 (48%)]\tLoss: 0.995604\n","Train Epoch: 3 [198400/400000 (50%)]\tLoss: 0.992597\n","Train Epoch: 3 [204800/400000 (51%)]\tLoss: 1.028629\n","Train Epoch: 3 [211200/400000 (53%)]\tLoss: 0.976178\n","Train Epoch: 3 [217600/400000 (54%)]\tLoss: 1.004261\n","Train Epoch: 3 [224000/400000 (56%)]\tLoss: 1.046100\n","Train Epoch: 3 [230400/400000 (58%)]\tLoss: 1.008973\n","Train Epoch: 3 [236800/400000 (59%)]\tLoss: 1.012865\n","Train Epoch: 3 [243200/400000 (61%)]\tLoss: 1.027388\n","Train Epoch: 3 [249600/400000 (62%)]\tLoss: 1.027977\n","Train Epoch: 3 [256000/400000 (64%)]\tLoss: 1.040664\n","Train Epoch: 3 [262400/400000 (66%)]\tLoss: 1.032685\n","Train Epoch: 3 [268800/400000 (67%)]\tLoss: 0.974893\n","Train Epoch: 3 [275200/400000 (69%)]\tLoss: 1.009292\n","Train Epoch: 3 [281600/400000 (70%)]\tLoss: 1.004775\n","Train Epoch: 3 [288000/400000 (72%)]\tLoss: 1.019139\n","Train Epoch: 3 [294400/400000 (74%)]\tLoss: 1.029786\n","Train Epoch: 3 [300800/400000 (75%)]\tLoss: 1.003985\n","Train Epoch: 3 [307200/400000 (77%)]\tLoss: 1.051087\n","Train Epoch: 3 [313600/400000 (78%)]\tLoss: 1.022952\n","Train Epoch: 3 [320000/400000 (80%)]\tLoss: 1.012957\n","Train Epoch: 3 [326400/400000 (82%)]\tLoss: 1.006618\n","Train Epoch: 3 [332800/400000 (83%)]\tLoss: 0.991000\n","Train Epoch: 3 [339200/400000 (85%)]\tLoss: 1.016568\n","Train Epoch: 3 [345600/400000 (86%)]\tLoss: 0.993394\n","Train Epoch: 3 [352000/400000 (88%)]\tLoss: 0.981593\n","Train Epoch: 3 [358400/400000 (90%)]\tLoss: 1.010479\n","Train Epoch: 3 [364800/400000 (91%)]\tLoss: 1.023328\n","Train Epoch: 3 [371200/400000 (93%)]\tLoss: 1.028007\n","Train Epoch: 3 [377600/400000 (94%)]\tLoss: 1.032053\n","Train Epoch: 3 [384000/400000 (96%)]\tLoss: 1.024071\n","Train Epoch: 3 [390400/400000 (98%)]\tLoss: 0.996975\n","Train Epoch: 3 [396800/400000 (99%)]\tLoss: 1.021693\n","\n","Test set: Average loss: 0.0000, Accuracy: 55216/100000 (55%)\n","\n","Train Epoch: 4 [0/400000 (0%)]\tLoss: 1.043672\n","Train Epoch: 4 [6400/400000 (2%)]\tLoss: 1.008781\n","Train Epoch: 4 [12800/400000 (3%)]\tLoss: 0.976860\n","Train Epoch: 4 [19200/400000 (5%)]\tLoss: 0.977696\n","Train Epoch: 4 [25600/400000 (6%)]\tLoss: 0.977715\n","Train Epoch: 4 [32000/400000 (8%)]\tLoss: 1.044185\n","Train Epoch: 4 [38400/400000 (10%)]\tLoss: 0.972364\n","Train Epoch: 4 [44800/400000 (11%)]\tLoss: 1.033639\n","Train Epoch: 4 [51200/400000 (13%)]\tLoss: 1.020383\n","Train Epoch: 4 [57600/400000 (14%)]\tLoss: 1.006635\n","Train Epoch: 4 [64000/400000 (16%)]\tLoss: 0.994747\n","Train Epoch: 4 [70400/400000 (18%)]\tLoss: 1.008256\n","Train Epoch: 4 [76800/400000 (19%)]\tLoss: 0.984640\n","Train Epoch: 4 [83200/400000 (21%)]\tLoss: 0.998720\n","Train Epoch: 4 [89600/400000 (22%)]\tLoss: 1.040619\n","Train Epoch: 4 [96000/400000 (24%)]\tLoss: 0.998545\n","Train Epoch: 4 [102400/400000 (26%)]\tLoss: 1.038484\n","Train Epoch: 4 [108800/400000 (27%)]\tLoss: 1.029967\n","Train Epoch: 4 [115200/400000 (29%)]\tLoss: 1.046345\n","Train Epoch: 4 [121600/400000 (30%)]\tLoss: 1.020501\n","Train Epoch: 4 [128000/400000 (32%)]\tLoss: 1.009350\n","Train Epoch: 4 [134400/400000 (34%)]\tLoss: 0.944447\n","Train Epoch: 4 [140800/400000 (35%)]\tLoss: 1.006631\n","Train Epoch: 4 [147200/400000 (37%)]\tLoss: 1.019824\n","Train Epoch: 4 [153600/400000 (38%)]\tLoss: 1.065621\n","Train Epoch: 4 [160000/400000 (40%)]\tLoss: 1.007470\n","Train Epoch: 4 [166400/400000 (42%)]\tLoss: 1.052193\n","Train Epoch: 4 [172800/400000 (43%)]\tLoss: 1.060873\n","Train Epoch: 4 [179200/400000 (45%)]\tLoss: 1.049291\n","Train Epoch: 4 [185600/400000 (46%)]\tLoss: 1.067209\n","Train Epoch: 4 [192000/400000 (48%)]\tLoss: 0.991270\n","Train Epoch: 4 [198400/400000 (50%)]\tLoss: 1.000381\n","Train Epoch: 4 [204800/400000 (51%)]\tLoss: 0.960119\n","Train Epoch: 4 [211200/400000 (53%)]\tLoss: 1.076890\n","Train Epoch: 4 [217600/400000 (54%)]\tLoss: 1.016718\n","Train Epoch: 4 [224000/400000 (56%)]\tLoss: 0.947849\n","Train Epoch: 4 [230400/400000 (58%)]\tLoss: 1.042167\n","Train Epoch: 4 [236800/400000 (59%)]\tLoss: 0.988108\n","Train Epoch: 4 [243200/400000 (61%)]\tLoss: 1.036496\n","Train Epoch: 4 [249600/400000 (62%)]\tLoss: 0.991128\n","Train Epoch: 4 [256000/400000 (64%)]\tLoss: 0.941865\n","Train Epoch: 4 [262400/400000 (66%)]\tLoss: 1.007771\n","Train Epoch: 4 [268800/400000 (67%)]\tLoss: 1.029742\n","Train Epoch: 4 [275200/400000 (69%)]\tLoss: 1.047083\n","Train Epoch: 4 [281600/400000 (70%)]\tLoss: 1.023973\n","Train Epoch: 4 [288000/400000 (72%)]\tLoss: 1.047342\n","Train Epoch: 4 [294400/400000 (74%)]\tLoss: 0.945386\n","Train Epoch: 4 [300800/400000 (75%)]\tLoss: 1.015514\n","Train Epoch: 4 [307200/400000 (77%)]\tLoss: 0.991120\n","Train Epoch: 4 [313600/400000 (78%)]\tLoss: 1.025103\n","Train Epoch: 4 [320000/400000 (80%)]\tLoss: 1.002200\n","Train Epoch: 4 [326400/400000 (82%)]\tLoss: 1.046422\n","Train Epoch: 4 [332800/400000 (83%)]\tLoss: 0.992196\n","Train Epoch: 4 [339200/400000 (85%)]\tLoss: 1.028863\n","Train Epoch: 4 [345600/400000 (86%)]\tLoss: 0.998150\n","Train Epoch: 4 [352000/400000 (88%)]\tLoss: 0.990435\n","Train Epoch: 4 [358400/400000 (90%)]\tLoss: 1.056001\n","Train Epoch: 4 [364800/400000 (91%)]\tLoss: 0.987251\n","Train Epoch: 4 [371200/400000 (93%)]\tLoss: 0.970727\n","Train Epoch: 4 [377600/400000 (94%)]\tLoss: 1.050554\n","Train Epoch: 4 [384000/400000 (96%)]\tLoss: 0.979134\n","Train Epoch: 4 [390400/400000 (98%)]\tLoss: 1.015381\n","Train Epoch: 4 [396800/400000 (99%)]\tLoss: 1.001501\n","\n","Test set: Average loss: 0.0000, Accuracy: 55579/100000 (56%)\n","\n","Train Epoch: 5 [0/400000 (0%)]\tLoss: 1.005942\n","Train Epoch: 5 [6400/400000 (2%)]\tLoss: 0.950159\n","Train Epoch: 5 [12800/400000 (3%)]\tLoss: 1.008469\n","Train Epoch: 5 [19200/400000 (5%)]\tLoss: 0.999434\n","Train Epoch: 5 [25600/400000 (6%)]\tLoss: 0.993781\n","Train Epoch: 5 [32000/400000 (8%)]\tLoss: 0.948611\n","Train Epoch: 5 [38400/400000 (10%)]\tLoss: 1.005478\n","Train Epoch: 5 [44800/400000 (11%)]\tLoss: 1.041710\n","Train Epoch: 5 [51200/400000 (13%)]\tLoss: 1.005231\n","Train Epoch: 5 [57600/400000 (14%)]\tLoss: 1.074507\n","Train Epoch: 5 [64000/400000 (16%)]\tLoss: 1.015853\n","Train Epoch: 5 [70400/400000 (18%)]\tLoss: 1.006147\n","Train Epoch: 5 [76800/400000 (19%)]\tLoss: 1.020175\n","Train Epoch: 5 [83200/400000 (21%)]\tLoss: 1.037868\n","Train Epoch: 5 [89600/400000 (22%)]\tLoss: 0.998385\n","Train Epoch: 5 [96000/400000 (24%)]\tLoss: 0.995113\n","Train Epoch: 5 [102400/400000 (26%)]\tLoss: 0.980819\n","Train Epoch: 5 [108800/400000 (27%)]\tLoss: 1.020788\n","Train Epoch: 5 [115200/400000 (29%)]\tLoss: 0.994741\n","Train Epoch: 5 [121600/400000 (30%)]\tLoss: 0.984740\n","Train Epoch: 5 [128000/400000 (32%)]\tLoss: 0.985746\n","Train Epoch: 5 [134400/400000 (34%)]\tLoss: 1.016628\n","Train Epoch: 5 [140800/400000 (35%)]\tLoss: 0.994002\n","Train Epoch: 5 [147200/400000 (37%)]\tLoss: 1.049596\n","Train Epoch: 5 [153600/400000 (38%)]\tLoss: 0.981664\n","Train Epoch: 5 [160000/400000 (40%)]\tLoss: 1.031910\n","Train Epoch: 5 [166400/400000 (42%)]\tLoss: 1.007586\n","Train Epoch: 5 [172800/400000 (43%)]\tLoss: 0.972750\n","Train Epoch: 5 [179200/400000 (45%)]\tLoss: 1.004011\n","Train Epoch: 5 [185600/400000 (46%)]\tLoss: 1.093949\n","Train Epoch: 5 [192000/400000 (48%)]\tLoss: 0.936183\n","Train Epoch: 5 [198400/400000 (50%)]\tLoss: 0.979595\n","Train Epoch: 5 [204800/400000 (51%)]\tLoss: 0.992169\n","Train Epoch: 5 [211200/400000 (53%)]\tLoss: 1.034284\n","Train Epoch: 5 [217600/400000 (54%)]\tLoss: 1.057219\n","Train Epoch: 5 [224000/400000 (56%)]\tLoss: 1.040779\n","Train Epoch: 5 [230400/400000 (58%)]\tLoss: 0.984955\n","Train Epoch: 5 [236800/400000 (59%)]\tLoss: 0.970343\n","Train Epoch: 5 [243200/400000 (61%)]\tLoss: 1.040014\n","Train Epoch: 5 [249600/400000 (62%)]\tLoss: 0.978321\n","Train Epoch: 5 [256000/400000 (64%)]\tLoss: 1.018376\n","Train Epoch: 5 [262400/400000 (66%)]\tLoss: 1.030034\n","Train Epoch: 5 [268800/400000 (67%)]\tLoss: 0.958418\n","Train Epoch: 5 [275200/400000 (69%)]\tLoss: 1.018817\n","Train Epoch: 5 [281600/400000 (70%)]\tLoss: 0.970955\n","Train Epoch: 5 [288000/400000 (72%)]\tLoss: 0.972184\n","Train Epoch: 5 [294400/400000 (74%)]\tLoss: 1.008408\n","Train Epoch: 5 [300800/400000 (75%)]\tLoss: 0.972406\n","Train Epoch: 5 [307200/400000 (77%)]\tLoss: 0.943693\n","Train Epoch: 5 [313600/400000 (78%)]\tLoss: 0.990505\n","Train Epoch: 5 [320000/400000 (80%)]\tLoss: 1.046733\n","Train Epoch: 5 [326400/400000 (82%)]\tLoss: 1.005042\n","Train Epoch: 5 [332800/400000 (83%)]\tLoss: 1.056421\n","Train Epoch: 5 [339200/400000 (85%)]\tLoss: 1.012874\n","Train Epoch: 5 [345600/400000 (86%)]\tLoss: 1.046643\n","Train Epoch: 5 [352000/400000 (88%)]\tLoss: 1.040499\n","Train Epoch: 5 [358400/400000 (90%)]\tLoss: 0.984494\n","Train Epoch: 5 [364800/400000 (91%)]\tLoss: 0.996542\n","Train Epoch: 5 [371200/400000 (93%)]\tLoss: 1.041743\n","Train Epoch: 5 [377600/400000 (94%)]\tLoss: 1.034744\n","Train Epoch: 5 [384000/400000 (96%)]\tLoss: 0.995043\n","Train Epoch: 5 [390400/400000 (98%)]\tLoss: 1.004670\n","Train Epoch: 5 [396800/400000 (99%)]\tLoss: 0.993963\n","\n","Test set: Average loss: 0.0000, Accuracy: 55638/100000 (56%)\n","\n","Train Epoch: 6 [0/400000 (0%)]\tLoss: 0.998130\n","Train Epoch: 6 [6400/400000 (2%)]\tLoss: 0.990755\n","Train Epoch: 6 [12800/400000 (3%)]\tLoss: 1.051178\n","Train Epoch: 6 [19200/400000 (5%)]\tLoss: 1.088092\n","Train Epoch: 6 [25600/400000 (6%)]\tLoss: 0.988239\n","Train Epoch: 6 [32000/400000 (8%)]\tLoss: 1.011735\n","Train Epoch: 6 [38400/400000 (10%)]\tLoss: 1.038693\n","Train Epoch: 6 [44800/400000 (11%)]\tLoss: 1.020267\n","Train Epoch: 6 [51200/400000 (13%)]\tLoss: 0.990509\n","Train Epoch: 6 [57600/400000 (14%)]\tLoss: 1.055323\n","Train Epoch: 6 [64000/400000 (16%)]\tLoss: 1.007644\n","Train Epoch: 6 [70400/400000 (18%)]\tLoss: 1.016292\n","Train Epoch: 6 [76800/400000 (19%)]\tLoss: 1.045201\n","Train Epoch: 6 [83200/400000 (21%)]\tLoss: 1.005782\n","Train Epoch: 6 [89600/400000 (22%)]\tLoss: 1.049465\n","Train Epoch: 6 [96000/400000 (24%)]\tLoss: 1.063028\n","Train Epoch: 6 [102400/400000 (26%)]\tLoss: 1.010524\n","Train Epoch: 6 [108800/400000 (27%)]\tLoss: 0.990939\n","Train Epoch: 6 [115200/400000 (29%)]\tLoss: 1.025422\n","Train Epoch: 6 [121600/400000 (30%)]\tLoss: 0.954024\n","Train Epoch: 6 [128000/400000 (32%)]\tLoss: 0.957597\n","Train Epoch: 6 [134400/400000 (34%)]\tLoss: 0.950839\n","Train Epoch: 6 [140800/400000 (35%)]\tLoss: 1.000303\n","Train Epoch: 6 [147200/400000 (37%)]\tLoss: 0.996991\n","Train Epoch: 6 [153600/400000 (38%)]\tLoss: 1.010972\n","Train Epoch: 6 [160000/400000 (40%)]\tLoss: 1.007217\n","Train Epoch: 6 [166400/400000 (42%)]\tLoss: 0.978532\n","Train Epoch: 6 [172800/400000 (43%)]\tLoss: 1.069585\n","Train Epoch: 6 [179200/400000 (45%)]\tLoss: 0.954740\n","Train Epoch: 6 [185600/400000 (46%)]\tLoss: 1.029844\n","Train Epoch: 6 [192000/400000 (48%)]\tLoss: 0.974613\n","Train Epoch: 6 [198400/400000 (50%)]\tLoss: 1.018184\n","Train Epoch: 6 [204800/400000 (51%)]\tLoss: 1.038884\n","Train Epoch: 6 [211200/400000 (53%)]\tLoss: 1.015687\n","Train Epoch: 6 [217600/400000 (54%)]\tLoss: 1.029561\n","Train Epoch: 6 [224000/400000 (56%)]\tLoss: 0.977200\n","Train Epoch: 6 [230400/400000 (58%)]\tLoss: 1.008928\n","Train Epoch: 6 [236800/400000 (59%)]\tLoss: 1.052315\n","Train Epoch: 6 [243200/400000 (61%)]\tLoss: 1.031040\n","Train Epoch: 6 [249600/400000 (62%)]\tLoss: 0.969074\n","Train Epoch: 6 [256000/400000 (64%)]\tLoss: 0.987831\n","Train Epoch: 6 [262400/400000 (66%)]\tLoss: 0.964788\n","Train Epoch: 6 [268800/400000 (67%)]\tLoss: 1.052497\n","Train Epoch: 6 [275200/400000 (69%)]\tLoss: 1.037834\n","Train Epoch: 6 [281600/400000 (70%)]\tLoss: 0.985577\n","Train Epoch: 6 [288000/400000 (72%)]\tLoss: 1.013316\n","Train Epoch: 6 [294400/400000 (74%)]\tLoss: 1.019681\n","Train Epoch: 6 [300800/400000 (75%)]\tLoss: 1.005348\n","Train Epoch: 6 [307200/400000 (77%)]\tLoss: 1.009000\n","Train Epoch: 6 [313600/400000 (78%)]\tLoss: 1.026810\n","Train Epoch: 6 [320000/400000 (80%)]\tLoss: 0.994356\n","Train Epoch: 6 [326400/400000 (82%)]\tLoss: 0.983653\n","Train Epoch: 6 [332800/400000 (83%)]\tLoss: 1.001522\n","Train Epoch: 6 [339200/400000 (85%)]\tLoss: 1.026329\n","Train Epoch: 6 [345600/400000 (86%)]\tLoss: 1.003097\n","Train Epoch: 6 [352000/400000 (88%)]\tLoss: 0.977075\n","Train Epoch: 6 [358400/400000 (90%)]\tLoss: 1.032969\n","Train Epoch: 6 [364800/400000 (91%)]\tLoss: 0.967752\n","Train Epoch: 6 [371200/400000 (93%)]\tLoss: 1.089856\n","Train Epoch: 6 [377600/400000 (94%)]\tLoss: 0.987170\n","Train Epoch: 6 [384000/400000 (96%)]\tLoss: 1.020695\n","Train Epoch: 6 [390400/400000 (98%)]\tLoss: 1.013710\n","Train Epoch: 6 [396800/400000 (99%)]\tLoss: 1.004815\n","\n","Test set: Average loss: 0.0000, Accuracy: 55798/100000 (56%)\n","\n","Train Epoch: 7 [0/400000 (0%)]\tLoss: 0.975385\n","Train Epoch: 7 [6400/400000 (2%)]\tLoss: 0.976693\n","Train Epoch: 7 [12800/400000 (3%)]\tLoss: 0.998874\n","Train Epoch: 7 [19200/400000 (5%)]\tLoss: 0.965014\n","Train Epoch: 7 [25600/400000 (6%)]\tLoss: 1.021158\n","Train Epoch: 7 [32000/400000 (8%)]\tLoss: 1.010015\n","Train Epoch: 7 [38400/400000 (10%)]\tLoss: 0.982823\n","Train Epoch: 7 [44800/400000 (11%)]\tLoss: 1.027995\n","Train Epoch: 7 [51200/400000 (13%)]\tLoss: 0.964285\n","Train Epoch: 7 [57600/400000 (14%)]\tLoss: 1.036116\n","Train Epoch: 7 [64000/400000 (16%)]\tLoss: 1.047519\n","Train Epoch: 7 [70400/400000 (18%)]\tLoss: 1.024059\n","Train Epoch: 7 [76800/400000 (19%)]\tLoss: 0.953349\n","Train Epoch: 7 [83200/400000 (21%)]\tLoss: 0.989222\n","Train Epoch: 7 [89600/400000 (22%)]\tLoss: 0.964668\n","Train Epoch: 7 [96000/400000 (24%)]\tLoss: 0.947912\n","Train Epoch: 7 [102400/400000 (26%)]\tLoss: 0.987225\n","Train Epoch: 7 [108800/400000 (27%)]\tLoss: 0.961863\n","Train Epoch: 7 [115200/400000 (29%)]\tLoss: 0.991004\n","Train Epoch: 7 [121600/400000 (30%)]\tLoss: 0.987398\n","Train Epoch: 7 [128000/400000 (32%)]\tLoss: 0.989813\n","Train Epoch: 7 [134400/400000 (34%)]\tLoss: 1.041609\n","Train Epoch: 7 [140800/400000 (35%)]\tLoss: 1.039413\n","Train Epoch: 7 [147200/400000 (37%)]\tLoss: 0.977775\n","Train Epoch: 7 [153600/400000 (38%)]\tLoss: 1.053519\n","Train Epoch: 7 [160000/400000 (40%)]\tLoss: 1.025099\n","Train Epoch: 7 [166400/400000 (42%)]\tLoss: 0.992065\n","Train Epoch: 7 [172800/400000 (43%)]\tLoss: 1.009751\n","Train Epoch: 7 [179200/400000 (45%)]\tLoss: 1.092908\n","Train Epoch: 7 [185600/400000 (46%)]\tLoss: 0.950390\n","Train Epoch: 7 [192000/400000 (48%)]\tLoss: 1.011697\n","Train Epoch: 7 [198400/400000 (50%)]\tLoss: 0.976474\n","Train Epoch: 7 [204800/400000 (51%)]\tLoss: 0.989056\n","Train Epoch: 7 [211200/400000 (53%)]\tLoss: 1.005887\n","Train Epoch: 7 [217600/400000 (54%)]\tLoss: 0.999102\n","Train Epoch: 7 [224000/400000 (56%)]\tLoss: 1.007468\n","Train Epoch: 7 [230400/400000 (58%)]\tLoss: 0.974051\n","Train Epoch: 7 [236800/400000 (59%)]\tLoss: 1.005039\n","Train Epoch: 7 [243200/400000 (61%)]\tLoss: 1.026311\n","Train Epoch: 7 [249600/400000 (62%)]\tLoss: 1.001974\n","Train Epoch: 7 [256000/400000 (64%)]\tLoss: 1.026339\n","Train Epoch: 7 [262400/400000 (66%)]\tLoss: 1.041073\n","Train Epoch: 7 [268800/400000 (67%)]\tLoss: 1.024532\n","Train Epoch: 7 [275200/400000 (69%)]\tLoss: 1.000023\n","Train Epoch: 7 [281600/400000 (70%)]\tLoss: 0.967280\n","Train Epoch: 7 [288000/400000 (72%)]\tLoss: 1.007913\n","Train Epoch: 7 [294400/400000 (74%)]\tLoss: 1.014093\n","Train Epoch: 7 [300800/400000 (75%)]\tLoss: 1.029457\n","Train Epoch: 7 [307200/400000 (77%)]\tLoss: 0.969806\n","Train Epoch: 7 [313600/400000 (78%)]\tLoss: 0.980403\n","Train Epoch: 7 [320000/400000 (80%)]\tLoss: 0.966926\n","Train Epoch: 7 [326400/400000 (82%)]\tLoss: 0.996251\n","Train Epoch: 7 [332800/400000 (83%)]\tLoss: 0.943756\n","Train Epoch: 7 [339200/400000 (85%)]\tLoss: 1.055754\n","Train Epoch: 7 [345600/400000 (86%)]\tLoss: 0.991022\n","Train Epoch: 7 [352000/400000 (88%)]\tLoss: 1.039535\n","Train Epoch: 7 [358400/400000 (90%)]\tLoss: 0.994425\n","Train Epoch: 7 [364800/400000 (91%)]\tLoss: 1.037547\n","Train Epoch: 7 [371200/400000 (93%)]\tLoss: 1.019273\n","Train Epoch: 7 [377600/400000 (94%)]\tLoss: 0.938408\n","Train Epoch: 7 [384000/400000 (96%)]\tLoss: 1.006680\n","Train Epoch: 7 [390400/400000 (98%)]\tLoss: 0.992115\n","Train Epoch: 7 [396800/400000 (99%)]\tLoss: 1.006577\n","\n","Test set: Average loss: 0.0000, Accuracy: 55789/100000 (56%)\n","\n","Train Epoch: 8 [0/400000 (0%)]\tLoss: 1.010792\n","Train Epoch: 8 [6400/400000 (2%)]\tLoss: 1.037625\n","Train Epoch: 8 [12800/400000 (3%)]\tLoss: 0.973346\n","Train Epoch: 8 [19200/400000 (5%)]\tLoss: 1.017542\n","Train Epoch: 8 [25600/400000 (6%)]\tLoss: 1.023231\n","Train Epoch: 8 [32000/400000 (8%)]\tLoss: 1.006591\n","Train Epoch: 8 [38400/400000 (10%)]\tLoss: 1.004259\n","Train Epoch: 8 [44800/400000 (11%)]\tLoss: 1.082700\n","Train Epoch: 8 [51200/400000 (13%)]\tLoss: 1.046210\n","Train Epoch: 8 [57600/400000 (14%)]\tLoss: 1.033530\n","Train Epoch: 8 [64000/400000 (16%)]\tLoss: 1.020714\n","Train Epoch: 8 [70400/400000 (18%)]\tLoss: 0.967926\n","Train Epoch: 8 [76800/400000 (19%)]\tLoss: 0.982666\n","Train Epoch: 8 [83200/400000 (21%)]\tLoss: 0.993646\n","Train Epoch: 8 [89600/400000 (22%)]\tLoss: 1.019340\n","Train Epoch: 8 [96000/400000 (24%)]\tLoss: 1.024090\n","Train Epoch: 8 [102400/400000 (26%)]\tLoss: 1.041444\n","Train Epoch: 8 [108800/400000 (27%)]\tLoss: 1.023833\n","Train Epoch: 8 [115200/400000 (29%)]\tLoss: 0.988713\n","Train Epoch: 8 [121600/400000 (30%)]\tLoss: 1.034073\n","Train Epoch: 8 [128000/400000 (32%)]\tLoss: 1.001324\n","Train Epoch: 8 [134400/400000 (34%)]\tLoss: 1.004386\n","Train Epoch: 8 [140800/400000 (35%)]\tLoss: 0.932016\n","Train Epoch: 8 [147200/400000 (37%)]\tLoss: 1.060713\n","Train Epoch: 8 [153600/400000 (38%)]\tLoss: 1.030478\n","Train Epoch: 8 [160000/400000 (40%)]\tLoss: 0.980264\n","Train Epoch: 8 [166400/400000 (42%)]\tLoss: 1.027149\n","Train Epoch: 8 [172800/400000 (43%)]\tLoss: 0.987222\n","Train Epoch: 8 [179200/400000 (45%)]\tLoss: 1.023914\n","Train Epoch: 8 [185600/400000 (46%)]\tLoss: 1.006313\n","Train Epoch: 8 [192000/400000 (48%)]\tLoss: 0.991523\n","Train Epoch: 8 [198400/400000 (50%)]\tLoss: 1.006180\n","Train Epoch: 8 [204800/400000 (51%)]\tLoss: 1.003744\n","Train Epoch: 8 [211200/400000 (53%)]\tLoss: 1.002780\n","Train Epoch: 8 [217600/400000 (54%)]\tLoss: 0.977944\n","Train Epoch: 8 [224000/400000 (56%)]\tLoss: 1.010517\n","Train Epoch: 8 [230400/400000 (58%)]\tLoss: 0.980917\n","Train Epoch: 8 [236800/400000 (59%)]\tLoss: 1.022133\n","Train Epoch: 8 [243200/400000 (61%)]\tLoss: 1.048005\n","Train Epoch: 8 [249600/400000 (62%)]\tLoss: 0.979952\n","Train Epoch: 8 [256000/400000 (64%)]\tLoss: 1.022030\n","Train Epoch: 8 [262400/400000 (66%)]\tLoss: 1.040676\n","Train Epoch: 8 [268800/400000 (67%)]\tLoss: 0.981591\n","Train Epoch: 8 [275200/400000 (69%)]\tLoss: 1.009232\n","Train Epoch: 8 [281600/400000 (70%)]\tLoss: 1.027860\n","Train Epoch: 8 [288000/400000 (72%)]\tLoss: 0.991457\n","Train Epoch: 8 [294400/400000 (74%)]\tLoss: 1.015471\n","Train Epoch: 8 [300800/400000 (75%)]\tLoss: 1.037264\n","Train Epoch: 8 [307200/400000 (77%)]\tLoss: 0.994713\n","Train Epoch: 8 [313600/400000 (78%)]\tLoss: 0.991817\n","Train Epoch: 8 [320000/400000 (80%)]\tLoss: 0.954483\n","Train Epoch: 8 [326400/400000 (82%)]\tLoss: 1.025354\n","Train Epoch: 8 [332800/400000 (83%)]\tLoss: 0.979932\n","Train Epoch: 8 [339200/400000 (85%)]\tLoss: 0.978518\n","Train Epoch: 8 [345600/400000 (86%)]\tLoss: 1.032140\n","Train Epoch: 8 [352000/400000 (88%)]\tLoss: 1.054221\n","Train Epoch: 8 [358400/400000 (90%)]\tLoss: 0.996398\n","Train Epoch: 8 [364800/400000 (91%)]\tLoss: 0.990913\n","Train Epoch: 8 [371200/400000 (93%)]\tLoss: 0.987094\n","Train Epoch: 8 [377600/400000 (94%)]\tLoss: 0.984861\n","Train Epoch: 8 [384000/400000 (96%)]\tLoss: 0.985044\n","Train Epoch: 8 [390400/400000 (98%)]\tLoss: 1.048867\n","Train Epoch: 8 [396800/400000 (99%)]\tLoss: 1.034237\n","\n","Test set: Average loss: 0.0000, Accuracy: 55797/100000 (56%)\n","\n","Train Epoch: 9 [0/400000 (0%)]\tLoss: 0.997234\n","Train Epoch: 9 [6400/400000 (2%)]\tLoss: 0.982253\n","Train Epoch: 9 [12800/400000 (3%)]\tLoss: 1.015909\n","Train Epoch: 9 [19200/400000 (5%)]\tLoss: 1.017880\n","Train Epoch: 9 [25600/400000 (6%)]\tLoss: 1.040933\n","Train Epoch: 9 [32000/400000 (8%)]\tLoss: 1.010880\n","Train Epoch: 9 [38400/400000 (10%)]\tLoss: 0.989170\n","Train Epoch: 9 [44800/400000 (11%)]\tLoss: 0.976395\n","Train Epoch: 9 [51200/400000 (13%)]\tLoss: 0.976694\n","Train Epoch: 9 [57600/400000 (14%)]\tLoss: 1.041483\n","Train Epoch: 9 [64000/400000 (16%)]\tLoss: 1.033961\n","Train Epoch: 9 [70400/400000 (18%)]\tLoss: 1.016200\n","Train Epoch: 9 [76800/400000 (19%)]\tLoss: 1.006390\n","Train Epoch: 9 [83200/400000 (21%)]\tLoss: 0.989475\n","Train Epoch: 9 [89600/400000 (22%)]\tLoss: 0.990304\n","Train Epoch: 9 [96000/400000 (24%)]\tLoss: 1.051226\n","Train Epoch: 9 [102400/400000 (26%)]\tLoss: 1.020366\n","Train Epoch: 9 [108800/400000 (27%)]\tLoss: 0.955237\n","Train Epoch: 9 [115200/400000 (29%)]\tLoss: 0.962520\n","Train Epoch: 9 [121600/400000 (30%)]\tLoss: 1.000978\n","Train Epoch: 9 [128000/400000 (32%)]\tLoss: 1.002125\n","Train Epoch: 9 [134400/400000 (34%)]\tLoss: 1.013253\n","Train Epoch: 9 [140800/400000 (35%)]\tLoss: 0.975394\n","Train Epoch: 9 [147200/400000 (37%)]\tLoss: 0.977475\n","Train Epoch: 9 [153600/400000 (38%)]\tLoss: 0.953586\n","Train Epoch: 9 [160000/400000 (40%)]\tLoss: 1.018885\n","Train Epoch: 9 [166400/400000 (42%)]\tLoss: 1.011182\n","Train Epoch: 9 [172800/400000 (43%)]\tLoss: 1.027015\n","Train Epoch: 9 [179200/400000 (45%)]\tLoss: 1.038030\n","Train Epoch: 9 [185600/400000 (46%)]\tLoss: 0.992762\n","Train Epoch: 9 [192000/400000 (48%)]\tLoss: 1.006504\n","Train Epoch: 9 [198400/400000 (50%)]\tLoss: 0.996034\n","Train Epoch: 9 [204800/400000 (51%)]\tLoss: 1.004794\n","Train Epoch: 9 [211200/400000 (53%)]\tLoss: 1.015660\n","Train Epoch: 9 [217600/400000 (54%)]\tLoss: 1.020191\n","Train Epoch: 9 [224000/400000 (56%)]\tLoss: 0.982687\n","Train Epoch: 9 [230400/400000 (58%)]\tLoss: 1.034692\n","Train Epoch: 9 [236800/400000 (59%)]\tLoss: 0.966016\n","Train Epoch: 9 [243200/400000 (61%)]\tLoss: 1.040116\n","Train Epoch: 9 [249600/400000 (62%)]\tLoss: 0.967443\n","Train Epoch: 9 [256000/400000 (64%)]\tLoss: 1.027322\n","Train Epoch: 9 [262400/400000 (66%)]\tLoss: 1.006194\n","Train Epoch: 9 [268800/400000 (67%)]\tLoss: 1.017176\n","Train Epoch: 9 [275200/400000 (69%)]\tLoss: 1.013453\n","Train Epoch: 9 [281600/400000 (70%)]\tLoss: 0.963879\n","Train Epoch: 9 [288000/400000 (72%)]\tLoss: 0.990019\n","Train Epoch: 9 [294400/400000 (74%)]\tLoss: 1.041174\n","Train Epoch: 9 [300800/400000 (75%)]\tLoss: 1.041923\n","Train Epoch: 9 [307200/400000 (77%)]\tLoss: 1.016481\n","Train Epoch: 9 [313600/400000 (78%)]\tLoss: 1.004810\n","Train Epoch: 9 [320000/400000 (80%)]\tLoss: 1.014100\n","Train Epoch: 9 [326400/400000 (82%)]\tLoss: 1.007019\n","Train Epoch: 9 [332800/400000 (83%)]\tLoss: 0.962892\n","Train Epoch: 9 [339200/400000 (85%)]\tLoss: 1.025695\n","Train Epoch: 9 [345600/400000 (86%)]\tLoss: 1.016099\n","Train Epoch: 9 [352000/400000 (88%)]\tLoss: 1.023252\n","Train Epoch: 9 [358400/400000 (90%)]\tLoss: 0.998280\n","Train Epoch: 9 [364800/400000 (91%)]\tLoss: 1.014733\n","Train Epoch: 9 [371200/400000 (93%)]\tLoss: 0.983732\n","Train Epoch: 9 [377600/400000 (94%)]\tLoss: 1.029157\n","Train Epoch: 9 [384000/400000 (96%)]\tLoss: 1.025991\n","Train Epoch: 9 [390400/400000 (98%)]\tLoss: 1.003405\n","Train Epoch: 9 [396800/400000 (99%)]\tLoss: 1.027864\n","\n","Test set: Average loss: 0.0000, Accuracy: 55798/100000 (56%)\n","\n","Train Epoch: 10 [0/400000 (0%)]\tLoss: 0.990252\n","Train Epoch: 10 [6400/400000 (2%)]\tLoss: 1.026742\n","Train Epoch: 10 [12800/400000 (3%)]\tLoss: 0.981071\n","Train Epoch: 10 [19200/400000 (5%)]\tLoss: 1.022476\n","Train Epoch: 10 [25600/400000 (6%)]\tLoss: 0.981588\n","Train Epoch: 10 [32000/400000 (8%)]\tLoss: 0.970894\n","Train Epoch: 10 [38400/400000 (10%)]\tLoss: 1.017305\n","Train Epoch: 10 [44800/400000 (11%)]\tLoss: 1.018611\n","Train Epoch: 10 [51200/400000 (13%)]\tLoss: 1.072457\n","Train Epoch: 10 [57600/400000 (14%)]\tLoss: 1.026451\n","Train Epoch: 10 [64000/400000 (16%)]\tLoss: 1.079303\n","Train Epoch: 10 [70400/400000 (18%)]\tLoss: 1.008042\n","Train Epoch: 10 [76800/400000 (19%)]\tLoss: 1.016556\n","Train Epoch: 10 [83200/400000 (21%)]\tLoss: 0.998370\n","Train Epoch: 10 [89600/400000 (22%)]\tLoss: 1.046377\n","Train Epoch: 10 [96000/400000 (24%)]\tLoss: 1.055498\n","Train Epoch: 10 [102400/400000 (26%)]\tLoss: 1.031465\n","Train Epoch: 10 [108800/400000 (27%)]\tLoss: 0.963077\n","Train Epoch: 10 [115200/400000 (29%)]\tLoss: 1.024696\n","Train Epoch: 10 [121600/400000 (30%)]\tLoss: 0.985302\n","Train Epoch: 10 [128000/400000 (32%)]\tLoss: 1.036173\n","Train Epoch: 10 [134400/400000 (34%)]\tLoss: 0.988037\n","Train Epoch: 10 [140800/400000 (35%)]\tLoss: 1.004081\n","Train Epoch: 10 [147200/400000 (37%)]\tLoss: 1.026005\n","Train Epoch: 10 [153600/400000 (38%)]\tLoss: 1.015273\n","Train Epoch: 10 [160000/400000 (40%)]\tLoss: 0.977480\n","Train Epoch: 10 [166400/400000 (42%)]\tLoss: 1.016882\n","Train Epoch: 10 [172800/400000 (43%)]\tLoss: 0.947102\n","Train Epoch: 10 [179200/400000 (45%)]\tLoss: 0.983807\n","Train Epoch: 10 [185600/400000 (46%)]\tLoss: 1.050761\n","Train Epoch: 10 [192000/400000 (48%)]\tLoss: 1.027483\n","Train Epoch: 10 [198400/400000 (50%)]\tLoss: 0.980686\n","Train Epoch: 10 [204800/400000 (51%)]\tLoss: 1.018072\n","Train Epoch: 10 [211200/400000 (53%)]\tLoss: 1.045339\n","Train Epoch: 10 [217600/400000 (54%)]\tLoss: 0.985715\n","Train Epoch: 10 [224000/400000 (56%)]\tLoss: 0.985896\n","Train Epoch: 10 [230400/400000 (58%)]\tLoss: 1.018504\n","Train Epoch: 10 [236800/400000 (59%)]\tLoss: 0.986411\n","Train Epoch: 10 [243200/400000 (61%)]\tLoss: 1.018100\n","Train Epoch: 10 [249600/400000 (62%)]\tLoss: 1.004836\n","Train Epoch: 10 [256000/400000 (64%)]\tLoss: 0.958470\n","Train Epoch: 10 [262400/400000 (66%)]\tLoss: 0.950289\n","Train Epoch: 10 [268800/400000 (67%)]\tLoss: 1.009115\n","Train Epoch: 10 [275200/400000 (69%)]\tLoss: 1.042407\n","Train Epoch: 10 [281600/400000 (70%)]\tLoss: 1.070775\n","Train Epoch: 10 [288000/400000 (72%)]\tLoss: 1.028559\n","Train Epoch: 10 [294400/400000 (74%)]\tLoss: 1.012357\n","Train Epoch: 10 [300800/400000 (75%)]\tLoss: 1.027415\n","Train Epoch: 10 [307200/400000 (77%)]\tLoss: 1.028270\n","Train Epoch: 10 [313600/400000 (78%)]\tLoss: 1.014909\n","Train Epoch: 10 [320000/400000 (80%)]\tLoss: 0.957225\n","Train Epoch: 10 [326400/400000 (82%)]\tLoss: 0.970690\n","Train Epoch: 10 [332800/400000 (83%)]\tLoss: 0.995810\n","Train Epoch: 10 [339200/400000 (85%)]\tLoss: 1.030769\n","Train Epoch: 10 [345600/400000 (86%)]\tLoss: 1.004386\n","Train Epoch: 10 [352000/400000 (88%)]\tLoss: 1.002389\n","Train Epoch: 10 [358400/400000 (90%)]\tLoss: 1.024620\n","Train Epoch: 10 [364800/400000 (91%)]\tLoss: 1.095046\n","Train Epoch: 10 [371200/400000 (93%)]\tLoss: 0.987811\n","Train Epoch: 10 [377600/400000 (94%)]\tLoss: 1.018773\n","Train Epoch: 10 [384000/400000 (96%)]\tLoss: 1.039149\n","Train Epoch: 10 [390400/400000 (98%)]\tLoss: 0.996172\n","Train Epoch: 10 [396800/400000 (99%)]\tLoss: 0.993047\n","\n","Test set: Average loss: 0.0000, Accuracy: 55787/100000 (56%)\n","\n","55798\n"]}]}]}