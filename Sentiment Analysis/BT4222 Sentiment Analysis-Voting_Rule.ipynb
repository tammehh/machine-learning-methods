{"cells":[{"cell_type":"markdown","source":["# Note:\n","- This notebook file may contain methods or algorithms that are NOT covered by the teaching content of BT4222 and hence will not be assessed in your midterm exam.\n","- It serves to increase your exposure in depth and breath to the practical methods in addressing the specific project topic. We believe it will be helpful for your current project and also your future internship endeavors."],"metadata":{"id":"zRZBT5iCiXxb"}},{"cell_type":"markdown","metadata":{"id":"UXYWxqcLTrsT"},"source":["# **Import Library**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"STdFlfuXL6Xz","outputId":"e76f1162-a8ae-4146-894c-57463d45fd39","executionInfo":{"status":"ok","timestamp":1690969588038,"user_tz":-480,"elapsed":15308,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR"]},{"cell_type":"markdown","metadata":{"id":"-98wEEiETkWO"},"source":["# **Define Network Structure**\n","The network structure was from \"Deep-Sentiment: Sentiment Analysis Using Ensemble of CNN and Bi-LSTM Models\" Figure1. Also, you can add three or as much as you want models to increase the accuracy. We just take 2 models as an example\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcaxvGyjMLef"},"outputs":[],"source":["class Net_cnn_lstm(nn.Module):\n","    def __init__(self):\n","        super(Net_cnn_lstm, self).__init__()\n","        self.conv1 = nn.Conv1d(1, 32, 3, 1,1, bias=True)\n","        self.Bn1 = nn.BatchNorm1d(32)\n","        #########################################\n","\n","        self.bi_lstm1 = nn.LSTM(input_size=50, hidden_size=128, num_layers=1, batch_first=True, bidirectional=False)\n","        self.bi_lstm2 = nn.LSTM(input_size=128, hidden_size=128, num_layers=1, batch_first=True, bidirectional=False)\n","\n","        self.conv1 = nn.Conv1d(1, 32, 3, 1,1, bias=True)\n","        self.Bn1 = nn.BatchNorm1d(32)\n","        self.pool1 = nn.AvgPool1d(kernel_size=4, stride=4)\n","\n","        self.fc1 = nn.Linear(1024, 100, bias=True)\n","        self.fc2 = nn.Linear(100, 5, bias=True)\n","\n","\n","\n","    def forward(self, x):\n","        x = torch.flatten(x, 1)\n","        x, _ = self.bi_lstm1(x)\n","        x, _ = self.bi_lstm2(x)\n","        x = x.view(-1, 1, 128)\n","        x = F.relu(self.Bn1(self.conv1(x)))\n","        x = self.pool1(x)\n","\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        return x\n","\n","class Net_CNN(nn.Module):\n","    def __init__(self):\n","        super(Net_CNN, self).__init__()\n","        self.conv1 = nn.Conv1d(1, 32, 3, 1,1, bias=True)\n","        self.Bn1 = nn.BatchNorm1d(32)\n","        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n","\n","        self.conv2 = nn.Conv1d(32, 32, 3, 1,1, bias=True)\n","        self.Bn2 = nn.BatchNorm1d(32)\n","        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n","\n","\n","        self.fc1 = nn.Linear(32*12, 5, bias=True)\n","        self.dropout3 = nn.Dropout2d(0.3)\n","\n","\n","    def forward(self, x):\n","        x = F.relu(self.Bn1(self.conv1(x)))\n","        x = self.pool1(x)\n","        x = F.relu(self.Bn2(self.conv2(x)))\n","        x = self.pool2(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        return x\n","\n","class Net_LSTM(nn.Module):\n","    def __init__(self):\n","        super(Net_LSTM, self).__init__()\n","\n","        #########################################\n","\n","        self.bi_lstm1 = nn.LSTM(input_size=50, hidden_size=128, num_layers=1, batch_first=True, bidirectional=False)\n","        self.bi_lstm2 = nn.LSTM(input_size=128, hidden_size=128, num_layers=1, batch_first=True, bidirectional=False)\n","        self.fc1 = nn.Linear(128, 5, bias=True)\n","\n","\n","    def forward(self, x):\n","        x = torch.flatten(x, 1)\n","        x, _ = self.bi_lstm1(x)\n","        x, _ = self.bi_lstm2(x)\n","        x= self.fc1 (x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"y2tLMOBDYdT7"},"source":["# **Training and Testing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRYHsopyMYYU"},"outputs":[],"source":["def test(model1,model2, model3,device, test_loader):\n","    model1.eval()\n","    model2.eval()\n","    model3.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            target = target-1\n","            output = (model1(data)+model2(data)+model3(data))/3\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n","    return correct"]},{"cell_type":"markdown","metadata":{"id":"vafhcsv9Yiql"},"source":["# **Hyperparameter**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wlKYHWEMdwi"},"outputs":[],"source":["class Args:\n","  epochs = 0\n","  lr = 1.0\n","  use_cuda=False\n","  gamma = 0.7\n","  log_interval = 10\n","  no_cuda = False\n","  seed = 1\n","\n","args = Args()"]},{"cell_type":"markdown","metadata":{"id":"HpgnI9XQR8x5"},"source":["# **Load Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"id":"niXJRCXUR9Uh","outputId":"7af8d591-7dd1-46eb-b178-a4be765e4d11","executionInfo":{"status":"ok","timestamp":1690970102691,"user_tz":-480,"elapsed":9478,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1CCIfElCaURQbuYvHZiL445UQIRzmmuM7\n","To: /content/train_vectors.pt\n","100%|██████████| 80.0M/80.0M [00:00<00:00, 156MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1bwkg7XdmH6Mkp_tkAakCbxJMWNAXJU43\n","To: /content/train_labels.pt\n","100%|██████████| 3.20M/3.20M [00:00<00:00, 177MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1fprUkqC9Qb-y1eDRZt0gA4-4gS941TUo\n","To: /content/test_vectors.pt\n","100%|██████████| 20.0M/20.0M [00:00<00:00, 113MB/s] \n","Downloading...\n","From: https://drive.google.com/uc?id=1VwOqpW7DZPhqAGDrreVwhtzCB2lUc_LD\n","To: /content/test_labels.pt\n","100%|██████████| 801k/801k [00:00<00:00, 101MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1q16FOtZw62Q84lfwqKaHPJpT8p7EVHAs\n","To: /content/Combination_lstm_CNN.pt\n","100%|██████████| 1.32M/1.32M [00:00<00:00, 128MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1ulkKik-gvtP5k00U79em7vUPczygXzM2\n","To: /content/Baseline_lstm.pt\n","100%|██████████| 902k/902k [00:00<00:00, 96.4MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1iWa39Ya8ggvgt8wIDOqIuEhALxtawOzK\n","To: /content/Baseline_CNN.pt\n","100%|██████████| 26.8k/26.8k [00:00<00:00, 45.1MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["'Baseline_CNN.pt'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}],"source":["from google.colab import drive\n","\n","import gdown\n","\n","file_id = '1CCIfElCaURQbuYvHZiL445UQIRzmmuM7'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'train_vectors.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1bwkg7XdmH6Mkp_tkAakCbxJMWNAXJU43'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'train_labels.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1fprUkqC9Qb-y1eDRZt0gA4-4gS941TUo'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'test_vectors.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1VwOqpW7DZPhqAGDrreVwhtzCB2lUc_LD'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'test_labels.pt'\n","gdown.download(url, output, quiet=False)\n","\n","\n","train_vectors = torch.load('train_vectors.pt')\n","train_labels = torch.load('train_labels.pt')\n","test_vectors = torch.load('test_vectors.pt')\n","test_labels = torch.load('test_labels.pt')\n","\n","\n","file_id = '1q16FOtZw62Q84lfwqKaHPJpT8p7EVHAs'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'Combination_lstm_CNN.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1ulkKik-gvtP5k00U79em7vUPczygXzM2'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'Baseline_lstm.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1iWa39Ya8ggvgt8wIDOqIuEhALxtawOzK'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'Baseline_CNN.pt'\n","gdown.download(url, output, quiet=False)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x_jwrzWbbENR"},"source":["# **Start training and testing**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPYD-LvcMi6w","outputId":"ffd9aaa7-4721-48f9-cdd3-345d0e41fdcc","executionInfo":{"status":"ok","timestamp":1690970267563,"user_tz":-480,"elapsed":13834,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0000, Accuracy: 56423/100000 (56%)\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["56423"]},"metadata":{},"execution_count":14}],"source":["torch.manual_seed(args.seed)\n","\n","device = torch.device(\"cpu\")\n","model_cnn_lstm = Net_cnn_lstm().to(device)\n","model_cnn_lstm.load_state_dict(torch.load('Combination_lstm_CNN.pt', map_location=device), strict=False)\n","\n","train_dataset = torch.utils.data.TensorDataset(train_vectors, train_labels)\n","test_dataset = torch.utils.data.TensorDataset(test_vectors, test_labels)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=640, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=640, shuffle=False)\n","\n","model_cnn = Net_CNN().to(device)\n","model_cnn.load_state_dict(torch.load('Baseline_CNN.pt', map_location=device), strict=False)\n","\n","model_LSTM = Net_LSTM().to(device)\n","model_LSTM.load_state_dict(torch.load('Baseline_lstm.pt', map_location=device), strict=False)\n","\n","test(model_cnn,model_LSTM,model_cnn_lstm, device, test_loader)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1A6_M2Aj-nH4ZoAc8moElSXpfuRojWs4f","timestamp":1691995088675}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}