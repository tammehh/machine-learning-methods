{"cells":[{"cell_type":"markdown","source":["# Note:\n","- This notebook file may contain methods or algorithms that are NOT covered by the teaching content of BT4222 and hence will not be assessed in your midterm exam.\n","- It serves to increase your exposure in depth and breath to the practical methods in addressing the specific project topic. We believe it will be helpful for your current project and also your future internship endeavors."],"metadata":{"id":"MucR7Nasli_S"}},{"cell_type":"markdown","metadata":{"id":"UXYWxqcLTrsT"},"source":["# **Import Library**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STdFlfuXL6Xz"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR"]},{"cell_type":"markdown","metadata":{"id":"-98wEEiETkWO"},"source":["# **Define Network Structure**\n","The network structure was from \"Performance Analysis of Different Neural Networks for Sentiment Analysis on IMDb Movie Reviews\" Figure3. and we add two more FC layers for classification.\n","```\n","conv1.weight \t torch.Size([32, 1, 3])\n","conv1.bias \t torch.Size([32])\n","Bn1.weight \t torch.Size([32])\n","Bn1.bias \t torch.Size([32])\n","Bn1.running_mean \t torch.Size([32])\n","Bn1.running_var \t torch.Size([32])\n","Bn1.num_batches_tracked \t torch.Size([])\n","bi_lstm1.weight_ih_l0 \t torch.Size([512, 50])\n","bi_lstm1.weight_hh_l0 \t torch.Size([512, 128])\n","bi_lstm1.bias_ih_l0 \t torch.Size([512])\n","bi_lstm1.bias_hh_l0 \t torch.Size([512])\n","bi_lstm2.weight_ih_l0 \t torch.Size([512, 128])\n","bi_lstm2.weight_hh_l0 \t torch.Size([512, 128])\n","bi_lstm2.bias_ih_l0 \t torch.Size([512])\n","bi_lstm2.bias_hh_l0 \t torch.Size([512])\n","fc1.weight \t torch.Size([100, 1024])\n","fc1.bias \t torch.Size([100])\n","fc2.weight \t torch.Size([5, 100])\n","fc2.bias \t torch.Size([5])\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcaxvGyjMLef"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv1d(1, 32, 3, 1,1, bias=True)\n","        self.Bn1 = nn.BatchNorm1d(32)\n","        #########################################\n","\n","        self.bi_lstm1 = nn.LSTM(input_size=50, hidden_size=128, num_layers=1, batch_first=True, bidirectional=False)\n","        self.bi_lstm2 = nn.LSTM(input_size=128, hidden_size=128, num_layers=1, batch_first=True, bidirectional=False)\n","\n","        self.conv1 = nn.Conv1d(1, 32, 3, 1,1, bias=True)\n","        self.Bn1 = nn.BatchNorm1d(32)\n","        self.pool1 = nn.AvgPool1d(kernel_size=4, stride=4)\n","\n","        self.fc1 = nn.Linear(1024, 100, bias=True)\n","        self.fc2 = nn.Linear(100, 5, bias=True)\n","\n","\n","\n","    def forward(self, x):\n","        x = torch.flatten(x, 1)\n","        x, _ = self.bi_lstm1(x)\n","        x, _ = self.bi_lstm2(x)\n","        x = x.view(-1, 1, 128)\n","        x = F.relu(self.Bn1(self.conv1(x)))\n","        x = self.pool1(x)\n","\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        return x\n","\n"]},{"cell_type":"markdown","metadata":{"id":"y2tLMOBDYdT7"},"source":["# **Training and Testing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRYHsopyMYYU"},"outputs":[],"source":["def train(args, model, device, train_loader, optimizer, epoch):\n","    model.train()  # Set the model to training mode\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):  # Loop over each batch from the training set\n","        data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n","\n","        target = target-1  # Adjust the target values (Moving 1-5 to 0-4  for easy training)\n","        target = target.long()  # Make sure that target data is long type (necessary for loss function)\n","\n","        optimizer.zero_grad()  # Clear gradients from the previous training step\n","        output = model(data)  # Run forward pass (model predictions)\n","\n","        loss = F.cross_entropy(output, target)  # Calculate the loss between the output and target\n","        loss.backward()  # Perform backpropagation (calculate gradients of loss w.r.t. parameters)\n","        optimizer.step()  # Update the model parameters\n","\n","        if batch_idx % args.log_interval == 0:  # Print log info for specified interval\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.item()))\n","\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()  # Set the model to evaluation mode\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n","        for data, target in test_loader:  # Loop over each batch from the testing set\n","            data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n","            target = target-1  # Adjust the target values\n","            output = model(data)  # Run forward pass (model predictions)\n","            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability as the predicted output\n","            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n","\n","    test_loss /= len(test_loader.dataset)  # Calculate the average loss\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n","    return correct  # Return the number of correctly classified samples\n"]},{"cell_type":"markdown","metadata":{"id":"vafhcsv9Yiql"},"source":["# **Hyperparameter**\n","\n","We use only cpu here as an example. learning rate is set as 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wlKYHWEMdwi"},"outputs":[],"source":["class Args:\n","  epochs = 10\n","  lr = 1.0\n","  use_cuda=False\n","  gamma = 0.7\n","  log_interval = 10\n","  no_cuda = False\n","  seed = 1\n","\n","args = Args()"]},{"cell_type":"markdown","metadata":{"id":"HpgnI9XQR8x5"},"source":["# **Load Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"niXJRCXUR9Uh","outputId":"e7af67da-c49d-4e7e-bf9a-75ccb5ec5009","executionInfo":{"status":"ok","timestamp":1691930677703,"user_tz":-480,"elapsed":9114,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1CCIfElCaURQbuYvHZiL445UQIRzmmuM7\n","To: /content/train_vectors.pt\n","100%|██████████| 80.0M/80.0M [00:00<00:00, 127MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1bwkg7XdmH6Mkp_tkAakCbxJMWNAXJU43\n","To: /content/train_labels.pt\n","100%|██████████| 3.20M/3.20M [00:00<00:00, 157MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1fprUkqC9Qb-y1eDRZt0gA4-4gS941TUo\n","To: /content/test_vectors.pt\n","100%|██████████| 20.0M/20.0M [00:00<00:00, 134MB/s] \n","Downloading...\n","From: https://drive.google.com/uc?id=1VwOqpW7DZPhqAGDrreVwhtzCB2lUc_LD\n","To: /content/test_labels.pt\n","100%|██████████| 801k/801k [00:00<00:00, 91.4MB/s]\n"]}],"source":["from google.colab import drive\n","\n","import gdown\n","\n","file_id = '1CCIfElCaURQbuYvHZiL445UQIRzmmuM7'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'train_vectors.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1bwkg7XdmH6Mkp_tkAakCbxJMWNAXJU43'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'train_labels.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1fprUkqC9Qb-y1eDRZt0gA4-4gS941TUo'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'test_vectors.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1VwOqpW7DZPhqAGDrreVwhtzCB2lUc_LD'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'test_labels.pt'\n","gdown.download(url, output, quiet=False)\n","\n","train_vectors = torch.load('train_vectors.pt')\n","train_labels = torch.load('train_labels.pt')\n","test_vectors = torch.load('test_vectors.pt')\n","test_labels = torch.load('test_labels.pt')\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x_jwrzWbbENR"},"source":["# **Start training and testing**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPYD-LvcMi6w","outputId":"506816d4-c46b-4700-b8c8-97be6a8afebd","executionInfo":{"status":"ok","timestamp":1691931643910,"user_tz":-480,"elapsed":960784,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["conv1.weight \t torch.Size([32, 1, 3])\n","conv1.bias \t torch.Size([32])\n","Bn1.weight \t torch.Size([32])\n","Bn1.bias \t torch.Size([32])\n","Bn1.running_mean \t torch.Size([32])\n","Bn1.running_var \t torch.Size([32])\n","Bn1.num_batches_tracked \t torch.Size([])\n","bi_lstm1.weight_ih_l0 \t torch.Size([512, 50])\n","bi_lstm1.weight_hh_l0 \t torch.Size([512, 128])\n","bi_lstm1.bias_ih_l0 \t torch.Size([512])\n","bi_lstm1.bias_hh_l0 \t torch.Size([512])\n","bi_lstm2.weight_ih_l0 \t torch.Size([512, 128])\n","bi_lstm2.weight_hh_l0 \t torch.Size([512, 128])\n","bi_lstm2.bias_ih_l0 \t torch.Size([512])\n","bi_lstm2.bias_hh_l0 \t torch.Size([512])\n","fc1.weight \t torch.Size([100, 1024])\n","fc1.bias \t torch.Size([100])\n","fc2.weight \t torch.Size([5, 100])\n","fc2.bias \t torch.Size([5])\n","Train Epoch: 1 [0/400000 (0%)]\tLoss: 1.611336\n","Train Epoch: 1 [6400/400000 (2%)]\tLoss: 1.649561\n","Train Epoch: 1 [12800/400000 (3%)]\tLoss: 1.567207\n","Train Epoch: 1 [19200/400000 (5%)]\tLoss: 1.541761\n","Train Epoch: 1 [25600/400000 (6%)]\tLoss: 1.450460\n","Train Epoch: 1 [32000/400000 (8%)]\tLoss: 1.405213\n","Train Epoch: 1 [38400/400000 (10%)]\tLoss: 1.375762\n","Train Epoch: 1 [44800/400000 (11%)]\tLoss: 1.170505\n","Train Epoch: 1 [51200/400000 (13%)]\tLoss: 1.084935\n","Train Epoch: 1 [57600/400000 (14%)]\tLoss: 1.242911\n","Train Epoch: 1 [64000/400000 (16%)]\tLoss: 1.172034\n","Train Epoch: 1 [70400/400000 (18%)]\tLoss: 1.102035\n","Train Epoch: 1 [76800/400000 (19%)]\tLoss: 1.206324\n","Train Epoch: 1 [83200/400000 (21%)]\tLoss: 1.127218\n","Train Epoch: 1 [89600/400000 (22%)]\tLoss: 1.027942\n","Train Epoch: 1 [96000/400000 (24%)]\tLoss: 1.044466\n","Train Epoch: 1 [102400/400000 (26%)]\tLoss: 1.064044\n","Train Epoch: 1 [108800/400000 (27%)]\tLoss: 1.173686\n","Train Epoch: 1 [115200/400000 (29%)]\tLoss: 1.077654\n","Train Epoch: 1 [121600/400000 (30%)]\tLoss: 1.108127\n","Train Epoch: 1 [128000/400000 (32%)]\tLoss: 1.066112\n","Train Epoch: 1 [134400/400000 (34%)]\tLoss: 1.067368\n","Train Epoch: 1 [140800/400000 (35%)]\tLoss: 1.064658\n","Train Epoch: 1 [147200/400000 (37%)]\tLoss: 1.020565\n","Train Epoch: 1 [153600/400000 (38%)]\tLoss: 1.025895\n","Train Epoch: 1 [160000/400000 (40%)]\tLoss: 1.031621\n","Train Epoch: 1 [166400/400000 (42%)]\tLoss: 1.130631\n","Train Epoch: 1 [172800/400000 (43%)]\tLoss: 1.075587\n","Train Epoch: 1 [179200/400000 (45%)]\tLoss: 1.089568\n","Train Epoch: 1 [185600/400000 (46%)]\tLoss: 1.066897\n","Train Epoch: 1 [192000/400000 (48%)]\tLoss: 1.064070\n","Train Epoch: 1 [198400/400000 (50%)]\tLoss: 1.042677\n","Train Epoch: 1 [204800/400000 (51%)]\tLoss: 1.052150\n","Train Epoch: 1 [211200/400000 (53%)]\tLoss: 1.094710\n","Train Epoch: 1 [217600/400000 (54%)]\tLoss: 1.017846\n","Train Epoch: 1 [224000/400000 (56%)]\tLoss: 0.994308\n","Train Epoch: 1 [230400/400000 (58%)]\tLoss: 1.034254\n","Train Epoch: 1 [236800/400000 (59%)]\tLoss: 0.996431\n","Train Epoch: 1 [243200/400000 (61%)]\tLoss: 1.065054\n","Train Epoch: 1 [249600/400000 (62%)]\tLoss: 1.036105\n","Train Epoch: 1 [256000/400000 (64%)]\tLoss: 1.098809\n","Train Epoch: 1 [262400/400000 (66%)]\tLoss: 1.071869\n","Train Epoch: 1 [268800/400000 (67%)]\tLoss: 1.053546\n","Train Epoch: 1 [275200/400000 (69%)]\tLoss: 1.027436\n","Train Epoch: 1 [281600/400000 (70%)]\tLoss: 1.107096\n","Train Epoch: 1 [288000/400000 (72%)]\tLoss: 1.096987\n","Train Epoch: 1 [294400/400000 (74%)]\tLoss: 1.021952\n","Train Epoch: 1 [300800/400000 (75%)]\tLoss: 1.039303\n","Train Epoch: 1 [307200/400000 (77%)]\tLoss: 1.110023\n","Train Epoch: 1 [313600/400000 (78%)]\tLoss: 1.059158\n","Train Epoch: 1 [320000/400000 (80%)]\tLoss: 1.016766\n","Train Epoch: 1 [326400/400000 (82%)]\tLoss: 1.044524\n","Train Epoch: 1 [332800/400000 (83%)]\tLoss: 1.087679\n","Train Epoch: 1 [339200/400000 (85%)]\tLoss: 1.052204\n","Train Epoch: 1 [345600/400000 (86%)]\tLoss: 1.034108\n","Train Epoch: 1 [352000/400000 (88%)]\tLoss: 0.948440\n","Train Epoch: 1 [358400/400000 (90%)]\tLoss: 1.051690\n","Train Epoch: 1 [364800/400000 (91%)]\tLoss: 1.026794\n","Train Epoch: 1 [371200/400000 (93%)]\tLoss: 1.001846\n","Train Epoch: 1 [377600/400000 (94%)]\tLoss: 1.044406\n","Train Epoch: 1 [384000/400000 (96%)]\tLoss: 0.995423\n","Train Epoch: 1 [390400/400000 (98%)]\tLoss: 1.047933\n","Train Epoch: 1 [396800/400000 (99%)]\tLoss: 1.002277\n","\n","Test set: Average loss: 0.0000, Accuracy: 54362/100000 (54%)\n","\n","Train Epoch: 2 [0/400000 (0%)]\tLoss: 1.013421\n","Train Epoch: 2 [6400/400000 (2%)]\tLoss: 1.013974\n","Train Epoch: 2 [12800/400000 (3%)]\tLoss: 1.072493\n","Train Epoch: 2 [19200/400000 (5%)]\tLoss: 1.073478\n","Train Epoch: 2 [25600/400000 (6%)]\tLoss: 0.982360\n","Train Epoch: 2 [32000/400000 (8%)]\tLoss: 1.021814\n","Train Epoch: 2 [38400/400000 (10%)]\tLoss: 0.985595\n","Train Epoch: 2 [44800/400000 (11%)]\tLoss: 0.992540\n","Train Epoch: 2 [51200/400000 (13%)]\tLoss: 1.002270\n","Train Epoch: 2 [57600/400000 (14%)]\tLoss: 1.018366\n","Train Epoch: 2 [64000/400000 (16%)]\tLoss: 1.076942\n","Train Epoch: 2 [70400/400000 (18%)]\tLoss: 1.046312\n","Train Epoch: 2 [76800/400000 (19%)]\tLoss: 1.031795\n","Train Epoch: 2 [83200/400000 (21%)]\tLoss: 1.032675\n","Train Epoch: 2 [89600/400000 (22%)]\tLoss: 1.045600\n","Train Epoch: 2 [96000/400000 (24%)]\tLoss: 1.011779\n","Train Epoch: 2 [102400/400000 (26%)]\tLoss: 1.040170\n","Train Epoch: 2 [108800/400000 (27%)]\tLoss: 1.007630\n","Train Epoch: 2 [115200/400000 (29%)]\tLoss: 1.048092\n","Train Epoch: 2 [121600/400000 (30%)]\tLoss: 1.023223\n","Train Epoch: 2 [128000/400000 (32%)]\tLoss: 1.017941\n","Train Epoch: 2 [134400/400000 (34%)]\tLoss: 1.007404\n","Train Epoch: 2 [140800/400000 (35%)]\tLoss: 0.991431\n","Train Epoch: 2 [147200/400000 (37%)]\tLoss: 1.068335\n","Train Epoch: 2 [153600/400000 (38%)]\tLoss: 0.984145\n","Train Epoch: 2 [160000/400000 (40%)]\tLoss: 1.037398\n","Train Epoch: 2 [166400/400000 (42%)]\tLoss: 1.057836\n","Train Epoch: 2 [172800/400000 (43%)]\tLoss: 1.002563\n","Train Epoch: 2 [179200/400000 (45%)]\tLoss: 0.977821\n","Train Epoch: 2 [185600/400000 (46%)]\tLoss: 1.004572\n","Train Epoch: 2 [192000/400000 (48%)]\tLoss: 1.026636\n","Train Epoch: 2 [198400/400000 (50%)]\tLoss: 0.998192\n","Train Epoch: 2 [204800/400000 (51%)]\tLoss: 1.028087\n","Train Epoch: 2 [211200/400000 (53%)]\tLoss: 1.022961\n","Train Epoch: 2 [217600/400000 (54%)]\tLoss: 0.961988\n","Train Epoch: 2 [224000/400000 (56%)]\tLoss: 1.023955\n","Train Epoch: 2 [230400/400000 (58%)]\tLoss: 1.019016\n","Train Epoch: 2 [236800/400000 (59%)]\tLoss: 1.049072\n","Train Epoch: 2 [243200/400000 (61%)]\tLoss: 0.973673\n","Train Epoch: 2 [249600/400000 (62%)]\tLoss: 0.974821\n","Train Epoch: 2 [256000/400000 (64%)]\tLoss: 1.025113\n","Train Epoch: 2 [262400/400000 (66%)]\tLoss: 0.983446\n","Train Epoch: 2 [268800/400000 (67%)]\tLoss: 0.986364\n","Train Epoch: 2 [275200/400000 (69%)]\tLoss: 1.006874\n","Train Epoch: 2 [281600/400000 (70%)]\tLoss: 1.032464\n","Train Epoch: 2 [288000/400000 (72%)]\tLoss: 1.012529\n","Train Epoch: 2 [294400/400000 (74%)]\tLoss: 1.022459\n","Train Epoch: 2 [300800/400000 (75%)]\tLoss: 1.028399\n","Train Epoch: 2 [307200/400000 (77%)]\tLoss: 0.981935\n","Train Epoch: 2 [313600/400000 (78%)]\tLoss: 0.983218\n","Train Epoch: 2 [320000/400000 (80%)]\tLoss: 1.024497\n","Train Epoch: 2 [326400/400000 (82%)]\tLoss: 1.002015\n","Train Epoch: 2 [332800/400000 (83%)]\tLoss: 1.029718\n","Train Epoch: 2 [339200/400000 (85%)]\tLoss: 1.017124\n","Train Epoch: 2 [345600/400000 (86%)]\tLoss: 0.985593\n","Train Epoch: 2 [352000/400000 (88%)]\tLoss: 1.021208\n","Train Epoch: 2 [358400/400000 (90%)]\tLoss: 0.973679\n","Train Epoch: 2 [364800/400000 (91%)]\tLoss: 1.031537\n","Train Epoch: 2 [371200/400000 (93%)]\tLoss: 1.030480\n","Train Epoch: 2 [377600/400000 (94%)]\tLoss: 1.050390\n","Train Epoch: 2 [384000/400000 (96%)]\tLoss: 0.971103\n","Train Epoch: 2 [390400/400000 (98%)]\tLoss: 1.004309\n","Train Epoch: 2 [396800/400000 (99%)]\tLoss: 1.018457\n","\n","Test set: Average loss: 0.0000, Accuracy: 55581/100000 (56%)\n","\n","Train Epoch: 3 [0/400000 (0%)]\tLoss: 0.999475\n","Train Epoch: 3 [6400/400000 (2%)]\tLoss: 0.962096\n","Train Epoch: 3 [12800/400000 (3%)]\tLoss: 1.059349\n","Train Epoch: 3 [19200/400000 (5%)]\tLoss: 0.989149\n","Train Epoch: 3 [25600/400000 (6%)]\tLoss: 1.013907\n","Train Epoch: 3 [32000/400000 (8%)]\tLoss: 1.061873\n","Train Epoch: 3 [38400/400000 (10%)]\tLoss: 0.993252\n","Train Epoch: 3 [44800/400000 (11%)]\tLoss: 0.965001\n","Train Epoch: 3 [51200/400000 (13%)]\tLoss: 1.085233\n","Train Epoch: 3 [57600/400000 (14%)]\tLoss: 1.004496\n","Train Epoch: 3 [64000/400000 (16%)]\tLoss: 1.025308\n","Train Epoch: 3 [70400/400000 (18%)]\tLoss: 1.045591\n","Train Epoch: 3 [76800/400000 (19%)]\tLoss: 0.974527\n","Train Epoch: 3 [83200/400000 (21%)]\tLoss: 0.969307\n","Train Epoch: 3 [89600/400000 (22%)]\tLoss: 1.054015\n","Train Epoch: 3 [96000/400000 (24%)]\tLoss: 1.049845\n","Train Epoch: 3 [102400/400000 (26%)]\tLoss: 0.963299\n","Train Epoch: 3 [108800/400000 (27%)]\tLoss: 1.029411\n","Train Epoch: 3 [115200/400000 (29%)]\tLoss: 1.041851\n","Train Epoch: 3 [121600/400000 (30%)]\tLoss: 1.037297\n","Train Epoch: 3 [128000/400000 (32%)]\tLoss: 1.016485\n","Train Epoch: 3 [134400/400000 (34%)]\tLoss: 1.005761\n","Train Epoch: 3 [140800/400000 (35%)]\tLoss: 0.955888\n","Train Epoch: 3 [147200/400000 (37%)]\tLoss: 1.025110\n","Train Epoch: 3 [153600/400000 (38%)]\tLoss: 1.009173\n","Train Epoch: 3 [160000/400000 (40%)]\tLoss: 1.010925\n","Train Epoch: 3 [166400/400000 (42%)]\tLoss: 1.030718\n","Train Epoch: 3 [172800/400000 (43%)]\tLoss: 1.043686\n","Train Epoch: 3 [179200/400000 (45%)]\tLoss: 1.004870\n","Train Epoch: 3 [185600/400000 (46%)]\tLoss: 1.048723\n","Train Epoch: 3 [192000/400000 (48%)]\tLoss: 1.024217\n","Train Epoch: 3 [198400/400000 (50%)]\tLoss: 0.964556\n","Train Epoch: 3 [204800/400000 (51%)]\tLoss: 1.038637\n","Train Epoch: 3 [211200/400000 (53%)]\tLoss: 1.013815\n","Train Epoch: 3 [217600/400000 (54%)]\tLoss: 0.932363\n","Train Epoch: 3 [224000/400000 (56%)]\tLoss: 1.037768\n","Train Epoch: 3 [230400/400000 (58%)]\tLoss: 1.004066\n","Train Epoch: 3 [236800/400000 (59%)]\tLoss: 1.022114\n","Train Epoch: 3 [243200/400000 (61%)]\tLoss: 1.008759\n","Train Epoch: 3 [249600/400000 (62%)]\tLoss: 1.030481\n","Train Epoch: 3 [256000/400000 (64%)]\tLoss: 1.013447\n","Train Epoch: 3 [262400/400000 (66%)]\tLoss: 1.019181\n","Train Epoch: 3 [268800/400000 (67%)]\tLoss: 1.026643\n","Train Epoch: 3 [275200/400000 (69%)]\tLoss: 1.030458\n","Train Epoch: 3 [281600/400000 (70%)]\tLoss: 0.978160\n","Train Epoch: 3 [288000/400000 (72%)]\tLoss: 1.050247\n","Train Epoch: 3 [294400/400000 (74%)]\tLoss: 0.997860\n","Train Epoch: 3 [300800/400000 (75%)]\tLoss: 0.960866\n","Train Epoch: 3 [307200/400000 (77%)]\tLoss: 1.020802\n","Train Epoch: 3 [313600/400000 (78%)]\tLoss: 0.983918\n","Train Epoch: 3 [320000/400000 (80%)]\tLoss: 0.990562\n","Train Epoch: 3 [326400/400000 (82%)]\tLoss: 1.012356\n","Train Epoch: 3 [332800/400000 (83%)]\tLoss: 1.027611\n","Train Epoch: 3 [339200/400000 (85%)]\tLoss: 1.008717\n","Train Epoch: 3 [345600/400000 (86%)]\tLoss: 0.986074\n","Train Epoch: 3 [352000/400000 (88%)]\tLoss: 1.013094\n","Train Epoch: 3 [358400/400000 (90%)]\tLoss: 0.936792\n","Train Epoch: 3 [364800/400000 (91%)]\tLoss: 0.969145\n","Train Epoch: 3 [371200/400000 (93%)]\tLoss: 1.021111\n","Train Epoch: 3 [377600/400000 (94%)]\tLoss: 0.988561\n","Train Epoch: 3 [384000/400000 (96%)]\tLoss: 0.920360\n","Train Epoch: 3 [390400/400000 (98%)]\tLoss: 1.013572\n","Train Epoch: 3 [396800/400000 (99%)]\tLoss: 1.047273\n","\n","Test set: Average loss: 0.0000, Accuracy: 56063/100000 (56%)\n","\n","Train Epoch: 4 [0/400000 (0%)]\tLoss: 0.964031\n","Train Epoch: 4 [6400/400000 (2%)]\tLoss: 1.002750\n","Train Epoch: 4 [12800/400000 (3%)]\tLoss: 0.974745\n","Train Epoch: 4 [19200/400000 (5%)]\tLoss: 1.015571\n","Train Epoch: 4 [25600/400000 (6%)]\tLoss: 1.002341\n","Train Epoch: 4 [32000/400000 (8%)]\tLoss: 1.021347\n","Train Epoch: 4 [38400/400000 (10%)]\tLoss: 0.999381\n","Train Epoch: 4 [44800/400000 (11%)]\tLoss: 0.997366\n","Train Epoch: 4 [51200/400000 (13%)]\tLoss: 0.998071\n","Train Epoch: 4 [57600/400000 (14%)]\tLoss: 0.995989\n","Train Epoch: 4 [64000/400000 (16%)]\tLoss: 0.975846\n","Train Epoch: 4 [70400/400000 (18%)]\tLoss: 1.024177\n","Train Epoch: 4 [76800/400000 (19%)]\tLoss: 0.986801\n","Train Epoch: 4 [83200/400000 (21%)]\tLoss: 1.001799\n","Train Epoch: 4 [89600/400000 (22%)]\tLoss: 1.047918\n","Train Epoch: 4 [96000/400000 (24%)]\tLoss: 1.004087\n","Train Epoch: 4 [102400/400000 (26%)]\tLoss: 0.934879\n","Train Epoch: 4 [108800/400000 (27%)]\tLoss: 1.038926\n","Train Epoch: 4 [115200/400000 (29%)]\tLoss: 0.976287\n","Train Epoch: 4 [121600/400000 (30%)]\tLoss: 1.011695\n","Train Epoch: 4 [128000/400000 (32%)]\tLoss: 1.067786\n","Train Epoch: 4 [134400/400000 (34%)]\tLoss: 0.981596\n","Train Epoch: 4 [140800/400000 (35%)]\tLoss: 0.995466\n","Train Epoch: 4 [147200/400000 (37%)]\tLoss: 1.061290\n","Train Epoch: 4 [153600/400000 (38%)]\tLoss: 1.002102\n","Train Epoch: 4 [160000/400000 (40%)]\tLoss: 0.935950\n","Train Epoch: 4 [166400/400000 (42%)]\tLoss: 0.945053\n","Train Epoch: 4 [172800/400000 (43%)]\tLoss: 0.986382\n","Train Epoch: 4 [179200/400000 (45%)]\tLoss: 1.024822\n","Train Epoch: 4 [185600/400000 (46%)]\tLoss: 0.983509\n","Train Epoch: 4 [192000/400000 (48%)]\tLoss: 1.075658\n","Train Epoch: 4 [198400/400000 (50%)]\tLoss: 0.964118\n","Train Epoch: 4 [204800/400000 (51%)]\tLoss: 0.992738\n","Train Epoch: 4 [211200/400000 (53%)]\tLoss: 1.053710\n","Train Epoch: 4 [217600/400000 (54%)]\tLoss: 0.973006\n","Train Epoch: 4 [224000/400000 (56%)]\tLoss: 1.009028\n","Train Epoch: 4 [230400/400000 (58%)]\tLoss: 0.999342\n","Train Epoch: 4 [236800/400000 (59%)]\tLoss: 1.073584\n","Train Epoch: 4 [243200/400000 (61%)]\tLoss: 0.961112\n","Train Epoch: 4 [249600/400000 (62%)]\tLoss: 1.016386\n","Train Epoch: 4 [256000/400000 (64%)]\tLoss: 0.985048\n","Train Epoch: 4 [262400/400000 (66%)]\tLoss: 0.999310\n","Train Epoch: 4 [268800/400000 (67%)]\tLoss: 1.019536\n","Train Epoch: 4 [275200/400000 (69%)]\tLoss: 1.016891\n","Train Epoch: 4 [281600/400000 (70%)]\tLoss: 0.961881\n","Train Epoch: 4 [288000/400000 (72%)]\tLoss: 0.980751\n","Train Epoch: 4 [294400/400000 (74%)]\tLoss: 1.012025\n","Train Epoch: 4 [300800/400000 (75%)]\tLoss: 0.987108\n","Train Epoch: 4 [307200/400000 (77%)]\tLoss: 1.003988\n","Train Epoch: 4 [313600/400000 (78%)]\tLoss: 1.021615\n","Train Epoch: 4 [320000/400000 (80%)]\tLoss: 0.986160\n","Train Epoch: 4 [326400/400000 (82%)]\tLoss: 1.028805\n","Train Epoch: 4 [332800/400000 (83%)]\tLoss: 0.996712\n","Train Epoch: 4 [339200/400000 (85%)]\tLoss: 0.999631\n","Train Epoch: 4 [345600/400000 (86%)]\tLoss: 0.953335\n","Train Epoch: 4 [352000/400000 (88%)]\tLoss: 0.989139\n","Train Epoch: 4 [358400/400000 (90%)]\tLoss: 0.972322\n","Train Epoch: 4 [364800/400000 (91%)]\tLoss: 1.002906\n","Train Epoch: 4 [371200/400000 (93%)]\tLoss: 1.026592\n","Train Epoch: 4 [377600/400000 (94%)]\tLoss: 1.002715\n","Train Epoch: 4 [384000/400000 (96%)]\tLoss: 1.079450\n","Train Epoch: 4 [390400/400000 (98%)]\tLoss: 0.974037\n","Train Epoch: 4 [396800/400000 (99%)]\tLoss: 1.031175\n","\n","Test set: Average loss: 0.0000, Accuracy: 56175/100000 (56%)\n","\n","Train Epoch: 5 [0/400000 (0%)]\tLoss: 0.931340\n","Train Epoch: 5 [6400/400000 (2%)]\tLoss: 1.071124\n","Train Epoch: 5 [12800/400000 (3%)]\tLoss: 1.016438\n","Train Epoch: 5 [19200/400000 (5%)]\tLoss: 1.033671\n","Train Epoch: 5 [25600/400000 (6%)]\tLoss: 0.995913\n","Train Epoch: 5 [32000/400000 (8%)]\tLoss: 1.007909\n","Train Epoch: 5 [38400/400000 (10%)]\tLoss: 1.015513\n","Train Epoch: 5 [44800/400000 (11%)]\tLoss: 0.980333\n","Train Epoch: 5 [51200/400000 (13%)]\tLoss: 0.975204\n","Train Epoch: 5 [57600/400000 (14%)]\tLoss: 0.985427\n","Train Epoch: 5 [64000/400000 (16%)]\tLoss: 1.009688\n","Train Epoch: 5 [70400/400000 (18%)]\tLoss: 1.014524\n","Train Epoch: 5 [76800/400000 (19%)]\tLoss: 0.972729\n","Train Epoch: 5 [83200/400000 (21%)]\tLoss: 1.044692\n","Train Epoch: 5 [89600/400000 (22%)]\tLoss: 0.996408\n","Train Epoch: 5 [96000/400000 (24%)]\tLoss: 1.012019\n","Train Epoch: 5 [102400/400000 (26%)]\tLoss: 1.031929\n","Train Epoch: 5 [108800/400000 (27%)]\tLoss: 1.047155\n","Train Epoch: 5 [115200/400000 (29%)]\tLoss: 1.040181\n","Train Epoch: 5 [121600/400000 (30%)]\tLoss: 0.986692\n","Train Epoch: 5 [128000/400000 (32%)]\tLoss: 1.040833\n","Train Epoch: 5 [134400/400000 (34%)]\tLoss: 1.015558\n","Train Epoch: 5 [140800/400000 (35%)]\tLoss: 1.021861\n","Train Epoch: 5 [147200/400000 (37%)]\tLoss: 1.014460\n","Train Epoch: 5 [153600/400000 (38%)]\tLoss: 0.975514\n","Train Epoch: 5 [160000/400000 (40%)]\tLoss: 1.011829\n","Train Epoch: 5 [166400/400000 (42%)]\tLoss: 1.002141\n","Train Epoch: 5 [172800/400000 (43%)]\tLoss: 0.987417\n","Train Epoch: 5 [179200/400000 (45%)]\tLoss: 1.009879\n","Train Epoch: 5 [185600/400000 (46%)]\tLoss: 1.018590\n","Train Epoch: 5 [192000/400000 (48%)]\tLoss: 1.005232\n","Train Epoch: 5 [198400/400000 (50%)]\tLoss: 0.988556\n","Train Epoch: 5 [204800/400000 (51%)]\tLoss: 1.004210\n","Train Epoch: 5 [211200/400000 (53%)]\tLoss: 1.024794\n","Train Epoch: 5 [217600/400000 (54%)]\tLoss: 0.993310\n","Train Epoch: 5 [224000/400000 (56%)]\tLoss: 0.989681\n","Train Epoch: 5 [230400/400000 (58%)]\tLoss: 0.971340\n","Train Epoch: 5 [236800/400000 (59%)]\tLoss: 1.028608\n","Train Epoch: 5 [243200/400000 (61%)]\tLoss: 0.951298\n","Train Epoch: 5 [249600/400000 (62%)]\tLoss: 1.036318\n","Train Epoch: 5 [256000/400000 (64%)]\tLoss: 0.994231\n","Train Epoch: 5 [262400/400000 (66%)]\tLoss: 1.015324\n","Train Epoch: 5 [268800/400000 (67%)]\tLoss: 1.012732\n","Train Epoch: 5 [275200/400000 (69%)]\tLoss: 0.971410\n","Train Epoch: 5 [281600/400000 (70%)]\tLoss: 0.946516\n","Train Epoch: 5 [288000/400000 (72%)]\tLoss: 0.983282\n","Train Epoch: 5 [294400/400000 (74%)]\tLoss: 1.072110\n","Train Epoch: 5 [300800/400000 (75%)]\tLoss: 1.015097\n","Train Epoch: 5 [307200/400000 (77%)]\tLoss: 1.042271\n","Train Epoch: 5 [313600/400000 (78%)]\tLoss: 0.973251\n","Train Epoch: 5 [320000/400000 (80%)]\tLoss: 0.984425\n","Train Epoch: 5 [326400/400000 (82%)]\tLoss: 1.025828\n","Train Epoch: 5 [332800/400000 (83%)]\tLoss: 1.024110\n","Train Epoch: 5 [339200/400000 (85%)]\tLoss: 1.022385\n","Train Epoch: 5 [345600/400000 (86%)]\tLoss: 0.958718\n","Train Epoch: 5 [352000/400000 (88%)]\tLoss: 1.011651\n","Train Epoch: 5 [358400/400000 (90%)]\tLoss: 1.041456\n","Train Epoch: 5 [364800/400000 (91%)]\tLoss: 1.005695\n","Train Epoch: 5 [371200/400000 (93%)]\tLoss: 1.019532\n","Train Epoch: 5 [377600/400000 (94%)]\tLoss: 0.938091\n","Train Epoch: 5 [384000/400000 (96%)]\tLoss: 1.010068\n","Train Epoch: 5 [390400/400000 (98%)]\tLoss: 0.993973\n","Train Epoch: 5 [396800/400000 (99%)]\tLoss: 0.993564\n","\n","Test set: Average loss: 0.0000, Accuracy: 56279/100000 (56%)\n","\n","Train Epoch: 6 [0/400000 (0%)]\tLoss: 0.989330\n","Train Epoch: 6 [6400/400000 (2%)]\tLoss: 1.017377\n","Train Epoch: 6 [12800/400000 (3%)]\tLoss: 0.981309\n","Train Epoch: 6 [19200/400000 (5%)]\tLoss: 0.970289\n","Train Epoch: 6 [25600/400000 (6%)]\tLoss: 0.972599\n","Train Epoch: 6 [32000/400000 (8%)]\tLoss: 0.997517\n","Train Epoch: 6 [38400/400000 (10%)]\tLoss: 0.952404\n","Train Epoch: 6 [44800/400000 (11%)]\tLoss: 1.012258\n","Train Epoch: 6 [51200/400000 (13%)]\tLoss: 1.030123\n","Train Epoch: 6 [57600/400000 (14%)]\tLoss: 1.008668\n","Train Epoch: 6 [64000/400000 (16%)]\tLoss: 1.017284\n","Train Epoch: 6 [70400/400000 (18%)]\tLoss: 0.995072\n","Train Epoch: 6 [76800/400000 (19%)]\tLoss: 0.978515\n","Train Epoch: 6 [83200/400000 (21%)]\tLoss: 1.001885\n","Train Epoch: 6 [89600/400000 (22%)]\tLoss: 1.007148\n","Train Epoch: 6 [96000/400000 (24%)]\tLoss: 1.030252\n","Train Epoch: 6 [102400/400000 (26%)]\tLoss: 1.008011\n","Train Epoch: 6 [108800/400000 (27%)]\tLoss: 0.998177\n","Train Epoch: 6 [115200/400000 (29%)]\tLoss: 0.951699\n","Train Epoch: 6 [121600/400000 (30%)]\tLoss: 0.960478\n","Train Epoch: 6 [128000/400000 (32%)]\tLoss: 0.969393\n","Train Epoch: 6 [134400/400000 (34%)]\tLoss: 1.007406\n","Train Epoch: 6 [140800/400000 (35%)]\tLoss: 0.964902\n","Train Epoch: 6 [147200/400000 (37%)]\tLoss: 0.991774\n","Train Epoch: 6 [153600/400000 (38%)]\tLoss: 0.956102\n","Train Epoch: 6 [160000/400000 (40%)]\tLoss: 0.970186\n","Train Epoch: 6 [166400/400000 (42%)]\tLoss: 1.057938\n","Train Epoch: 6 [172800/400000 (43%)]\tLoss: 0.994133\n","Train Epoch: 6 [179200/400000 (45%)]\tLoss: 1.016902\n","Train Epoch: 6 [185600/400000 (46%)]\tLoss: 1.006878\n","Train Epoch: 6 [192000/400000 (48%)]\tLoss: 0.984645\n","Train Epoch: 6 [198400/400000 (50%)]\tLoss: 1.041984\n","Train Epoch: 6 [204800/400000 (51%)]\tLoss: 1.001111\n","Train Epoch: 6 [211200/400000 (53%)]\tLoss: 0.996900\n","Train Epoch: 6 [217600/400000 (54%)]\tLoss: 1.038364\n","Train Epoch: 6 [224000/400000 (56%)]\tLoss: 1.017613\n","Train Epoch: 6 [230400/400000 (58%)]\tLoss: 0.995762\n","Train Epoch: 6 [236800/400000 (59%)]\tLoss: 1.018857\n","Train Epoch: 6 [243200/400000 (61%)]\tLoss: 1.026627\n","Train Epoch: 6 [249600/400000 (62%)]\tLoss: 1.046836\n","Train Epoch: 6 [256000/400000 (64%)]\tLoss: 0.993331\n","Train Epoch: 6 [262400/400000 (66%)]\tLoss: 1.012913\n","Train Epoch: 6 [268800/400000 (67%)]\tLoss: 1.040429\n","Train Epoch: 6 [275200/400000 (69%)]\tLoss: 1.005667\n","Train Epoch: 6 [281600/400000 (70%)]\tLoss: 0.949808\n","Train Epoch: 6 [288000/400000 (72%)]\tLoss: 0.974789\n","Train Epoch: 6 [294400/400000 (74%)]\tLoss: 0.986738\n","Train Epoch: 6 [300800/400000 (75%)]\tLoss: 1.021934\n","Train Epoch: 6 [307200/400000 (77%)]\tLoss: 0.969446\n","Train Epoch: 6 [313600/400000 (78%)]\tLoss: 1.003177\n","Train Epoch: 6 [320000/400000 (80%)]\tLoss: 1.015049\n","Train Epoch: 6 [326400/400000 (82%)]\tLoss: 0.961872\n","Train Epoch: 6 [332800/400000 (83%)]\tLoss: 1.013020\n","Train Epoch: 6 [339200/400000 (85%)]\tLoss: 0.995615\n","Train Epoch: 6 [345600/400000 (86%)]\tLoss: 0.976153\n","Train Epoch: 6 [352000/400000 (88%)]\tLoss: 1.058182\n","Train Epoch: 6 [358400/400000 (90%)]\tLoss: 1.049539\n","Train Epoch: 6 [364800/400000 (91%)]\tLoss: 1.014585\n","Train Epoch: 6 [371200/400000 (93%)]\tLoss: 0.968316\n","Train Epoch: 6 [377600/400000 (94%)]\tLoss: 1.047577\n","Train Epoch: 6 [384000/400000 (96%)]\tLoss: 0.986681\n","Train Epoch: 6 [390400/400000 (98%)]\tLoss: 1.030380\n","Train Epoch: 6 [396800/400000 (99%)]\tLoss: 1.008233\n","\n","Test set: Average loss: 0.0000, Accuracy: 56432/100000 (56%)\n","\n","Train Epoch: 7 [0/400000 (0%)]\tLoss: 0.976387\n","Train Epoch: 7 [6400/400000 (2%)]\tLoss: 1.017518\n","Train Epoch: 7 [12800/400000 (3%)]\tLoss: 0.972594\n","Train Epoch: 7 [19200/400000 (5%)]\tLoss: 0.983910\n","Train Epoch: 7 [25600/400000 (6%)]\tLoss: 1.008809\n","Train Epoch: 7 [32000/400000 (8%)]\tLoss: 0.919894\n","Train Epoch: 7 [38400/400000 (10%)]\tLoss: 1.024052\n","Train Epoch: 7 [44800/400000 (11%)]\tLoss: 0.978581\n","Train Epoch: 7 [51200/400000 (13%)]\tLoss: 0.966275\n","Train Epoch: 7 [57600/400000 (14%)]\tLoss: 1.013259\n","Train Epoch: 7 [64000/400000 (16%)]\tLoss: 0.991413\n","Train Epoch: 7 [70400/400000 (18%)]\tLoss: 1.033451\n","Train Epoch: 7 [76800/400000 (19%)]\tLoss: 0.996558\n","Train Epoch: 7 [83200/400000 (21%)]\tLoss: 1.011189\n","Train Epoch: 7 [89600/400000 (22%)]\tLoss: 0.967770\n","Train Epoch: 7 [96000/400000 (24%)]\tLoss: 0.973255\n","Train Epoch: 7 [102400/400000 (26%)]\tLoss: 1.025574\n","Train Epoch: 7 [108800/400000 (27%)]\tLoss: 0.979849\n","Train Epoch: 7 [115200/400000 (29%)]\tLoss: 0.958795\n","Train Epoch: 7 [121600/400000 (30%)]\tLoss: 0.973095\n","Train Epoch: 7 [128000/400000 (32%)]\tLoss: 0.970153\n","Train Epoch: 7 [134400/400000 (34%)]\tLoss: 1.001657\n","Train Epoch: 7 [140800/400000 (35%)]\tLoss: 0.976042\n","Train Epoch: 7 [147200/400000 (37%)]\tLoss: 0.954978\n","Train Epoch: 7 [153600/400000 (38%)]\tLoss: 0.979801\n","Train Epoch: 7 [160000/400000 (40%)]\tLoss: 0.997688\n","Train Epoch: 7 [166400/400000 (42%)]\tLoss: 1.011608\n","Train Epoch: 7 [172800/400000 (43%)]\tLoss: 0.979096\n","Train Epoch: 7 [179200/400000 (45%)]\tLoss: 1.035005\n","Train Epoch: 7 [185600/400000 (46%)]\tLoss: 0.985407\n","Train Epoch: 7 [192000/400000 (48%)]\tLoss: 0.967199\n","Train Epoch: 7 [198400/400000 (50%)]\tLoss: 0.927214\n","Train Epoch: 7 [204800/400000 (51%)]\tLoss: 1.006682\n","Train Epoch: 7 [211200/400000 (53%)]\tLoss: 1.051688\n","Train Epoch: 7 [217600/400000 (54%)]\tLoss: 1.004501\n","Train Epoch: 7 [224000/400000 (56%)]\tLoss: 0.995520\n","Train Epoch: 7 [230400/400000 (58%)]\tLoss: 0.977025\n","Train Epoch: 7 [236800/400000 (59%)]\tLoss: 0.955853\n","Train Epoch: 7 [243200/400000 (61%)]\tLoss: 0.991637\n","Train Epoch: 7 [249600/400000 (62%)]\tLoss: 0.968579\n","Train Epoch: 7 [256000/400000 (64%)]\tLoss: 1.029592\n","Train Epoch: 7 [262400/400000 (66%)]\tLoss: 0.996360\n","Train Epoch: 7 [268800/400000 (67%)]\tLoss: 1.013008\n","Train Epoch: 7 [275200/400000 (69%)]\tLoss: 0.967124\n","Train Epoch: 7 [281600/400000 (70%)]\tLoss: 1.028506\n","Train Epoch: 7 [288000/400000 (72%)]\tLoss: 0.994418\n","Train Epoch: 7 [294400/400000 (74%)]\tLoss: 0.983418\n","Train Epoch: 7 [300800/400000 (75%)]\tLoss: 0.977987\n","Train Epoch: 7 [307200/400000 (77%)]\tLoss: 0.969269\n","Train Epoch: 7 [313600/400000 (78%)]\tLoss: 1.022198\n","Train Epoch: 7 [320000/400000 (80%)]\tLoss: 0.998278\n","Train Epoch: 7 [326400/400000 (82%)]\tLoss: 1.024349\n","Train Epoch: 7 [332800/400000 (83%)]\tLoss: 1.022290\n","Train Epoch: 7 [339200/400000 (85%)]\tLoss: 0.992738\n","Train Epoch: 7 [345600/400000 (86%)]\tLoss: 1.017053\n","Train Epoch: 7 [352000/400000 (88%)]\tLoss: 0.990826\n","Train Epoch: 7 [358400/400000 (90%)]\tLoss: 1.022424\n","Train Epoch: 7 [364800/400000 (91%)]\tLoss: 0.973257\n","Train Epoch: 7 [371200/400000 (93%)]\tLoss: 1.041410\n","Train Epoch: 7 [377600/400000 (94%)]\tLoss: 0.966007\n","Train Epoch: 7 [384000/400000 (96%)]\tLoss: 0.983409\n","Train Epoch: 7 [390400/400000 (98%)]\tLoss: 0.988434\n","Train Epoch: 7 [396800/400000 (99%)]\tLoss: 0.999819\n","\n","Test set: Average loss: 0.0000, Accuracy: 56378/100000 (56%)\n","\n","Train Epoch: 8 [0/400000 (0%)]\tLoss: 0.991150\n","Train Epoch: 8 [6400/400000 (2%)]\tLoss: 1.008111\n","Train Epoch: 8 [12800/400000 (3%)]\tLoss: 1.010765\n","Train Epoch: 8 [19200/400000 (5%)]\tLoss: 0.977611\n","Train Epoch: 8 [25600/400000 (6%)]\tLoss: 1.026487\n","Train Epoch: 8 [32000/400000 (8%)]\tLoss: 1.045297\n","Train Epoch: 8 [38400/400000 (10%)]\tLoss: 0.974710\n","Train Epoch: 8 [44800/400000 (11%)]\tLoss: 1.009790\n","Train Epoch: 8 [51200/400000 (13%)]\tLoss: 1.005177\n","Train Epoch: 8 [57600/400000 (14%)]\tLoss: 0.961653\n","Train Epoch: 8 [64000/400000 (16%)]\tLoss: 0.961227\n","Train Epoch: 8 [70400/400000 (18%)]\tLoss: 0.977483\n","Train Epoch: 8 [76800/400000 (19%)]\tLoss: 0.997870\n","Train Epoch: 8 [83200/400000 (21%)]\tLoss: 1.004407\n","Train Epoch: 8 [89600/400000 (22%)]\tLoss: 0.980702\n","Train Epoch: 8 [96000/400000 (24%)]\tLoss: 0.947995\n","Train Epoch: 8 [102400/400000 (26%)]\tLoss: 1.034031\n","Train Epoch: 8 [108800/400000 (27%)]\tLoss: 0.940879\n","Train Epoch: 8 [115200/400000 (29%)]\tLoss: 1.022762\n","Train Epoch: 8 [121600/400000 (30%)]\tLoss: 1.013410\n","Train Epoch: 8 [128000/400000 (32%)]\tLoss: 0.949188\n","Train Epoch: 8 [134400/400000 (34%)]\tLoss: 1.033682\n","Train Epoch: 8 [140800/400000 (35%)]\tLoss: 1.004737\n","Train Epoch: 8 [147200/400000 (37%)]\tLoss: 0.984147\n","Train Epoch: 8 [153600/400000 (38%)]\tLoss: 1.011067\n","Train Epoch: 8 [160000/400000 (40%)]\tLoss: 1.062889\n","Train Epoch: 8 [166400/400000 (42%)]\tLoss: 1.024683\n","Train Epoch: 8 [172800/400000 (43%)]\tLoss: 0.959729\n","Train Epoch: 8 [179200/400000 (45%)]\tLoss: 1.007118\n","Train Epoch: 8 [185600/400000 (46%)]\tLoss: 1.010650\n","Train Epoch: 8 [192000/400000 (48%)]\tLoss: 1.035165\n","Train Epoch: 8 [198400/400000 (50%)]\tLoss: 0.975599\n","Train Epoch: 8 [204800/400000 (51%)]\tLoss: 0.960576\n","Train Epoch: 8 [211200/400000 (53%)]\tLoss: 1.017338\n","Train Epoch: 8 [217600/400000 (54%)]\tLoss: 1.020722\n","Train Epoch: 8 [224000/400000 (56%)]\tLoss: 1.006242\n","Train Epoch: 8 [230400/400000 (58%)]\tLoss: 0.961487\n","Train Epoch: 8 [236800/400000 (59%)]\tLoss: 1.035794\n","Train Epoch: 8 [243200/400000 (61%)]\tLoss: 1.000689\n","Train Epoch: 8 [249600/400000 (62%)]\tLoss: 1.013311\n","Train Epoch: 8 [256000/400000 (64%)]\tLoss: 1.043578\n","Train Epoch: 8 [262400/400000 (66%)]\tLoss: 0.986140\n","Train Epoch: 8 [268800/400000 (67%)]\tLoss: 1.027493\n","Train Epoch: 8 [275200/400000 (69%)]\tLoss: 0.950369\n","Train Epoch: 8 [281600/400000 (70%)]\tLoss: 1.060872\n","Train Epoch: 8 [288000/400000 (72%)]\tLoss: 0.952162\n","Train Epoch: 8 [294400/400000 (74%)]\tLoss: 1.010436\n","Train Epoch: 8 [300800/400000 (75%)]\tLoss: 0.974014\n","Train Epoch: 8 [307200/400000 (77%)]\tLoss: 1.038586\n","Train Epoch: 8 [313600/400000 (78%)]\tLoss: 1.019537\n","Train Epoch: 8 [320000/400000 (80%)]\tLoss: 0.982056\n","Train Epoch: 8 [326400/400000 (82%)]\tLoss: 0.941887\n","Train Epoch: 8 [332800/400000 (83%)]\tLoss: 1.050263\n","Train Epoch: 8 [339200/400000 (85%)]\tLoss: 1.000746\n","Train Epoch: 8 [345600/400000 (86%)]\tLoss: 0.962852\n","Train Epoch: 8 [352000/400000 (88%)]\tLoss: 0.951404\n","Train Epoch: 8 [358400/400000 (90%)]\tLoss: 1.027620\n","Train Epoch: 8 [364800/400000 (91%)]\tLoss: 0.980587\n","Train Epoch: 8 [371200/400000 (93%)]\tLoss: 0.985324\n","Train Epoch: 8 [377600/400000 (94%)]\tLoss: 1.014884\n","Train Epoch: 8 [384000/400000 (96%)]\tLoss: 0.964871\n","Train Epoch: 8 [390400/400000 (98%)]\tLoss: 1.006582\n","Train Epoch: 8 [396800/400000 (99%)]\tLoss: 0.915107\n","\n","Test set: Average loss: 0.0000, Accuracy: 56473/100000 (56%)\n","\n","Train Epoch: 9 [0/400000 (0%)]\tLoss: 0.986754\n","Train Epoch: 9 [6400/400000 (2%)]\tLoss: 0.997828\n","Train Epoch: 9 [12800/400000 (3%)]\tLoss: 0.989501\n","Train Epoch: 9 [19200/400000 (5%)]\tLoss: 1.038165\n","Train Epoch: 9 [25600/400000 (6%)]\tLoss: 1.024364\n","Train Epoch: 9 [32000/400000 (8%)]\tLoss: 0.932984\n","Train Epoch: 9 [38400/400000 (10%)]\tLoss: 1.028601\n","Train Epoch: 9 [44800/400000 (11%)]\tLoss: 0.997195\n","Train Epoch: 9 [51200/400000 (13%)]\tLoss: 1.001698\n","Train Epoch: 9 [57600/400000 (14%)]\tLoss: 1.015157\n","Train Epoch: 9 [64000/400000 (16%)]\tLoss: 0.938242\n","Train Epoch: 9 [70400/400000 (18%)]\tLoss: 1.006006\n","Train Epoch: 9 [76800/400000 (19%)]\tLoss: 1.018476\n","Train Epoch: 9 [83200/400000 (21%)]\tLoss: 1.011697\n","Train Epoch: 9 [89600/400000 (22%)]\tLoss: 1.095609\n","Train Epoch: 9 [96000/400000 (24%)]\tLoss: 1.015848\n","Train Epoch: 9 [102400/400000 (26%)]\tLoss: 1.050565\n","Train Epoch: 9 [108800/400000 (27%)]\tLoss: 1.001163\n","Train Epoch: 9 [115200/400000 (29%)]\tLoss: 0.945288\n","Train Epoch: 9 [121600/400000 (30%)]\tLoss: 0.961460\n","Train Epoch: 9 [128000/400000 (32%)]\tLoss: 1.008690\n","Train Epoch: 9 [134400/400000 (34%)]\tLoss: 1.012984\n","Train Epoch: 9 [140800/400000 (35%)]\tLoss: 0.969541\n","Train Epoch: 9 [147200/400000 (37%)]\tLoss: 0.995509\n","Train Epoch: 9 [153600/400000 (38%)]\tLoss: 0.943116\n","Train Epoch: 9 [160000/400000 (40%)]\tLoss: 0.973532\n","Train Epoch: 9 [166400/400000 (42%)]\tLoss: 0.988029\n","Train Epoch: 9 [172800/400000 (43%)]\tLoss: 1.009737\n","Train Epoch: 9 [179200/400000 (45%)]\tLoss: 0.982323\n","Train Epoch: 9 [185600/400000 (46%)]\tLoss: 1.014464\n","Train Epoch: 9 [192000/400000 (48%)]\tLoss: 1.021308\n","Train Epoch: 9 [198400/400000 (50%)]\tLoss: 1.011573\n","Train Epoch: 9 [204800/400000 (51%)]\tLoss: 1.031716\n","Train Epoch: 9 [211200/400000 (53%)]\tLoss: 1.001383\n","Train Epoch: 9 [217600/400000 (54%)]\tLoss: 0.975645\n","Train Epoch: 9 [224000/400000 (56%)]\tLoss: 1.028735\n","Train Epoch: 9 [230400/400000 (58%)]\tLoss: 0.999199\n","Train Epoch: 9 [236800/400000 (59%)]\tLoss: 1.038936\n","Train Epoch: 9 [243200/400000 (61%)]\tLoss: 0.989506\n","Train Epoch: 9 [249600/400000 (62%)]\tLoss: 0.966937\n","Train Epoch: 9 [256000/400000 (64%)]\tLoss: 0.994216\n","Train Epoch: 9 [262400/400000 (66%)]\tLoss: 0.928342\n","Train Epoch: 9 [268800/400000 (67%)]\tLoss: 0.987633\n","Train Epoch: 9 [275200/400000 (69%)]\tLoss: 1.023693\n","Train Epoch: 9 [281600/400000 (70%)]\tLoss: 0.976691\n","Train Epoch: 9 [288000/400000 (72%)]\tLoss: 1.018792\n","Train Epoch: 9 [294400/400000 (74%)]\tLoss: 0.933304\n","Train Epoch: 9 [300800/400000 (75%)]\tLoss: 1.013025\n","Train Epoch: 9 [307200/400000 (77%)]\tLoss: 0.979783\n","Train Epoch: 9 [313600/400000 (78%)]\tLoss: 0.984060\n","Train Epoch: 9 [320000/400000 (80%)]\tLoss: 0.966873\n","Train Epoch: 9 [326400/400000 (82%)]\tLoss: 1.019160\n","Train Epoch: 9 [332800/400000 (83%)]\tLoss: 0.978784\n","Train Epoch: 9 [339200/400000 (85%)]\tLoss: 0.988403\n","Train Epoch: 9 [345600/400000 (86%)]\tLoss: 0.983206\n","Train Epoch: 9 [352000/400000 (88%)]\tLoss: 0.985336\n","Train Epoch: 9 [358400/400000 (90%)]\tLoss: 1.005301\n","Train Epoch: 9 [364800/400000 (91%)]\tLoss: 1.037698\n","Train Epoch: 9 [371200/400000 (93%)]\tLoss: 1.006987\n","Train Epoch: 9 [377600/400000 (94%)]\tLoss: 0.990565\n","Train Epoch: 9 [384000/400000 (96%)]\tLoss: 1.010646\n","Train Epoch: 9 [390400/400000 (98%)]\tLoss: 0.989241\n","Train Epoch: 9 [396800/400000 (99%)]\tLoss: 0.958052\n","\n","Test set: Average loss: 0.0000, Accuracy: 56494/100000 (56%)\n","\n","Train Epoch: 10 [0/400000 (0%)]\tLoss: 1.075575\n","Train Epoch: 10 [6400/400000 (2%)]\tLoss: 1.015969\n","Train Epoch: 10 [12800/400000 (3%)]\tLoss: 1.047946\n","Train Epoch: 10 [19200/400000 (5%)]\tLoss: 1.000833\n","Train Epoch: 10 [25600/400000 (6%)]\tLoss: 1.055849\n","Train Epoch: 10 [32000/400000 (8%)]\tLoss: 0.971129\n","Train Epoch: 10 [38400/400000 (10%)]\tLoss: 0.974538\n","Train Epoch: 10 [44800/400000 (11%)]\tLoss: 0.978993\n","Train Epoch: 10 [51200/400000 (13%)]\tLoss: 0.983635\n","Train Epoch: 10 [57600/400000 (14%)]\tLoss: 1.033300\n","Train Epoch: 10 [64000/400000 (16%)]\tLoss: 1.021377\n","Train Epoch: 10 [70400/400000 (18%)]\tLoss: 0.988191\n","Train Epoch: 10 [76800/400000 (19%)]\tLoss: 1.045746\n","Train Epoch: 10 [83200/400000 (21%)]\tLoss: 0.999375\n","Train Epoch: 10 [89600/400000 (22%)]\tLoss: 1.046716\n","Train Epoch: 10 [96000/400000 (24%)]\tLoss: 0.998053\n","Train Epoch: 10 [102400/400000 (26%)]\tLoss: 1.016319\n","Train Epoch: 10 [108800/400000 (27%)]\tLoss: 1.005556\n","Train Epoch: 10 [115200/400000 (29%)]\tLoss: 1.011325\n","Train Epoch: 10 [121600/400000 (30%)]\tLoss: 1.020246\n","Train Epoch: 10 [128000/400000 (32%)]\tLoss: 0.992872\n","Train Epoch: 10 [134400/400000 (34%)]\tLoss: 0.970954\n","Train Epoch: 10 [140800/400000 (35%)]\tLoss: 0.961877\n","Train Epoch: 10 [147200/400000 (37%)]\tLoss: 0.984379\n","Train Epoch: 10 [153600/400000 (38%)]\tLoss: 0.981065\n","Train Epoch: 10 [160000/400000 (40%)]\tLoss: 0.952006\n","Train Epoch: 10 [166400/400000 (42%)]\tLoss: 0.985202\n","Train Epoch: 10 [172800/400000 (43%)]\tLoss: 0.976761\n","Train Epoch: 10 [179200/400000 (45%)]\tLoss: 1.017793\n","Train Epoch: 10 [185600/400000 (46%)]\tLoss: 1.072139\n","Train Epoch: 10 [192000/400000 (48%)]\tLoss: 0.962704\n","Train Epoch: 10 [198400/400000 (50%)]\tLoss: 1.009212\n","Train Epoch: 10 [204800/400000 (51%)]\tLoss: 0.993874\n","Train Epoch: 10 [211200/400000 (53%)]\tLoss: 0.942217\n","Train Epoch: 10 [217600/400000 (54%)]\tLoss: 0.954374\n","Train Epoch: 10 [224000/400000 (56%)]\tLoss: 0.975539\n","Train Epoch: 10 [230400/400000 (58%)]\tLoss: 0.974095\n","Train Epoch: 10 [236800/400000 (59%)]\tLoss: 0.972957\n","Train Epoch: 10 [243200/400000 (61%)]\tLoss: 1.031726\n","Train Epoch: 10 [249600/400000 (62%)]\tLoss: 1.049438\n","Train Epoch: 10 [256000/400000 (64%)]\tLoss: 1.043702\n","Train Epoch: 10 [262400/400000 (66%)]\tLoss: 1.047910\n","Train Epoch: 10 [268800/400000 (67%)]\tLoss: 1.003702\n","Train Epoch: 10 [275200/400000 (69%)]\tLoss: 1.029810\n","Train Epoch: 10 [281600/400000 (70%)]\tLoss: 0.991842\n","Train Epoch: 10 [288000/400000 (72%)]\tLoss: 0.962974\n","Train Epoch: 10 [294400/400000 (74%)]\tLoss: 1.041149\n","Train Epoch: 10 [300800/400000 (75%)]\tLoss: 1.055517\n","Train Epoch: 10 [307200/400000 (77%)]\tLoss: 0.991473\n","Train Epoch: 10 [313600/400000 (78%)]\tLoss: 1.021784\n","Train Epoch: 10 [320000/400000 (80%)]\tLoss: 0.961463\n","Train Epoch: 10 [326400/400000 (82%)]\tLoss: 0.969622\n","Train Epoch: 10 [332800/400000 (83%)]\tLoss: 1.046380\n","Train Epoch: 10 [339200/400000 (85%)]\tLoss: 0.960963\n","Train Epoch: 10 [345600/400000 (86%)]\tLoss: 0.941324\n","Train Epoch: 10 [352000/400000 (88%)]\tLoss: 1.000533\n","Train Epoch: 10 [358400/400000 (90%)]\tLoss: 1.018760\n","Train Epoch: 10 [364800/400000 (91%)]\tLoss: 0.949962\n","Train Epoch: 10 [371200/400000 (93%)]\tLoss: 1.012673\n","Train Epoch: 10 [377600/400000 (94%)]\tLoss: 1.002422\n","Train Epoch: 10 [384000/400000 (96%)]\tLoss: 1.016460\n","Train Epoch: 10 [390400/400000 (98%)]\tLoss: 0.961822\n","Train Epoch: 10 [396800/400000 (99%)]\tLoss: 0.967705\n","\n","Test set: Average loss: 0.0000, Accuracy: 56540/100000 (57%)\n","\n","56540\n"]}],"source":["torch.manual_seed(args.seed)\n","\n","device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n","model = Net().to(device)\n","\n","for param_tensor in model.state_dict():\n","        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n","\n","#Form training and testing dataset\n","optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n","\n","train_dataset = torch.utils.data.TensorDataset(train_vectors, train_labels)\n","test_dataset = torch.utils.data.TensorDataset(test_vectors, test_labels)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=640, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=640, shuffle=False)\n","scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n","\n","#Model training\n","ACC = 0\n","for epoch in range(1, args.epochs + 1):\n","    train(args, model, device, train_loader, optimizer, epoch)\n","    ACC_ = test(model, device, test_loader)\n","    if ACC_>ACC or ACC_ == ACC:\n","        ACC = ACC_\n","        torch.save(model.state_dict(), \"Baseline_CNN_lstm.pt\")\n","\n","    scheduler.step()\n","\n","print(ACC)\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1xW3qllc0X06Z3X7jLgmopmTAkrdX_R5c","timestamp":1691994978598}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}