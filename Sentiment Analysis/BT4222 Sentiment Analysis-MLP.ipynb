{"cells":[{"cell_type":"markdown","source":["# Note:\n","- This notebook file may contain methods or algorithms that are NOT covered by the teaching content of BT4222 and hence will not be assessed in your midterm exam.\n","- It serves to increase your exposure in depth and breath to the practical methods in addressing the specific project topic. We believe it will be helpful for your current project and also your future internship endeavors."],"metadata":{"id":"wSlXAcXqjcB3"}},{"cell_type":"markdown","source":[],"metadata":{"id":"AsVI9uRojb-w"}},{"cell_type":"markdown","metadata":{"id":"UXYWxqcLTrsT"},"source":["# **Import Library**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STdFlfuXL6Xz"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR"]},{"cell_type":"markdown","metadata":{"id":"-98wEEiETkWO"},"source":["# **Define Network Structure**\n","Here, I use a 2-layer MLP for classification, the network structure is as followed:\n","\n","```\n","fc1.weight       torch.Size([128, 50])\n","fc1.bias         torch.Size([128])\n","Bn1.weight       torch.Size([128])\n","Bn1.bias         torch.Size([128])\n","Bn1.running_mean         torch.Size([128])\n","Bn1.running_var          torch.Size([128])\n","Bn1.num_batches_tracked          torch.Size([])\n","fc2.weight       torch.Size([128, 128])\n","fc2.bias         torch.Size([128])\n","Bn2.weight       torch.Size([128])\n","Bn2.bias         torch.Size([128])\n","Bn2.running_mean         torch.Size([128])\n","Bn2.running_var          torch.Size([128])\n","Bn2.num_batches_tracked          torch.Size([])\n","fc3.weight       torch.Size([5, 128])\n","fc3.bias         torch.Size([5])\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcaxvGyjMLef"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        #########################################\n","        #MLP\n","\n","        self.fc1 = nn.Linear(50, 128, bias=True)\n","        self.Bn1 = nn.BatchNorm1d(128)\n","        self.fc2 = nn.Linear(128, 128, bias=True)\n","        self.Bn2 = nn.BatchNorm1d(128)\n","        self.fc3 = nn.Linear(128, 5, bias=True)\n","        self.dropout3 = nn.Dropout2d(0.3)\n","\n","\n","\n","    def forward(self, x):\n","\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.Bn1(self.fc1(x)))\n","        x = F.relu(self.Bn1(self.fc2(x)))\n","        x = self.fc3(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"y2tLMOBDYdT7"},"source":["# **Training and Testing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRYHsopyMYYU"},"outputs":[],"source":["def train(args, model, device, train_loader, optimizer, epoch):\n","    model.train()  # Set the model to training mode\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):  # Loop over each batch from the training set\n","        data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n","\n","        target = target-1  # Adjust the target values (Moving 1-5 to 0-4  for easy training)\n","        target = target.long()  # Make sure that target data is long type (necessary for loss function)\n","\n","        optimizer.zero_grad()  # Clear gradients from the previous training step\n","        output = model(data)  # Run forward pass (model predictions)\n","\n","        loss = F.cross_entropy(output, target)  # Calculate the loss between the output and target\n","        loss.backward()  # Perform backpropagation (calculate gradients of loss w.r.t. parameters)\n","        optimizer.step()  # Update the model parameters\n","\n","        if batch_idx % args.log_interval == 0:  # Print log info for specified interval\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.item()))\n","\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()  # Set the model to evaluation mode\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n","        for data, target in test_loader:  # Loop over each batch from the testing set\n","            data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n","            target = target-1  # Adjust the target values\n","            output = model(data)  # Run forward pass (model predictions)\n","            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability as the predicted output\n","            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n","\n","    test_loss /= len(test_loader.dataset)  # Calculate the average loss\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n","    return correct  # Return the number of correctly classified samples\n"]},{"cell_type":"markdown","metadata":{"id":"vafhcsv9Yiql"},"source":["# **Hyperparameter**\n","\n","1. epochs: The number of times the entire dataset is passed forward and backward through the neural network.\n","\n","2. lr: Learning rate, which determines the step size at each iteration while moving towards a minimum in the loss function.\n","\n","3. use_cuda: A boolean flag indicating whether to use CUDA (NVIDIA's parallel computing platform and API) for computations. This would be set to True if you want to utilize GPU acceleration.\n","\n","4. gamma: Typically used in learning rate scheduling. It's a factor by which the learning rate is reduced at certain intervals or when certain conditions are met.\n","\n","5. log_interval: The interval in terms of batches during training.\n","\n","6. seed: A seed value for random number generators to ensure reproducibility of results.\n","\n","For simple networks and small datasets, we typically set the learning rate to 1, the number of epochs to 10 and gamma to 0.7 for model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wlKYHWEMdwi"},"outputs":[],"source":["class Args:\n","  epochs = 10\n","  lr = 1.0\n","  use_cuda=False\n","  gamma = 0.7\n","  log_interval = 10\n","  no_cuda = False\n","  seed = 1\n","\n","args = Args()"]},{"cell_type":"markdown","metadata":{"id":"HpgnI9XQR8x5"},"source":["# **Load Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"niXJRCXUR9Uh","outputId":"881c8714-f4cc-4984-ecc3-97195c1e8f76","executionInfo":{"status":"ok","timestamp":1691930489853,"user_tz":-480,"elapsed":15628,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1CCIfElCaURQbuYvHZiL445UQIRzmmuM7\n","To: /content/train_vectors.pt\n","100%|██████████| 80.0M/80.0M [00:03<00:00, 25.3MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1bwkg7XdmH6Mkp_tkAakCbxJMWNAXJU43\n","To: /content/train_labels.pt\n","100%|██████████| 3.20M/3.20M [00:00<00:00, 181MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1fprUkqC9Qb-y1eDRZt0gA4-4gS941TUo\n","To: /content/test_vectors.pt\n","100%|██████████| 20.0M/20.0M [00:00<00:00, 85.9MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1VwOqpW7DZPhqAGDrreVwhtzCB2lUc_LD\n","To: /content/test_labels.pt\n","100%|██████████| 801k/801k [00:00<00:00, 85.6MB/s]\n"]}],"source":["from google.colab import drive\n","\n","import gdown\n","\n","file_id = '1CCIfElCaURQbuYvHZiL445UQIRzmmuM7'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'train_vectors.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1bwkg7XdmH6Mkp_tkAakCbxJMWNAXJU43'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'train_labels.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1fprUkqC9Qb-y1eDRZt0gA4-4gS941TUo'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'test_vectors.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1VwOqpW7DZPhqAGDrreVwhtzCB2lUc_LD'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'test_labels.pt'\n","gdown.download(url, output, quiet=False)\n","\n","train_vectors = torch.load('train_vectors.pt')\n","train_labels = torch.load('train_labels.pt')\n","test_vectors = torch.load('test_vectors.pt')\n","test_labels = torch.load('test_labels.pt')\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x_jwrzWbbENR"},"source":["# **Start training and testing**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPYD-LvcMi6w","outputId":"25206e9e-b689-4b08-931d-dd005e955b71","executionInfo":{"status":"ok","timestamp":1691930593192,"user_tz":-480,"elapsed":97360,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["fc1.weight \t torch.Size([128, 50])\n","fc1.bias \t torch.Size([128])\n","Bn1.weight \t torch.Size([128])\n","Bn1.bias \t torch.Size([128])\n","Bn1.running_mean \t torch.Size([128])\n","Bn1.running_var \t torch.Size([128])\n","Bn1.num_batches_tracked \t torch.Size([])\n","fc2.weight \t torch.Size([128, 128])\n","fc2.bias \t torch.Size([128])\n","Bn2.weight \t torch.Size([128])\n","Bn2.bias \t torch.Size([128])\n","Bn2.running_mean \t torch.Size([128])\n","Bn2.running_var \t torch.Size([128])\n","Bn2.num_batches_tracked \t torch.Size([])\n","fc3.weight \t torch.Size([5, 128])\n","fc3.bias \t torch.Size([5])\n","Train Epoch: 1 [0/400000 (0%)]\tLoss: 1.647235\n","Train Epoch: 1 [6400/400000 (2%)]\tLoss: 1.150226\n","Train Epoch: 1 [12800/400000 (3%)]\tLoss: 1.065536\n","Train Epoch: 1 [19200/400000 (5%)]\tLoss: 1.070828\n","Train Epoch: 1 [25600/400000 (6%)]\tLoss: 1.048282\n","Train Epoch: 1 [32000/400000 (8%)]\tLoss: 1.076160\n","Train Epoch: 1 [38400/400000 (10%)]\tLoss: 1.056868\n","Train Epoch: 1 [44800/400000 (11%)]\tLoss: 0.996057\n","Train Epoch: 1 [51200/400000 (13%)]\tLoss: 1.042056\n","Train Epoch: 1 [57600/400000 (14%)]\tLoss: 1.070973\n","Train Epoch: 1 [64000/400000 (16%)]\tLoss: 1.051159\n","Train Epoch: 1 [70400/400000 (18%)]\tLoss: 0.993510\n","Train Epoch: 1 [76800/400000 (19%)]\tLoss: 1.039639\n","Train Epoch: 1 [83200/400000 (21%)]\tLoss: 0.988181\n","Train Epoch: 1 [89600/400000 (22%)]\tLoss: 1.070170\n","Train Epoch: 1 [96000/400000 (24%)]\tLoss: 1.006216\n","Train Epoch: 1 [102400/400000 (26%)]\tLoss: 0.986724\n","Train Epoch: 1 [108800/400000 (27%)]\tLoss: 1.122882\n","Train Epoch: 1 [115200/400000 (29%)]\tLoss: 1.026837\n","Train Epoch: 1 [121600/400000 (30%)]\tLoss: 1.043699\n","Train Epoch: 1 [128000/400000 (32%)]\tLoss: 0.960655\n","Train Epoch: 1 [134400/400000 (34%)]\tLoss: 0.986667\n","Train Epoch: 1 [140800/400000 (35%)]\tLoss: 0.991863\n","Train Epoch: 1 [147200/400000 (37%)]\tLoss: 1.008985\n","Train Epoch: 1 [153600/400000 (38%)]\tLoss: 1.002967\n","Train Epoch: 1 [160000/400000 (40%)]\tLoss: 1.009148\n","Train Epoch: 1 [166400/400000 (42%)]\tLoss: 1.029370\n","Train Epoch: 1 [172800/400000 (43%)]\tLoss: 1.022246\n","Train Epoch: 1 [179200/400000 (45%)]\tLoss: 0.950275\n","Train Epoch: 1 [185600/400000 (46%)]\tLoss: 0.974157\n","Train Epoch: 1 [192000/400000 (48%)]\tLoss: 1.018793\n","Train Epoch: 1 [198400/400000 (50%)]\tLoss: 0.986852\n","Train Epoch: 1 [204800/400000 (51%)]\tLoss: 0.995394\n","Train Epoch: 1 [211200/400000 (53%)]\tLoss: 1.032051\n","Train Epoch: 1 [217600/400000 (54%)]\tLoss: 1.036979\n","Train Epoch: 1 [224000/400000 (56%)]\tLoss: 0.977104\n","Train Epoch: 1 [230400/400000 (58%)]\tLoss: 1.015719\n","Train Epoch: 1 [236800/400000 (59%)]\tLoss: 0.985856\n","Train Epoch: 1 [243200/400000 (61%)]\tLoss: 1.015310\n","Train Epoch: 1 [249600/400000 (62%)]\tLoss: 1.008080\n","Train Epoch: 1 [256000/400000 (64%)]\tLoss: 1.020506\n","Train Epoch: 1 [262400/400000 (66%)]\tLoss: 1.004200\n","Train Epoch: 1 [268800/400000 (67%)]\tLoss: 0.993687\n","Train Epoch: 1 [275200/400000 (69%)]\tLoss: 1.030361\n","Train Epoch: 1 [281600/400000 (70%)]\tLoss: 0.957586\n","Train Epoch: 1 [288000/400000 (72%)]\tLoss: 0.990532\n","Train Epoch: 1 [294400/400000 (74%)]\tLoss: 1.048918\n","Train Epoch: 1 [300800/400000 (75%)]\tLoss: 1.007012\n","Train Epoch: 1 [307200/400000 (77%)]\tLoss: 0.962661\n","Train Epoch: 1 [313600/400000 (78%)]\tLoss: 1.030122\n","Train Epoch: 1 [320000/400000 (80%)]\tLoss: 0.991783\n","Train Epoch: 1 [326400/400000 (82%)]\tLoss: 1.026751\n","Train Epoch: 1 [332800/400000 (83%)]\tLoss: 1.025433\n","Train Epoch: 1 [339200/400000 (85%)]\tLoss: 0.996058\n","Train Epoch: 1 [345600/400000 (86%)]\tLoss: 0.968520\n","Train Epoch: 1 [352000/400000 (88%)]\tLoss: 0.974793\n","Train Epoch: 1 [358400/400000 (90%)]\tLoss: 1.021030\n","Train Epoch: 1 [364800/400000 (91%)]\tLoss: 0.961072\n","Train Epoch: 1 [371200/400000 (93%)]\tLoss: 1.025801\n","Train Epoch: 1 [377600/400000 (94%)]\tLoss: 0.945586\n","Train Epoch: 1 [384000/400000 (96%)]\tLoss: 0.971429\n","Train Epoch: 1 [390400/400000 (98%)]\tLoss: 1.020027\n","Train Epoch: 1 [396800/400000 (99%)]\tLoss: 0.954869\n","\n","Test set: Average loss: 0.0000, Accuracy: 51055/100000 (51%)\n","\n","Train Epoch: 2 [0/400000 (0%)]\tLoss: 0.980651\n","Train Epoch: 2 [6400/400000 (2%)]\tLoss: 0.971864\n","Train Epoch: 2 [12800/400000 (3%)]\tLoss: 0.964041\n","Train Epoch: 2 [19200/400000 (5%)]\tLoss: 0.943848\n","Train Epoch: 2 [25600/400000 (6%)]\tLoss: 1.004255\n","Train Epoch: 2 [32000/400000 (8%)]\tLoss: 0.976741\n","Train Epoch: 2 [38400/400000 (10%)]\tLoss: 0.982202\n","Train Epoch: 2 [44800/400000 (11%)]\tLoss: 0.928583\n","Train Epoch: 2 [51200/400000 (13%)]\tLoss: 0.993055\n","Train Epoch: 2 [57600/400000 (14%)]\tLoss: 1.045858\n","Train Epoch: 2 [64000/400000 (16%)]\tLoss: 1.038475\n","Train Epoch: 2 [70400/400000 (18%)]\tLoss: 0.973803\n","Train Epoch: 2 [76800/400000 (19%)]\tLoss: 1.008894\n","Train Epoch: 2 [83200/400000 (21%)]\tLoss: 0.943137\n","Train Epoch: 2 [89600/400000 (22%)]\tLoss: 1.010875\n","Train Epoch: 2 [96000/400000 (24%)]\tLoss: 1.012224\n","Train Epoch: 2 [102400/400000 (26%)]\tLoss: 1.046829\n","Train Epoch: 2 [108800/400000 (27%)]\tLoss: 1.005662\n","Train Epoch: 2 [115200/400000 (29%)]\tLoss: 1.003911\n","Train Epoch: 2 [121600/400000 (30%)]\tLoss: 0.928423\n","Train Epoch: 2 [128000/400000 (32%)]\tLoss: 1.017308\n","Train Epoch: 2 [134400/400000 (34%)]\tLoss: 0.965174\n","Train Epoch: 2 [140800/400000 (35%)]\tLoss: 1.010249\n","Train Epoch: 2 [147200/400000 (37%)]\tLoss: 1.021218\n","Train Epoch: 2 [153600/400000 (38%)]\tLoss: 0.980817\n","Train Epoch: 2 [160000/400000 (40%)]\tLoss: 0.986174\n","Train Epoch: 2 [166400/400000 (42%)]\tLoss: 1.005728\n","Train Epoch: 2 [172800/400000 (43%)]\tLoss: 0.978828\n","Train Epoch: 2 [179200/400000 (45%)]\tLoss: 1.006426\n","Train Epoch: 2 [185600/400000 (46%)]\tLoss: 0.953824\n","Train Epoch: 2 [192000/400000 (48%)]\tLoss: 0.996545\n","Train Epoch: 2 [198400/400000 (50%)]\tLoss: 1.004811\n","Train Epoch: 2 [204800/400000 (51%)]\tLoss: 0.992261\n","Train Epoch: 2 [211200/400000 (53%)]\tLoss: 0.988186\n","Train Epoch: 2 [217600/400000 (54%)]\tLoss: 0.978279\n","Train Epoch: 2 [224000/400000 (56%)]\tLoss: 1.012340\n","Train Epoch: 2 [230400/400000 (58%)]\tLoss: 0.950690\n","Train Epoch: 2 [236800/400000 (59%)]\tLoss: 1.053597\n","Train Epoch: 2 [243200/400000 (61%)]\tLoss: 0.954469\n","Train Epoch: 2 [249600/400000 (62%)]\tLoss: 1.014224\n","Train Epoch: 2 [256000/400000 (64%)]\tLoss: 1.003141\n","Train Epoch: 2 [262400/400000 (66%)]\tLoss: 0.977284\n","Train Epoch: 2 [268800/400000 (67%)]\tLoss: 0.930171\n","Train Epoch: 2 [275200/400000 (69%)]\tLoss: 0.958111\n","Train Epoch: 2 [281600/400000 (70%)]\tLoss: 1.012605\n","Train Epoch: 2 [288000/400000 (72%)]\tLoss: 1.033182\n","Train Epoch: 2 [294400/400000 (74%)]\tLoss: 1.012513\n","Train Epoch: 2 [300800/400000 (75%)]\tLoss: 0.941744\n","Train Epoch: 2 [307200/400000 (77%)]\tLoss: 0.987245\n","Train Epoch: 2 [313600/400000 (78%)]\tLoss: 0.986382\n","Train Epoch: 2 [320000/400000 (80%)]\tLoss: 0.957812\n","Train Epoch: 2 [326400/400000 (82%)]\tLoss: 1.010702\n","Train Epoch: 2 [332800/400000 (83%)]\tLoss: 0.952992\n","Train Epoch: 2 [339200/400000 (85%)]\tLoss: 1.011674\n","Train Epoch: 2 [345600/400000 (86%)]\tLoss: 0.972169\n","Train Epoch: 2 [352000/400000 (88%)]\tLoss: 0.967547\n","Train Epoch: 2 [358400/400000 (90%)]\tLoss: 0.979663\n","Train Epoch: 2 [364800/400000 (91%)]\tLoss: 0.961216\n","Train Epoch: 2 [371200/400000 (93%)]\tLoss: 1.009607\n","Train Epoch: 2 [377600/400000 (94%)]\tLoss: 0.956865\n","Train Epoch: 2 [384000/400000 (96%)]\tLoss: 0.957786\n","Train Epoch: 2 [390400/400000 (98%)]\tLoss: 0.943777\n","Train Epoch: 2 [396800/400000 (99%)]\tLoss: 0.951596\n","\n","Test set: Average loss: 0.0000, Accuracy: 52261/100000 (52%)\n","\n","Train Epoch: 3 [0/400000 (0%)]\tLoss: 0.963822\n","Train Epoch: 3 [6400/400000 (2%)]\tLoss: 0.998353\n","Train Epoch: 3 [12800/400000 (3%)]\tLoss: 1.015407\n","Train Epoch: 3 [19200/400000 (5%)]\tLoss: 0.995068\n","Train Epoch: 3 [25600/400000 (6%)]\tLoss: 0.982191\n","Train Epoch: 3 [32000/400000 (8%)]\tLoss: 1.018781\n","Train Epoch: 3 [38400/400000 (10%)]\tLoss: 0.962447\n","Train Epoch: 3 [44800/400000 (11%)]\tLoss: 0.980164\n","Train Epoch: 3 [51200/400000 (13%)]\tLoss: 0.980688\n","Train Epoch: 3 [57600/400000 (14%)]\tLoss: 0.983620\n","Train Epoch: 3 [64000/400000 (16%)]\tLoss: 1.009337\n","Train Epoch: 3 [70400/400000 (18%)]\tLoss: 0.934215\n","Train Epoch: 3 [76800/400000 (19%)]\tLoss: 1.024350\n","Train Epoch: 3 [83200/400000 (21%)]\tLoss: 0.942752\n","Train Epoch: 3 [89600/400000 (22%)]\tLoss: 1.004937\n","Train Epoch: 3 [96000/400000 (24%)]\tLoss: 0.956798\n","Train Epoch: 3 [102400/400000 (26%)]\tLoss: 0.999797\n","Train Epoch: 3 [108800/400000 (27%)]\tLoss: 0.995195\n","Train Epoch: 3 [115200/400000 (29%)]\tLoss: 0.936941\n","Train Epoch: 3 [121600/400000 (30%)]\tLoss: 0.927253\n","Train Epoch: 3 [128000/400000 (32%)]\tLoss: 0.988644\n","Train Epoch: 3 [134400/400000 (34%)]\tLoss: 1.011828\n","Train Epoch: 3 [140800/400000 (35%)]\tLoss: 1.013514\n","Train Epoch: 3 [147200/400000 (37%)]\tLoss: 0.961923\n","Train Epoch: 3 [153600/400000 (38%)]\tLoss: 0.938551\n","Train Epoch: 3 [160000/400000 (40%)]\tLoss: 0.970501\n","Train Epoch: 3 [166400/400000 (42%)]\tLoss: 1.010315\n","Train Epoch: 3 [172800/400000 (43%)]\tLoss: 1.005515\n","Train Epoch: 3 [179200/400000 (45%)]\tLoss: 1.000499\n","Train Epoch: 3 [185600/400000 (46%)]\tLoss: 0.936876\n","Train Epoch: 3 [192000/400000 (48%)]\tLoss: 0.995399\n","Train Epoch: 3 [198400/400000 (50%)]\tLoss: 0.931343\n","Train Epoch: 3 [204800/400000 (51%)]\tLoss: 1.016256\n","Train Epoch: 3 [211200/400000 (53%)]\tLoss: 0.962872\n","Train Epoch: 3 [217600/400000 (54%)]\tLoss: 0.990130\n","Train Epoch: 3 [224000/400000 (56%)]\tLoss: 1.010933\n","Train Epoch: 3 [230400/400000 (58%)]\tLoss: 0.998067\n","Train Epoch: 3 [236800/400000 (59%)]\tLoss: 0.970105\n","Train Epoch: 3 [243200/400000 (61%)]\tLoss: 0.989562\n","Train Epoch: 3 [249600/400000 (62%)]\tLoss: 0.956099\n","Train Epoch: 3 [256000/400000 (64%)]\tLoss: 1.042362\n","Train Epoch: 3 [262400/400000 (66%)]\tLoss: 0.999208\n","Train Epoch: 3 [268800/400000 (67%)]\tLoss: 0.972238\n","Train Epoch: 3 [275200/400000 (69%)]\tLoss: 1.009084\n","Train Epoch: 3 [281600/400000 (70%)]\tLoss: 0.999933\n","Train Epoch: 3 [288000/400000 (72%)]\tLoss: 0.997572\n","Train Epoch: 3 [294400/400000 (74%)]\tLoss: 0.975927\n","Train Epoch: 3 [300800/400000 (75%)]\tLoss: 1.001655\n","Train Epoch: 3 [307200/400000 (77%)]\tLoss: 1.009516\n","Train Epoch: 3 [313600/400000 (78%)]\tLoss: 0.975204\n","Train Epoch: 3 [320000/400000 (80%)]\tLoss: 0.937692\n","Train Epoch: 3 [326400/400000 (82%)]\tLoss: 0.979691\n","Train Epoch: 3 [332800/400000 (83%)]\tLoss: 0.984322\n","Train Epoch: 3 [339200/400000 (85%)]\tLoss: 0.952723\n","Train Epoch: 3 [345600/400000 (86%)]\tLoss: 0.970524\n","Train Epoch: 3 [352000/400000 (88%)]\tLoss: 0.994897\n","Train Epoch: 3 [358400/400000 (90%)]\tLoss: 0.951562\n","Train Epoch: 3 [364800/400000 (91%)]\tLoss: 0.962373\n","Train Epoch: 3 [371200/400000 (93%)]\tLoss: 0.988094\n","Train Epoch: 3 [377600/400000 (94%)]\tLoss: 0.950884\n","Train Epoch: 3 [384000/400000 (96%)]\tLoss: 1.022405\n","Train Epoch: 3 [390400/400000 (98%)]\tLoss: 1.021029\n","Train Epoch: 3 [396800/400000 (99%)]\tLoss: 0.980733\n","\n","Test set: Average loss: 0.0000, Accuracy: 52114/100000 (52%)\n","\n","Train Epoch: 4 [0/400000 (0%)]\tLoss: 0.959209\n","Train Epoch: 4 [6400/400000 (2%)]\tLoss: 0.973158\n","Train Epoch: 4 [12800/400000 (3%)]\tLoss: 0.963187\n","Train Epoch: 4 [19200/400000 (5%)]\tLoss: 1.046414\n","Train Epoch: 4 [25600/400000 (6%)]\tLoss: 0.983924\n","Train Epoch: 4 [32000/400000 (8%)]\tLoss: 0.930395\n","Train Epoch: 4 [38400/400000 (10%)]\tLoss: 0.941277\n","Train Epoch: 4 [44800/400000 (11%)]\tLoss: 0.991749\n","Train Epoch: 4 [51200/400000 (13%)]\tLoss: 0.953723\n","Train Epoch: 4 [57600/400000 (14%)]\tLoss: 1.006297\n","Train Epoch: 4 [64000/400000 (16%)]\tLoss: 0.978054\n","Train Epoch: 4 [70400/400000 (18%)]\tLoss: 0.984992\n","Train Epoch: 4 [76800/400000 (19%)]\tLoss: 1.031034\n","Train Epoch: 4 [83200/400000 (21%)]\tLoss: 0.983672\n","Train Epoch: 4 [89600/400000 (22%)]\tLoss: 1.009884\n","Train Epoch: 4 [96000/400000 (24%)]\tLoss: 0.986631\n","Train Epoch: 4 [102400/400000 (26%)]\tLoss: 0.952234\n","Train Epoch: 4 [108800/400000 (27%)]\tLoss: 0.950340\n","Train Epoch: 4 [115200/400000 (29%)]\tLoss: 0.936459\n","Train Epoch: 4 [121600/400000 (30%)]\tLoss: 0.998800\n","Train Epoch: 4 [128000/400000 (32%)]\tLoss: 0.950588\n","Train Epoch: 4 [134400/400000 (34%)]\tLoss: 1.004938\n","Train Epoch: 4 [140800/400000 (35%)]\tLoss: 0.941150\n","Train Epoch: 4 [147200/400000 (37%)]\tLoss: 0.964243\n","Train Epoch: 4 [153600/400000 (38%)]\tLoss: 0.936715\n","Train Epoch: 4 [160000/400000 (40%)]\tLoss: 0.999861\n","Train Epoch: 4 [166400/400000 (42%)]\tLoss: 0.987313\n","Train Epoch: 4 [172800/400000 (43%)]\tLoss: 0.959825\n","Train Epoch: 4 [179200/400000 (45%)]\tLoss: 0.965026\n","Train Epoch: 4 [185600/400000 (46%)]\tLoss: 0.989461\n","Train Epoch: 4 [192000/400000 (48%)]\tLoss: 0.999722\n","Train Epoch: 4 [198400/400000 (50%)]\tLoss: 0.931756\n","Train Epoch: 4 [204800/400000 (51%)]\tLoss: 0.963860\n","Train Epoch: 4 [211200/400000 (53%)]\tLoss: 0.929742\n","Train Epoch: 4 [217600/400000 (54%)]\tLoss: 1.015853\n","Train Epoch: 4 [224000/400000 (56%)]\tLoss: 0.993597\n","Train Epoch: 4 [230400/400000 (58%)]\tLoss: 0.983343\n","Train Epoch: 4 [236800/400000 (59%)]\tLoss: 0.971472\n","Train Epoch: 4 [243200/400000 (61%)]\tLoss: 0.971219\n","Train Epoch: 4 [249600/400000 (62%)]\tLoss: 1.061017\n","Train Epoch: 4 [256000/400000 (64%)]\tLoss: 0.966578\n","Train Epoch: 4 [262400/400000 (66%)]\tLoss: 1.009651\n","Train Epoch: 4 [268800/400000 (67%)]\tLoss: 1.003289\n","Train Epoch: 4 [275200/400000 (69%)]\tLoss: 1.006580\n","Train Epoch: 4 [281600/400000 (70%)]\tLoss: 0.942582\n","Train Epoch: 4 [288000/400000 (72%)]\tLoss: 0.955159\n","Train Epoch: 4 [294400/400000 (74%)]\tLoss: 0.949006\n","Train Epoch: 4 [300800/400000 (75%)]\tLoss: 0.987638\n","Train Epoch: 4 [307200/400000 (77%)]\tLoss: 0.967618\n","Train Epoch: 4 [313600/400000 (78%)]\tLoss: 0.999265\n","Train Epoch: 4 [320000/400000 (80%)]\tLoss: 0.948768\n","Train Epoch: 4 [326400/400000 (82%)]\tLoss: 0.965447\n","Train Epoch: 4 [332800/400000 (83%)]\tLoss: 1.010987\n","Train Epoch: 4 [339200/400000 (85%)]\tLoss: 0.998611\n","Train Epoch: 4 [345600/400000 (86%)]\tLoss: 0.937404\n","Train Epoch: 4 [352000/400000 (88%)]\tLoss: 0.966004\n","Train Epoch: 4 [358400/400000 (90%)]\tLoss: 0.964296\n","Train Epoch: 4 [364800/400000 (91%)]\tLoss: 1.035665\n","Train Epoch: 4 [371200/400000 (93%)]\tLoss: 0.969054\n","Train Epoch: 4 [377600/400000 (94%)]\tLoss: 0.970798\n","Train Epoch: 4 [384000/400000 (96%)]\tLoss: 0.934858\n","Train Epoch: 4 [390400/400000 (98%)]\tLoss: 0.977284\n","Train Epoch: 4 [396800/400000 (99%)]\tLoss: 1.016165\n","\n","Test set: Average loss: 0.0000, Accuracy: 52456/100000 (52%)\n","\n","Train Epoch: 5 [0/400000 (0%)]\tLoss: 0.916978\n","Train Epoch: 5 [6400/400000 (2%)]\tLoss: 0.955853\n","Train Epoch: 5 [12800/400000 (3%)]\tLoss: 0.992841\n","Train Epoch: 5 [19200/400000 (5%)]\tLoss: 0.993705\n","Train Epoch: 5 [25600/400000 (6%)]\tLoss: 0.959136\n","Train Epoch: 5 [32000/400000 (8%)]\tLoss: 0.963246\n","Train Epoch: 5 [38400/400000 (10%)]\tLoss: 0.963848\n","Train Epoch: 5 [44800/400000 (11%)]\tLoss: 0.941491\n","Train Epoch: 5 [51200/400000 (13%)]\tLoss: 0.936210\n","Train Epoch: 5 [57600/400000 (14%)]\tLoss: 0.968532\n","Train Epoch: 5 [64000/400000 (16%)]\tLoss: 0.987600\n","Train Epoch: 5 [70400/400000 (18%)]\tLoss: 0.997226\n","Train Epoch: 5 [76800/400000 (19%)]\tLoss: 0.936769\n","Train Epoch: 5 [83200/400000 (21%)]\tLoss: 0.977687\n","Train Epoch: 5 [89600/400000 (22%)]\tLoss: 1.041621\n","Train Epoch: 5 [96000/400000 (24%)]\tLoss: 0.969363\n","Train Epoch: 5 [102400/400000 (26%)]\tLoss: 0.984611\n","Train Epoch: 5 [108800/400000 (27%)]\tLoss: 0.952961\n","Train Epoch: 5 [115200/400000 (29%)]\tLoss: 0.967858\n","Train Epoch: 5 [121600/400000 (30%)]\tLoss: 0.987104\n","Train Epoch: 5 [128000/400000 (32%)]\tLoss: 0.999152\n","Train Epoch: 5 [134400/400000 (34%)]\tLoss: 0.991755\n","Train Epoch: 5 [140800/400000 (35%)]\tLoss: 1.005730\n","Train Epoch: 5 [147200/400000 (37%)]\tLoss: 0.982206\n","Train Epoch: 5 [153600/400000 (38%)]\tLoss: 0.974693\n","Train Epoch: 5 [160000/400000 (40%)]\tLoss: 0.973731\n","Train Epoch: 5 [166400/400000 (42%)]\tLoss: 0.948262\n","Train Epoch: 5 [172800/400000 (43%)]\tLoss: 0.974299\n","Train Epoch: 5 [179200/400000 (45%)]\tLoss: 0.976933\n","Train Epoch: 5 [185600/400000 (46%)]\tLoss: 0.982071\n","Train Epoch: 5 [192000/400000 (48%)]\tLoss: 0.954813\n","Train Epoch: 5 [198400/400000 (50%)]\tLoss: 1.006150\n","Train Epoch: 5 [204800/400000 (51%)]\tLoss: 0.944413\n","Train Epoch: 5 [211200/400000 (53%)]\tLoss: 0.999248\n","Train Epoch: 5 [217600/400000 (54%)]\tLoss: 0.950599\n","Train Epoch: 5 [224000/400000 (56%)]\tLoss: 0.897727\n","Train Epoch: 5 [230400/400000 (58%)]\tLoss: 0.997826\n","Train Epoch: 5 [236800/400000 (59%)]\tLoss: 0.992540\n","Train Epoch: 5 [243200/400000 (61%)]\tLoss: 0.985291\n","Train Epoch: 5 [249600/400000 (62%)]\tLoss: 0.985083\n","Train Epoch: 5 [256000/400000 (64%)]\tLoss: 0.989524\n","Train Epoch: 5 [262400/400000 (66%)]\tLoss: 0.981340\n","Train Epoch: 5 [268800/400000 (67%)]\tLoss: 0.999532\n","Train Epoch: 5 [275200/400000 (69%)]\tLoss: 0.962420\n","Train Epoch: 5 [281600/400000 (70%)]\tLoss: 0.974079\n","Train Epoch: 5 [288000/400000 (72%)]\tLoss: 0.932055\n","Train Epoch: 5 [294400/400000 (74%)]\tLoss: 0.928463\n","Train Epoch: 5 [300800/400000 (75%)]\tLoss: 0.975731\n","Train Epoch: 5 [307200/400000 (77%)]\tLoss: 0.950564\n","Train Epoch: 5 [313600/400000 (78%)]\tLoss: 0.958407\n","Train Epoch: 5 [320000/400000 (80%)]\tLoss: 1.023832\n","Train Epoch: 5 [326400/400000 (82%)]\tLoss: 0.951955\n","Train Epoch: 5 [332800/400000 (83%)]\tLoss: 0.947285\n","Train Epoch: 5 [339200/400000 (85%)]\tLoss: 0.963680\n","Train Epoch: 5 [345600/400000 (86%)]\tLoss: 1.020622\n","Train Epoch: 5 [352000/400000 (88%)]\tLoss: 0.926721\n","Train Epoch: 5 [358400/400000 (90%)]\tLoss: 0.953244\n","Train Epoch: 5 [364800/400000 (91%)]\tLoss: 0.945888\n","Train Epoch: 5 [371200/400000 (93%)]\tLoss: 1.000652\n","Train Epoch: 5 [377600/400000 (94%)]\tLoss: 0.935809\n","Train Epoch: 5 [384000/400000 (96%)]\tLoss: 0.939845\n","Train Epoch: 5 [390400/400000 (98%)]\tLoss: 0.915783\n","Train Epoch: 5 [396800/400000 (99%)]\tLoss: 0.984381\n","\n","Test set: Average loss: 0.0000, Accuracy: 51853/100000 (52%)\n","\n","Train Epoch: 6 [0/400000 (0%)]\tLoss: 0.988924\n","Train Epoch: 6 [6400/400000 (2%)]\tLoss: 0.940834\n","Train Epoch: 6 [12800/400000 (3%)]\tLoss: 0.990459\n","Train Epoch: 6 [19200/400000 (5%)]\tLoss: 0.978571\n","Train Epoch: 6 [25600/400000 (6%)]\tLoss: 1.003199\n","Train Epoch: 6 [32000/400000 (8%)]\tLoss: 0.978702\n","Train Epoch: 6 [38400/400000 (10%)]\tLoss: 0.922484\n","Train Epoch: 6 [44800/400000 (11%)]\tLoss: 0.929714\n","Train Epoch: 6 [51200/400000 (13%)]\tLoss: 0.975679\n","Train Epoch: 6 [57600/400000 (14%)]\tLoss: 0.880027\n","Train Epoch: 6 [64000/400000 (16%)]\tLoss: 0.917747\n","Train Epoch: 6 [70400/400000 (18%)]\tLoss: 0.964943\n","Train Epoch: 6 [76800/400000 (19%)]\tLoss: 0.944245\n","Train Epoch: 6 [83200/400000 (21%)]\tLoss: 0.980860\n","Train Epoch: 6 [89600/400000 (22%)]\tLoss: 0.974727\n","Train Epoch: 6 [96000/400000 (24%)]\tLoss: 0.932903\n","Train Epoch: 6 [102400/400000 (26%)]\tLoss: 0.953383\n","Train Epoch: 6 [108800/400000 (27%)]\tLoss: 0.982631\n","Train Epoch: 6 [115200/400000 (29%)]\tLoss: 0.925648\n","Train Epoch: 6 [121600/400000 (30%)]\tLoss: 0.962739\n","Train Epoch: 6 [128000/400000 (32%)]\tLoss: 0.982637\n","Train Epoch: 6 [134400/400000 (34%)]\tLoss: 0.940329\n","Train Epoch: 6 [140800/400000 (35%)]\tLoss: 0.990439\n","Train Epoch: 6 [147200/400000 (37%)]\tLoss: 0.940025\n","Train Epoch: 6 [153600/400000 (38%)]\tLoss: 0.954136\n","Train Epoch: 6 [160000/400000 (40%)]\tLoss: 0.948088\n","Train Epoch: 6 [166400/400000 (42%)]\tLoss: 1.024196\n","Train Epoch: 6 [172800/400000 (43%)]\tLoss: 1.012511\n","Train Epoch: 6 [179200/400000 (45%)]\tLoss: 0.941206\n","Train Epoch: 6 [185600/400000 (46%)]\tLoss: 1.014727\n","Train Epoch: 6 [192000/400000 (48%)]\tLoss: 0.946041\n","Train Epoch: 6 [198400/400000 (50%)]\tLoss: 0.936206\n","Train Epoch: 6 [204800/400000 (51%)]\tLoss: 0.963253\n","Train Epoch: 6 [211200/400000 (53%)]\tLoss: 0.978296\n","Train Epoch: 6 [217600/400000 (54%)]\tLoss: 0.969458\n","Train Epoch: 6 [224000/400000 (56%)]\tLoss: 0.968614\n","Train Epoch: 6 [230400/400000 (58%)]\tLoss: 0.994967\n","Train Epoch: 6 [236800/400000 (59%)]\tLoss: 0.987988\n","Train Epoch: 6 [243200/400000 (61%)]\tLoss: 0.945258\n","Train Epoch: 6 [249600/400000 (62%)]\tLoss: 0.976752\n","Train Epoch: 6 [256000/400000 (64%)]\tLoss: 0.901947\n","Train Epoch: 6 [262400/400000 (66%)]\tLoss: 0.976137\n","Train Epoch: 6 [268800/400000 (67%)]\tLoss: 0.987930\n","Train Epoch: 6 [275200/400000 (69%)]\tLoss: 0.902407\n","Train Epoch: 6 [281600/400000 (70%)]\tLoss: 0.991259\n","Train Epoch: 6 [288000/400000 (72%)]\tLoss: 0.922550\n","Train Epoch: 6 [294400/400000 (74%)]\tLoss: 0.999458\n","Train Epoch: 6 [300800/400000 (75%)]\tLoss: 0.931217\n","Train Epoch: 6 [307200/400000 (77%)]\tLoss: 0.991591\n","Train Epoch: 6 [313600/400000 (78%)]\tLoss: 0.922572\n","Train Epoch: 6 [320000/400000 (80%)]\tLoss: 0.944733\n","Train Epoch: 6 [326400/400000 (82%)]\tLoss: 0.934838\n","Train Epoch: 6 [332800/400000 (83%)]\tLoss: 0.946705\n","Train Epoch: 6 [339200/400000 (85%)]\tLoss: 0.984167\n","Train Epoch: 6 [345600/400000 (86%)]\tLoss: 0.960028\n","Train Epoch: 6 [352000/400000 (88%)]\tLoss: 0.954745\n","Train Epoch: 6 [358400/400000 (90%)]\tLoss: 0.931583\n","Train Epoch: 6 [364800/400000 (91%)]\tLoss: 0.968269\n","Train Epoch: 6 [371200/400000 (93%)]\tLoss: 0.966885\n","Train Epoch: 6 [377600/400000 (94%)]\tLoss: 1.053002\n","Train Epoch: 6 [384000/400000 (96%)]\tLoss: 0.995016\n","Train Epoch: 6 [390400/400000 (98%)]\tLoss: 0.951958\n","Train Epoch: 6 [396800/400000 (99%)]\tLoss: 0.924041\n","\n","Test set: Average loss: 0.0000, Accuracy: 52198/100000 (52%)\n","\n","Train Epoch: 7 [0/400000 (0%)]\tLoss: 0.947534\n","Train Epoch: 7 [6400/400000 (2%)]\tLoss: 0.985864\n","Train Epoch: 7 [12800/400000 (3%)]\tLoss: 0.925835\n","Train Epoch: 7 [19200/400000 (5%)]\tLoss: 0.978625\n","Train Epoch: 7 [25600/400000 (6%)]\tLoss: 1.009501\n","Train Epoch: 7 [32000/400000 (8%)]\tLoss: 0.961949\n","Train Epoch: 7 [38400/400000 (10%)]\tLoss: 0.965150\n","Train Epoch: 7 [44800/400000 (11%)]\tLoss: 0.946416\n","Train Epoch: 7 [51200/400000 (13%)]\tLoss: 0.913965\n","Train Epoch: 7 [57600/400000 (14%)]\tLoss: 0.957549\n","Train Epoch: 7 [64000/400000 (16%)]\tLoss: 1.019760\n","Train Epoch: 7 [70400/400000 (18%)]\tLoss: 0.934740\n","Train Epoch: 7 [76800/400000 (19%)]\tLoss: 1.004594\n","Train Epoch: 7 [83200/400000 (21%)]\tLoss: 0.992290\n","Train Epoch: 7 [89600/400000 (22%)]\tLoss: 0.894965\n","Train Epoch: 7 [96000/400000 (24%)]\tLoss: 0.946549\n","Train Epoch: 7 [102400/400000 (26%)]\tLoss: 0.955952\n","Train Epoch: 7 [108800/400000 (27%)]\tLoss: 0.994010\n","Train Epoch: 7 [115200/400000 (29%)]\tLoss: 0.969385\n","Train Epoch: 7 [121600/400000 (30%)]\tLoss: 0.976019\n","Train Epoch: 7 [128000/400000 (32%)]\tLoss: 1.005872\n","Train Epoch: 7 [134400/400000 (34%)]\tLoss: 0.965689\n","Train Epoch: 7 [140800/400000 (35%)]\tLoss: 0.967251\n","Train Epoch: 7 [147200/400000 (37%)]\tLoss: 0.947776\n","Train Epoch: 7 [153600/400000 (38%)]\tLoss: 0.955667\n","Train Epoch: 7 [160000/400000 (40%)]\tLoss: 0.982778\n","Train Epoch: 7 [166400/400000 (42%)]\tLoss: 0.912806\n","Train Epoch: 7 [172800/400000 (43%)]\tLoss: 0.948561\n","Train Epoch: 7 [179200/400000 (45%)]\tLoss: 0.984456\n","Train Epoch: 7 [185600/400000 (46%)]\tLoss: 0.971312\n","Train Epoch: 7 [192000/400000 (48%)]\tLoss: 1.004441\n","Train Epoch: 7 [198400/400000 (50%)]\tLoss: 0.985977\n","Train Epoch: 7 [204800/400000 (51%)]\tLoss: 0.981965\n","Train Epoch: 7 [211200/400000 (53%)]\tLoss: 0.963403\n","Train Epoch: 7 [217600/400000 (54%)]\tLoss: 0.963366\n","Train Epoch: 7 [224000/400000 (56%)]\tLoss: 0.987950\n","Train Epoch: 7 [230400/400000 (58%)]\tLoss: 0.947726\n","Train Epoch: 7 [236800/400000 (59%)]\tLoss: 0.936192\n","Train Epoch: 7 [243200/400000 (61%)]\tLoss: 0.919895\n","Train Epoch: 7 [249600/400000 (62%)]\tLoss: 0.927176\n","Train Epoch: 7 [256000/400000 (64%)]\tLoss: 0.968267\n","Train Epoch: 7 [262400/400000 (66%)]\tLoss: 0.993059\n","Train Epoch: 7 [268800/400000 (67%)]\tLoss: 0.953120\n","Train Epoch: 7 [275200/400000 (69%)]\tLoss: 0.944405\n","Train Epoch: 7 [281600/400000 (70%)]\tLoss: 0.975737\n","Train Epoch: 7 [288000/400000 (72%)]\tLoss: 0.993001\n","Train Epoch: 7 [294400/400000 (74%)]\tLoss: 0.919138\n","Train Epoch: 7 [300800/400000 (75%)]\tLoss: 0.972128\n","Train Epoch: 7 [307200/400000 (77%)]\tLoss: 0.932270\n","Train Epoch: 7 [313600/400000 (78%)]\tLoss: 0.935707\n","Train Epoch: 7 [320000/400000 (80%)]\tLoss: 0.904540\n","Train Epoch: 7 [326400/400000 (82%)]\tLoss: 0.952158\n","Train Epoch: 7 [332800/400000 (83%)]\tLoss: 0.962260\n","Train Epoch: 7 [339200/400000 (85%)]\tLoss: 0.974096\n","Train Epoch: 7 [345600/400000 (86%)]\tLoss: 0.953381\n","Train Epoch: 7 [352000/400000 (88%)]\tLoss: 0.967818\n","Train Epoch: 7 [358400/400000 (90%)]\tLoss: 0.977267\n","Train Epoch: 7 [364800/400000 (91%)]\tLoss: 0.958672\n","Train Epoch: 7 [371200/400000 (93%)]\tLoss: 0.997428\n","Train Epoch: 7 [377600/400000 (94%)]\tLoss: 0.988022\n","Train Epoch: 7 [384000/400000 (96%)]\tLoss: 0.894704\n","Train Epoch: 7 [390400/400000 (98%)]\tLoss: 0.947691\n","Train Epoch: 7 [396800/400000 (99%)]\tLoss: 0.992146\n","\n","Test set: Average loss: 0.0000, Accuracy: 51918/100000 (52%)\n","\n","Train Epoch: 8 [0/400000 (0%)]\tLoss: 0.964084\n","Train Epoch: 8 [6400/400000 (2%)]\tLoss: 0.990765\n","Train Epoch: 8 [12800/400000 (3%)]\tLoss: 1.006405\n","Train Epoch: 8 [19200/400000 (5%)]\tLoss: 0.994763\n","Train Epoch: 8 [25600/400000 (6%)]\tLoss: 0.938074\n","Train Epoch: 8 [32000/400000 (8%)]\tLoss: 0.946101\n","Train Epoch: 8 [38400/400000 (10%)]\tLoss: 0.980951\n","Train Epoch: 8 [44800/400000 (11%)]\tLoss: 1.028965\n","Train Epoch: 8 [51200/400000 (13%)]\tLoss: 0.936754\n","Train Epoch: 8 [57600/400000 (14%)]\tLoss: 0.980823\n","Train Epoch: 8 [64000/400000 (16%)]\tLoss: 0.940764\n","Train Epoch: 8 [70400/400000 (18%)]\tLoss: 0.973992\n","Train Epoch: 8 [76800/400000 (19%)]\tLoss: 0.971477\n","Train Epoch: 8 [83200/400000 (21%)]\tLoss: 0.943180\n","Train Epoch: 8 [89600/400000 (22%)]\tLoss: 0.968529\n","Train Epoch: 8 [96000/400000 (24%)]\tLoss: 0.919139\n","Train Epoch: 8 [102400/400000 (26%)]\tLoss: 1.007756\n","Train Epoch: 8 [108800/400000 (27%)]\tLoss: 0.949024\n","Train Epoch: 8 [115200/400000 (29%)]\tLoss: 0.960123\n","Train Epoch: 8 [121600/400000 (30%)]\tLoss: 0.962649\n","Train Epoch: 8 [128000/400000 (32%)]\tLoss: 0.955245\n","Train Epoch: 8 [134400/400000 (34%)]\tLoss: 0.959956\n","Train Epoch: 8 [140800/400000 (35%)]\tLoss: 0.995264\n","Train Epoch: 8 [147200/400000 (37%)]\tLoss: 1.034269\n","Train Epoch: 8 [153600/400000 (38%)]\tLoss: 0.998571\n","Train Epoch: 8 [160000/400000 (40%)]\tLoss: 0.942554\n","Train Epoch: 8 [166400/400000 (42%)]\tLoss: 0.974112\n","Train Epoch: 8 [172800/400000 (43%)]\tLoss: 0.950332\n","Train Epoch: 8 [179200/400000 (45%)]\tLoss: 0.920179\n","Train Epoch: 8 [185600/400000 (46%)]\tLoss: 0.988557\n","Train Epoch: 8 [192000/400000 (48%)]\tLoss: 0.998683\n","Train Epoch: 8 [198400/400000 (50%)]\tLoss: 0.916644\n","Train Epoch: 8 [204800/400000 (51%)]\tLoss: 0.967488\n","Train Epoch: 8 [211200/400000 (53%)]\tLoss: 0.937664\n","Train Epoch: 8 [217600/400000 (54%)]\tLoss: 0.951287\n","Train Epoch: 8 [224000/400000 (56%)]\tLoss: 0.880891\n","Train Epoch: 8 [230400/400000 (58%)]\tLoss: 0.970142\n","Train Epoch: 8 [236800/400000 (59%)]\tLoss: 0.983036\n","Train Epoch: 8 [243200/400000 (61%)]\tLoss: 0.964171\n","Train Epoch: 8 [249600/400000 (62%)]\tLoss: 0.975117\n","Train Epoch: 8 [256000/400000 (64%)]\tLoss: 0.979411\n","Train Epoch: 8 [262400/400000 (66%)]\tLoss: 0.888605\n","Train Epoch: 8 [268800/400000 (67%)]\tLoss: 0.957663\n","Train Epoch: 8 [275200/400000 (69%)]\tLoss: 0.966510\n","Train Epoch: 8 [281600/400000 (70%)]\tLoss: 0.967692\n","Train Epoch: 8 [288000/400000 (72%)]\tLoss: 0.972632\n","Train Epoch: 8 [294400/400000 (74%)]\tLoss: 0.996420\n","Train Epoch: 8 [300800/400000 (75%)]\tLoss: 1.002311\n","Train Epoch: 8 [307200/400000 (77%)]\tLoss: 0.965086\n","Train Epoch: 8 [313600/400000 (78%)]\tLoss: 0.962709\n","Train Epoch: 8 [320000/400000 (80%)]\tLoss: 0.944335\n","Train Epoch: 8 [326400/400000 (82%)]\tLoss: 0.940205\n","Train Epoch: 8 [332800/400000 (83%)]\tLoss: 1.014436\n","Train Epoch: 8 [339200/400000 (85%)]\tLoss: 0.949483\n","Train Epoch: 8 [345600/400000 (86%)]\tLoss: 0.937750\n","Train Epoch: 8 [352000/400000 (88%)]\tLoss: 0.956402\n","Train Epoch: 8 [358400/400000 (90%)]\tLoss: 0.949486\n","Train Epoch: 8 [364800/400000 (91%)]\tLoss: 0.962228\n","Train Epoch: 8 [371200/400000 (93%)]\tLoss: 0.984562\n","Train Epoch: 8 [377600/400000 (94%)]\tLoss: 0.951819\n","Train Epoch: 8 [384000/400000 (96%)]\tLoss: 0.990539\n","Train Epoch: 8 [390400/400000 (98%)]\tLoss: 0.909163\n","Train Epoch: 8 [396800/400000 (99%)]\tLoss: 0.989312\n","\n","Test set: Average loss: 0.0000, Accuracy: 51946/100000 (52%)\n","\n","Train Epoch: 9 [0/400000 (0%)]\tLoss: 0.991719\n","Train Epoch: 9 [6400/400000 (2%)]\tLoss: 0.972349\n","Train Epoch: 9 [12800/400000 (3%)]\tLoss: 0.936797\n","Train Epoch: 9 [19200/400000 (5%)]\tLoss: 0.930222\n","Train Epoch: 9 [25600/400000 (6%)]\tLoss: 1.013325\n","Train Epoch: 9 [32000/400000 (8%)]\tLoss: 0.971106\n","Train Epoch: 9 [38400/400000 (10%)]\tLoss: 0.902913\n","Train Epoch: 9 [44800/400000 (11%)]\tLoss: 1.015298\n","Train Epoch: 9 [51200/400000 (13%)]\tLoss: 0.973934\n","Train Epoch: 9 [57600/400000 (14%)]\tLoss: 1.007827\n","Train Epoch: 9 [64000/400000 (16%)]\tLoss: 0.997466\n","Train Epoch: 9 [70400/400000 (18%)]\tLoss: 0.971460\n","Train Epoch: 9 [76800/400000 (19%)]\tLoss: 0.976904\n","Train Epoch: 9 [83200/400000 (21%)]\tLoss: 0.942994\n","Train Epoch: 9 [89600/400000 (22%)]\tLoss: 0.941670\n","Train Epoch: 9 [96000/400000 (24%)]\tLoss: 0.971557\n","Train Epoch: 9 [102400/400000 (26%)]\tLoss: 1.022699\n","Train Epoch: 9 [108800/400000 (27%)]\tLoss: 0.931880\n","Train Epoch: 9 [115200/400000 (29%)]\tLoss: 0.935085\n","Train Epoch: 9 [121600/400000 (30%)]\tLoss: 0.927046\n","Train Epoch: 9 [128000/400000 (32%)]\tLoss: 0.933818\n","Train Epoch: 9 [134400/400000 (34%)]\tLoss: 0.940453\n","Train Epoch: 9 [140800/400000 (35%)]\tLoss: 0.979761\n","Train Epoch: 9 [147200/400000 (37%)]\tLoss: 0.966968\n","Train Epoch: 9 [153600/400000 (38%)]\tLoss: 0.986141\n","Train Epoch: 9 [160000/400000 (40%)]\tLoss: 0.973709\n","Train Epoch: 9 [166400/400000 (42%)]\tLoss: 0.993502\n","Train Epoch: 9 [172800/400000 (43%)]\tLoss: 0.990836\n","Train Epoch: 9 [179200/400000 (45%)]\tLoss: 0.953243\n","Train Epoch: 9 [185600/400000 (46%)]\tLoss: 0.929057\n","Train Epoch: 9 [192000/400000 (48%)]\tLoss: 0.979438\n","Train Epoch: 9 [198400/400000 (50%)]\tLoss: 0.941778\n","Train Epoch: 9 [204800/400000 (51%)]\tLoss: 0.962913\n","Train Epoch: 9 [211200/400000 (53%)]\tLoss: 1.023168\n","Train Epoch: 9 [217600/400000 (54%)]\tLoss: 0.965733\n","Train Epoch: 9 [224000/400000 (56%)]\tLoss: 0.946324\n","Train Epoch: 9 [230400/400000 (58%)]\tLoss: 0.979856\n","Train Epoch: 9 [236800/400000 (59%)]\tLoss: 0.989229\n","Train Epoch: 9 [243200/400000 (61%)]\tLoss: 0.953086\n","Train Epoch: 9 [249600/400000 (62%)]\tLoss: 0.987484\n","Train Epoch: 9 [256000/400000 (64%)]\tLoss: 0.964077\n","Train Epoch: 9 [262400/400000 (66%)]\tLoss: 0.957143\n","Train Epoch: 9 [268800/400000 (67%)]\tLoss: 0.955414\n","Train Epoch: 9 [275200/400000 (69%)]\tLoss: 0.944987\n","Train Epoch: 9 [281600/400000 (70%)]\tLoss: 0.973806\n","Train Epoch: 9 [288000/400000 (72%)]\tLoss: 0.971893\n","Train Epoch: 9 [294400/400000 (74%)]\tLoss: 0.993837\n","Train Epoch: 9 [300800/400000 (75%)]\tLoss: 0.953405\n","Train Epoch: 9 [307200/400000 (77%)]\tLoss: 0.973190\n","Train Epoch: 9 [313600/400000 (78%)]\tLoss: 0.972026\n","Train Epoch: 9 [320000/400000 (80%)]\tLoss: 0.989887\n","Train Epoch: 9 [326400/400000 (82%)]\tLoss: 0.913068\n","Train Epoch: 9 [332800/400000 (83%)]\tLoss: 0.995335\n","Train Epoch: 9 [339200/400000 (85%)]\tLoss: 0.967460\n","Train Epoch: 9 [345600/400000 (86%)]\tLoss: 0.937251\n","Train Epoch: 9 [352000/400000 (88%)]\tLoss: 0.970185\n","Train Epoch: 9 [358400/400000 (90%)]\tLoss: 0.944468\n","Train Epoch: 9 [364800/400000 (91%)]\tLoss: 0.990956\n","Train Epoch: 9 [371200/400000 (93%)]\tLoss: 0.954853\n","Train Epoch: 9 [377600/400000 (94%)]\tLoss: 0.999434\n","Train Epoch: 9 [384000/400000 (96%)]\tLoss: 1.000939\n","Train Epoch: 9 [390400/400000 (98%)]\tLoss: 0.960999\n","Train Epoch: 9 [396800/400000 (99%)]\tLoss: 0.943982\n","\n","Test set: Average loss: 0.0000, Accuracy: 51919/100000 (52%)\n","\n","Train Epoch: 10 [0/400000 (0%)]\tLoss: 0.932658\n","Train Epoch: 10 [6400/400000 (2%)]\tLoss: 0.945304\n","Train Epoch: 10 [12800/400000 (3%)]\tLoss: 0.955770\n","Train Epoch: 10 [19200/400000 (5%)]\tLoss: 0.977108\n","Train Epoch: 10 [25600/400000 (6%)]\tLoss: 0.965265\n","Train Epoch: 10 [32000/400000 (8%)]\tLoss: 0.923929\n","Train Epoch: 10 [38400/400000 (10%)]\tLoss: 1.002063\n","Train Epoch: 10 [44800/400000 (11%)]\tLoss: 0.963373\n","Train Epoch: 10 [51200/400000 (13%)]\tLoss: 0.971952\n","Train Epoch: 10 [57600/400000 (14%)]\tLoss: 0.940221\n","Train Epoch: 10 [64000/400000 (16%)]\tLoss: 0.941488\n","Train Epoch: 10 [70400/400000 (18%)]\tLoss: 0.906189\n","Train Epoch: 10 [76800/400000 (19%)]\tLoss: 0.950377\n","Train Epoch: 10 [83200/400000 (21%)]\tLoss: 0.962726\n","Train Epoch: 10 [89600/400000 (22%)]\tLoss: 0.939281\n","Train Epoch: 10 [96000/400000 (24%)]\tLoss: 0.993061\n","Train Epoch: 10 [102400/400000 (26%)]\tLoss: 0.923635\n","Train Epoch: 10 [108800/400000 (27%)]\tLoss: 1.014460\n","Train Epoch: 10 [115200/400000 (29%)]\tLoss: 0.963157\n","Train Epoch: 10 [121600/400000 (30%)]\tLoss: 0.984523\n","Train Epoch: 10 [128000/400000 (32%)]\tLoss: 1.007398\n","Train Epoch: 10 [134400/400000 (34%)]\tLoss: 0.982109\n","Train Epoch: 10 [140800/400000 (35%)]\tLoss: 0.974886\n","Train Epoch: 10 [147200/400000 (37%)]\tLoss: 0.954697\n","Train Epoch: 10 [153600/400000 (38%)]\tLoss: 0.894008\n","Train Epoch: 10 [160000/400000 (40%)]\tLoss: 0.927576\n","Train Epoch: 10 [166400/400000 (42%)]\tLoss: 0.960857\n","Train Epoch: 10 [172800/400000 (43%)]\tLoss: 0.973370\n","Train Epoch: 10 [179200/400000 (45%)]\tLoss: 0.915559\n","Train Epoch: 10 [185600/400000 (46%)]\tLoss: 1.019630\n","Train Epoch: 10 [192000/400000 (48%)]\tLoss: 0.929993\n","Train Epoch: 10 [198400/400000 (50%)]\tLoss: 1.015669\n","Train Epoch: 10 [204800/400000 (51%)]\tLoss: 0.977122\n","Train Epoch: 10 [211200/400000 (53%)]\tLoss: 0.973296\n","Train Epoch: 10 [217600/400000 (54%)]\tLoss: 0.958381\n","Train Epoch: 10 [224000/400000 (56%)]\tLoss: 0.924858\n","Train Epoch: 10 [230400/400000 (58%)]\tLoss: 0.972453\n","Train Epoch: 10 [236800/400000 (59%)]\tLoss: 0.966414\n","Train Epoch: 10 [243200/400000 (61%)]\tLoss: 0.973074\n","Train Epoch: 10 [249600/400000 (62%)]\tLoss: 0.997904\n","Train Epoch: 10 [256000/400000 (64%)]\tLoss: 0.998626\n","Train Epoch: 10 [262400/400000 (66%)]\tLoss: 0.922075\n","Train Epoch: 10 [268800/400000 (67%)]\tLoss: 0.961838\n","Train Epoch: 10 [275200/400000 (69%)]\tLoss: 0.993755\n","Train Epoch: 10 [281600/400000 (70%)]\tLoss: 0.994326\n","Train Epoch: 10 [288000/400000 (72%)]\tLoss: 0.959309\n","Train Epoch: 10 [294400/400000 (74%)]\tLoss: 0.918989\n","Train Epoch: 10 [300800/400000 (75%)]\tLoss: 0.970601\n","Train Epoch: 10 [307200/400000 (77%)]\tLoss: 0.986103\n","Train Epoch: 10 [313600/400000 (78%)]\tLoss: 0.970875\n","Train Epoch: 10 [320000/400000 (80%)]\tLoss: 0.905208\n","Train Epoch: 10 [326400/400000 (82%)]\tLoss: 0.956460\n","Train Epoch: 10 [332800/400000 (83%)]\tLoss: 0.952842\n","Train Epoch: 10 [339200/400000 (85%)]\tLoss: 0.952575\n","Train Epoch: 10 [345600/400000 (86%)]\tLoss: 1.026871\n","Train Epoch: 10 [352000/400000 (88%)]\tLoss: 0.988704\n","Train Epoch: 10 [358400/400000 (90%)]\tLoss: 0.916356\n","Train Epoch: 10 [364800/400000 (91%)]\tLoss: 0.973519\n","Train Epoch: 10 [371200/400000 (93%)]\tLoss: 0.970211\n","Train Epoch: 10 [377600/400000 (94%)]\tLoss: 0.933861\n","Train Epoch: 10 [384000/400000 (96%)]\tLoss: 1.032714\n","Train Epoch: 10 [390400/400000 (98%)]\tLoss: 0.925749\n","Train Epoch: 10 [396800/400000 (99%)]\tLoss: 1.010625\n","\n","Test set: Average loss: 0.0000, Accuracy: 51880/100000 (52%)\n","\n","52456\n"]}],"source":["torch.manual_seed(args.seed)\n","\n","device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n","model = Net().to(device)\n","\n","for param_tensor in model.state_dict():\n","        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n","\n","#Form training and testing dataset\n","optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n","\n","train_dataset = torch.utils.data.TensorDataset(train_vectors, train_labels)\n","test_dataset = torch.utils.data.TensorDataset(test_vectors, test_labels)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=640, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=640, shuffle=False)\n","scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n","\n","#Model training\n","ACC = 0\n","for epoch in range(1, args.epochs + 1):\n","    train(args, model, device, train_loader, optimizer, epoch)\n","    ACC_ = test(model, device, test_loader)\n","    if ACC_>ACC or ACC_ == ACC:\n","        ACC = ACC_\n","        torch.save(model.state_dict(), \"Baseline_MLP.pt\")\n","\n","    scheduler.step()\n","\n","print(ACC)\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1UmHMTfjrXDpqwqA87uFRnupEQOFfcgsb","timestamp":1691994937982}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}