{"cells":[{"cell_type":"markdown","source":["# Note:\n","- This notebook file may contain methods or algorithms that are NOT covered by the teaching content of BT4222 and hence will not be assessed in your midterm exam.\n","- It serves to increase your exposure in depth and breath to the practical methods in addressing the specific project topic. We believe it will be helpful for your current project and also your future internship endeavors."],"metadata":{"id":"0iLaNrCaiROY"}},{"cell_type":"markdown","metadata":{"id":"UXYWxqcLTrsT"},"source":["# **Import Library**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"STdFlfuXL6Xz","outputId":"c5a81dba-e810-4eb3-9946-7352adaeeda6","executionInfo":{"status":"ok","timestamp":1691931348558,"user_tz":-480,"elapsed":16703,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import nltk\n","from nltk.corpus import stopwords\n","import string\n","import re\n","import seaborn as sns\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from torch.utils.data import TensorDataset, DataLoader\n","from nltk.tokenize import word_tokenize\n","import gensim\n","from gensim.models import Word2Vec\n","from gensim.models.keyedvectors import KeyedVectors\n","from nltk.tokenize import word_tokenize\n","import pandas as pd\n","import numpy as np\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"markdown","metadata":{"id":"-98wEEiETkWO"},"source":["# **Word Preprocessing**\n","Here, we show the change of first sentence as an example:\n","\n","Origin: I wanted some pizza...\n","\n","After moving the stop words: wanted pizza looking outside ...\n","\n","After tokenizing the text: ['wanted', 'pizza', 'looking', 'outside', 'box', 'found', 'local', 'pizzeria'....]\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcaxvGyjMLef","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691931867244,"user_tz":-480,"elapsed":460719,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}},"outputId":"5a89df37-2e8c-4eb6-936b-be6313f9f09e"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1pvDQhnGgeLWjqkHFxc3G0rkGpv6T3VN3\n","To: /content/train_dataset.csv\n","100%|██████████| 256M/256M [00:05<00:00, 50.8MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1LtTRhyhYAcXY5Yh_p8TdR_4LOvLguvZG\n","To: /content/test_dataset.csv\n","100%|██████████| 63.8M/63.8M [00:02<00:00, 29.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["First 5 sentences and their label                                                 text  stars\n","0  I wanted some pizza...but I was looking outsid...    3.0\n","1  Buy 3 get one free bang bang shrimp tacos - if...    5.0\n","2  Here on a business trip, ordered pizza. \\n\\nTh...    1.0\n","3  It's ok got the pho ga. The  broth was kinda o...    3.0\n","4  Stopped by during lunch time and it was obviou...    4.0\n","First 5 sentences and their label after cleaning                                                 text  stars\n","0  wanted pizza looking outside box found local p...    3.0\n","1  buy get one free bang bang shrimp tacos ya kno...    5.0\n","2  business trip ordered pizza said would minutes...    1.0\n","3  ok got pho ga broth kinda light side flavorful...    3.0\n","4  stopped lunch time obviously packed wanted try...    4.0\n","The first sentence ['wanted', 'pizza', 'looking', 'outside', 'box', 'found', 'local', 'pizzeria', 'well', 'love', 'supporting', 'places', 'like', 'buffet', 'night', 'really', 'wanted', 'call', 'ahead', 'take', 'pie', 'go', 'ended', 'ordering', 'garlic', 'chicken', 'extreme', 'combo', 'anchovies', 'also', 'picked', 'order', 'wings', 'prior', 'arriving', 'bought', 'yelp', 'deal', 'saved', 'us', 'bucks', 'walked', 'place', 'super', 'packed', 'glanced', 'buffet', 'area', 'right', 'next', 'register', 'saw', 'pizzas', 'salad', 'warmer', 'full', 'wings', 'anyhoo', 'went', 'register', 'gave', 'yelp', 'deal', 'paid', 'way', 'everyone', 'behind', 'counter', 'working', 'quickly', 'pump', 'pizzas', 'pizza', 'ok', 'looked', 'tasted', 'like', 'pizza', 'round', 'table', 'crust', 'thin', 'similar', 'toppings', 'also', 'reminded', 'wow', 'factor', 'oh', 'also', 'put', 'many', 'large', 'anchovies', 'pizza', 'took', 'away', 'rest', 'toppings', 'anchovies', 'meat', 'ended', 'taking', 'order', 'take', 'bake', 'knock', 'couple', 'bucks', 'think', 'try', 'next', 'time', 'supporting', 'local', 'places', 'looking', 'forward', 'going', 'back', 'hopefully', 'updating', 'review', 'better']\n"]}],"source":["####################################\n","import gdown\n","# load data\n","file_id = '1pvDQhnGgeLWjqkHFxc3G0rkGpv6T3VN3'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'train_dataset.csv'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1LtTRhyhYAcXY5Yh_p8TdR_4LOvLguvZG'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'test_dataset.csv'\n","gdown.download(url, output, quiet=False)\n","#train_dataset.csv and test_dataset.csv are subsets of Yelp dataset https://www.yelp.com/dataset\n","\n","train_data = pd.read_csv('train_dataset.csv')\n","test_data = pd.read_csv('test_dataset.csv')\n","print(\"First 5 sentences and their label\",train_data.head())\n","# get the stopword\n","stop_words = set(stopwords.words('english'))\n","\n","def clean_text(text):\n","    #Converts all characters in text to lowercase.\n","    text = text.lower()\n","\n","    #converts all characters in text to lowercase\n","    #replace all non-word characters (characters that are not a letter, digit, or underscore) in text with a space.\n","    text = re.sub(r'\\W', ' ', text)\n","    text = re.sub(r'\\d', '', text)\n","\n","    words = word_tokenize(text)\n","    #split the text into individual words.\n","\n","    words = [word for word in words if word not in stop_words]\n","    return ' '.join(words)\n","\n","# Apply the clean_text function to the 'text' column of the training and testing datasets\n","train_data['text'] = train_data['text'].apply(clean_text)\n","print(\"First 5 sentences and their label after cleaning\",train_data.head())\n","test_data['text'] = test_data['text'].apply(clean_text)\n","\n","##Tokenize the 'text' column of the training and testing datasets and convert to a list\n","train_sentences = train_data['text'].apply(word_tokenize).tolist()\n","print(\"The first sentence\", train_sentences[0])\n","\n","test_sentences = test_data['text'].apply(word_tokenize).tolist()\n"]},{"cell_type":"markdown","metadata":{"id":"y2tLMOBDYdT7"},"source":["# **Word2Vec embedding**\n","Here, we show how to embedding a sentence as a vector that can be sent to different kinds of models like CNN ot LSTM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRYHsopyMYYU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691931971353,"user_tz":-480,"elapsed":98700,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}},"outputId":"24b46425-df87-43df-fa69-31181625cc7c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1iIZEpngke06CvLXGjyuO2XYn9mOiEZ1M\n","To: /content/model_in_vector_format.txt\n","100%|██████████| 82.5M/82.5M [00:04<00:00, 20.5MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["vector of the first training sentence: [ 0.18806686 -0.81407547  0.2731794  -0.19380964 -0.9803709   1.2326044\n"," -0.31043512 -1.4522214   1.2036858  -0.2991813   0.52706    -0.6913312\n","  0.01106239  0.10594404  0.33188528  0.14175086 -0.48426813  0.86514705\n","  1.0458082   0.87707883 -0.09251958  0.15961069 -1.7640945   0.68583244\n"," -0.61002684  0.10527007  0.8128444   0.09110551  0.4896992  -0.651426\n"," -0.2042806   0.43941516  0.20847556  0.12768184  0.517038   -0.7411599\n"," -0.21149275 -0.11730701  0.19832464  1.0790299  -1.0857916   0.43405077\n"," -0.42623082 -0.68324167 -1.0194739   0.9581027   0.16632354 -0.33166024\n","  0.06498344 -0.72173446]\n"]}],"source":["\n","\n","\n","train=False\n","\n","if train:\n","    # train Word2Vec model\n","    model = Word2Vec(sentences=train_sentences, vector_size=50, window=5, min_count=1, workers=4)\n","    model.save(\"word2vec_model_all_yours.bin\")\n","    wv =model.wv\n","else:\n","    #or you can directly use the trained model\n","    file_id = '1iIZEpngke06CvLXGjyuO2XYn9mOiEZ1M'\n","    url = f'https://drive.google.com/uc?id={file_id}'\n","    output = 'model_in_vector_format.txt'\n","    gdown.download(url, output, quiet=False)\n","    wv = KeyedVectors.load_word2vec_format('model_in_vector_format.txt', binary=False)\n","\n","\n","def get_sentence_vectors(sentences):\n","    vectors = []\n","    for sentence in sentences:\n","        #This line creates a list of word vectors for each word in the sentence that is in the Word2Vec model's vocabulary.\n","        sentence_vectors = [wv[word] for word in sentence if word in wv]\n","        if len(sentence_vectors) == 0:\n","            vectors.append([0] * 50)  # If the sentence doesn't have any words that are in\n","            # the Word2Vec model's vocabulary, the sentence is represented by a vector of 50 zeros.\n","        else:\n","            vectors.append(np.mean(sentence_vectors, axis=0))  # Otherwise, the sentence vector is the average\n","            # of its word vectors. This vector is then added to the list of sentence vectors.\n","    return vectors\n","\n","train_vectors = get_sentence_vectors(train_sentences)\n","print(\"vector of the first training sentence:\", train_vectors[0])\n","\n","test_vectors = get_sentence_vectors(test_sentences)"]},{"cell_type":"markdown","source":["# **Creating training and testing dataset**"],"metadata":{"id":"9_P_faMxwpfP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wlKYHWEMdwi","executionInfo":{"status":"ok","timestamp":1691932084850,"user_tz":-480,"elapsed":1260,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3e66b8ff-ab5b-49df-9ca6-6a7c8229e62a"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-b98bd721c745>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_vectors = torch.tensor(train_vectors)\n"]},{"output_type":"stream","name":"stdout","text":["shape of training data torch.Size([400000, 1, 50]) shape of testing data torch.Size([100000, 1, 50])\n"]}],"source":["# turn to tensor\n","train_vectors = torch.tensor(train_vectors)\n","#Reshape the train_vectors tensor. The value -1 is used to infer the batch size automatically,\n","#while (1,50) specifies that the input size is 50 with a single channel.\"\n","train_vectors = train_vectors.reshape(-1,1,50)\n","test_vectors = torch.tensor(test_vectors)\n","test_vectors = test_vectors.reshape(-1,1,50)\n","print(\"shape of training data\", train_vectors.shape, \"shape of testing data\",test_vectors.shape)\n","# get the label\n","train_labels = torch.tensor(train_data['stars'].values)\n","test_labels = torch.tensor(test_data['stars'].values)\n","#save the training dataset as 'train_vectors.pt' and 'train_labels.pt'\n","torch.save(train_vectors, 'train_vectors_yours.pt')\n","torch.save(train_labels, 'train_labels_yours.pt')\n","#save the testing dataset as 'test_vectors.pt' and 'test_labels.pt'\n","torch.save(test_vectors, 'test_vectors_yours.pt')\n","torch.save(test_labels, 'test_labels_yours.pt')"]}],"metadata":{"colab":{"provenance":[{"file_id":"1YVSSZt4ZkntLvhZ8N1dXLK9vt4FXkhtH","timestamp":1691935266118}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}