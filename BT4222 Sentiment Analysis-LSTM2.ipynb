{"cells":[{"cell_type":"markdown","source":["# Note:\n","- This notebook file may contain methods or algorithms that are NOT covered by the teaching content of BT4222 and hence will not be assessed in your midterm exam.\n","- It serves to increase your exposure in depth and breath to the practical methods in addressing the specific project topic. We believe it will be helpful for your current project and also your future internship endeavors."],"metadata":{"id":"jIePQnMtjhrr"}},{"cell_type":"markdown","metadata":{"id":"UXYWxqcLTrsT"},"source":["# **Import Library**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STdFlfuXL6Xz"},"outputs":[],"source":["import numpy as np # linear algebra\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR"]},{"cell_type":"markdown","metadata":{"id":"-98wEEiETkWO"},"source":["# **Define Network Structure**\n","Here, I use a 2-layer LSTM for classification, the network structure is as followed:\n","\n","```\n","bi_lstm1.weight_ih_l0    torch.Size([512, 50])\n","bi_lstm1.weight_hh_l0    torch.Size([512, 128])\n","bi_lstm1.bias_ih_l0      torch.Size([512])\n","bi_lstm1.bias_hh_l0      torch.Size([512])\n","bi_lstm2.weight_ih_l0    torch.Size([512, 128])\n","bi_lstm2.weight_hh_l0    torch.Size([512, 128])\n","bi_lstm2.bias_ih_l0      torch.Size([512])\n","bi_lstm2.bias_hh_l0      torch.Size([512])\n","fc1.weight       torch.Size([5, 128])\n","fc1.bias         torch.Size([5])\n","```\n","\n","self.bi_lstm1 = nn.LSTM(input_size=50, hidden_size=128, num_layers=1, batch_first=True, bidirectional=False): The first layer is an LSTM layer that takes an input with 50 features and outputs 128 features. The num_layers=1 parameter means this LSTM has only one layer. batch_first=True means the input and output tensors are provided as (batch_size, sequence_length, num_features). The LSTM is unidirectional, meaning that it processes the sequence data in one direction.\n","\n","x = torch.flatten(x, 1): Before the output is passed to the LSTM layers, it is flattened from a 3D tensor to a 2D tensor. In this case,  I am flattening the sequence_length and num_features dimensions because the sequence length is just 1. However, Normally, for LSTM, we use sequence data with sequence_length > 1 and we do not flatten it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcaxvGyjMLef"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()  # Calls the constructor of the parent class nn.Module.\n","\n","        # Define a LSTM (Long Short-Term Memory) layer. It takes inputs of size 50, outputs of size 128, 1 layer, processes batches first, and is unidirectional.\n","        self.bi_lstm1 = nn.LSTM(input_size=50, hidden_size=128, num_layers=1, batch_first=True, bidirectional=False)\n","\n","        # Define a second LSTM layer. Takes inputs of size 128 (from the previous LSTM layer), outputs of size 128, 1 layer, processes batches first, and is unidirectional.\n","        self.bi_lstm2 = nn.LSTM(input_size=128, hidden_size=128, num_layers=1, batch_first=True, bidirectional=False)\n","\n","        # A fully connected (linear) layer that takes inputs of size 128 (from the previous LSTM layer) and outputs 5 nodes.\n","        self.fc1 = nn.Linear(128, 5, bias=True)\n","\n","    def forward(self, x):\n","        #x = torch.flatten(x, 1)\n","        # Flattens the input. Because for lstm here, we use seq of lenth 1. we can just use a 2d vector as the input of nn.LSTM\n","        x = torch.flatten(x, 1)\n","        x, _ = self.bi_lstm1(x)\n","        # Pass the input through the first LSTM layer.\n","        x, _ = self.bi_lstm2(x)\n","        # Pass the output from the previous LSTM layer through the second LSTM layer.、\n","        x= self.fc1(x)\n","\n","        # Pass the output from the previous LSTM layer through the fully connected layer.\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"y2tLMOBDYdT7"},"source":["# **Training and Testing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRYHsopyMYYU"},"outputs":[],"source":["def train(args, model, device, train_loader, optimizer, epoch):\n","    model.train()  # Set the model to training mode\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):  # Loop over each batch from the training set\n","        data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n","\n","        target = target-1  # Adjust the target values (Moving 1-5 to 0-4  for easy training)\n","        target = target.long()  # Make sure that target data is long type (necessary for loss function)\n","\n","        optimizer.zero_grad()  # Clear gradients from the previous training step\n","        output = model(data)  # Run forward pass (model predictions)\n","\n","        loss = F.cross_entropy(output, target)  # Calculate the loss between the output and target\n","        loss.backward()  # Perform backpropagation (calculate gradients of loss w.r.t. parameters)\n","        optimizer.step()  # Update the model parameters\n","\n","        if batch_idx % args.log_interval == 0:  # Print log info for specified interval\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.item()))\n","\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()  # Set the model to evaluation mode\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n","        for data, target in test_loader:  # Loop over each batch from the testing set\n","            data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n","            target = target-1  # Adjust the target values\n","            output = model(data)  # Run forward pass (model predictions)\n","            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability as the predicted output\n","            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n","\n","    test_loss /= len(test_loader.dataset)  # Calculate the average loss\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n","    return correct  # Return the number of correctly classified samples\n"]},{"cell_type":"markdown","metadata":{"id":"vafhcsv9Yiql"},"source":["# **Hyperparameter**\n","\n","1. epochs: The number of times the entire dataset is passed forward and backward through the neural network.\n","\n","2. lr: Learning rate, which determines the step size at each iteration while moving towards a minimum in the loss function.\n","\n","3. use_cuda: A boolean flag indicating whether to use CUDA (NVIDIA's parallel computing platform and API) for computations. This would be set to True if you want to utilize GPU acceleration.\n","\n","4. gamma: Typically used in learning rate scheduling. It's a factor by which the learning rate is reduced at certain intervals or when certain conditions are met.\n","\n","5. log_interval: The interval in terms of batches during training.\n","\n","6. seed: A seed value for random number generators to ensure reproducibility of results.\n","\n","For simple networks and small datasets, we typically set the learning rate to 1, the number of epochs to 10 and gamma to 0.7 for model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wlKYHWEMdwi"},"outputs":[],"source":["class Args:\n","  epochs = 10\n","  lr = 1.0\n","  use_cuda=False\n","  gamma = 0.7\n","  log_interval = 10\n","  seed = 1\n","\n","args = Args()"]},{"cell_type":"markdown","metadata":{"id":"HpgnI9XQR8x5"},"source":["# **Load Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"niXJRCXUR9Uh","outputId":"f85699eb-121c-43bb-a3ca-3d6bb2be0188","executionInfo":{"status":"ok","timestamp":1691929307079,"user_tz":-480,"elapsed":11198,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1CCIfElCaURQbuYvHZiL445UQIRzmmuM7\n","To: /content/train_vectors.pt\n","100%|██████████| 80.0M/80.0M [00:01<00:00, 58.6MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1bwkg7XdmH6Mkp_tkAakCbxJMWNAXJU43\n","To: /content/train_labels.pt\n","100%|██████████| 3.20M/3.20M [00:00<00:00, 151MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1fprUkqC9Qb-y1eDRZt0gA4-4gS941TUo\n","To: /content/test_vectors.pt\n","100%|██████████| 20.0M/20.0M [00:00<00:00, 137MB/s] \n","Downloading...\n","From: https://drive.google.com/uc?id=1VwOqpW7DZPhqAGDrreVwhtzCB2lUc_LD\n","To: /content/test_labels.pt\n","100%|██████████| 801k/801k [00:00<00:00, 92.6MB/s]\n"]}],"source":["from google.colab import drive\n","\n","import gdown\n","\n","file_id = '1CCIfElCaURQbuYvHZiL445UQIRzmmuM7'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'train_vectors.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1bwkg7XdmH6Mkp_tkAakCbxJMWNAXJU43'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'train_labels.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1fprUkqC9Qb-y1eDRZt0gA4-4gS941TUo'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'test_vectors.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1VwOqpW7DZPhqAGDrreVwhtzCB2lUc_LD'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'test_labels.pt'\n","gdown.download(url, output, quiet=False)\n","\n","train_vectors = torch.load('train_vectors.pt')\n","train_labels = torch.load('train_labels.pt')\n","test_vectors = torch.load('test_vectors.pt')\n","test_labels = torch.load('test_labels.pt')\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x_jwrzWbbENR"},"source":["# **Start training and testing**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPYD-LvcMi6w","outputId":"94f59507-2e2c-4297-9d65-afac14fe8ee7","executionInfo":{"status":"ok","timestamp":1691929639545,"user_tz":-480,"elapsed":327371,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["bi_lstm1.weight_ih_l0 \t torch.Size([512, 50])\n","bi_lstm1.weight_hh_l0 \t torch.Size([512, 128])\n","bi_lstm1.bias_ih_l0 \t torch.Size([512])\n","bi_lstm1.bias_hh_l0 \t torch.Size([512])\n","bi_lstm2.weight_ih_l0 \t torch.Size([512, 128])\n","bi_lstm2.weight_hh_l0 \t torch.Size([512, 128])\n","bi_lstm2.bias_ih_l0 \t torch.Size([512])\n","bi_lstm2.bias_hh_l0 \t torch.Size([512])\n","fc1.weight \t torch.Size([5, 128])\n","fc1.bias \t torch.Size([5])\n","Train Epoch: 1 [0/400000 (0%)]\tLoss: 1.611715\n","Train Epoch: 1 [6400/400000 (2%)]\tLoss: 1.605752\n","Train Epoch: 1 [12800/400000 (3%)]\tLoss: 1.601666\n","Train Epoch: 1 [19200/400000 (5%)]\tLoss: 1.594738\n","Train Epoch: 1 [25600/400000 (6%)]\tLoss: 1.582746\n","Train Epoch: 1 [32000/400000 (8%)]\tLoss: 1.553567\n","Train Epoch: 1 [38400/400000 (10%)]\tLoss: 1.525949\n","Train Epoch: 1 [44800/400000 (11%)]\tLoss: 1.470039\n","Train Epoch: 1 [51200/400000 (13%)]\tLoss: 1.414804\n","Train Epoch: 1 [57600/400000 (14%)]\tLoss: 1.321380\n","Train Epoch: 1 [64000/400000 (16%)]\tLoss: 1.295945\n","Train Epoch: 1 [70400/400000 (18%)]\tLoss: 1.260755\n","Train Epoch: 1 [76800/400000 (19%)]\tLoss: 1.222080\n","Train Epoch: 1 [83200/400000 (21%)]\tLoss: 1.266532\n","Train Epoch: 1 [89600/400000 (22%)]\tLoss: 1.147008\n","Train Epoch: 1 [96000/400000 (24%)]\tLoss: 1.175035\n","Train Epoch: 1 [102400/400000 (26%)]\tLoss: 1.116957\n","Train Epoch: 1 [108800/400000 (27%)]\tLoss: 1.168474\n","Train Epoch: 1 [115200/400000 (29%)]\tLoss: 1.171700\n","Train Epoch: 1 [121600/400000 (30%)]\tLoss: 1.109406\n","Train Epoch: 1 [128000/400000 (32%)]\tLoss: 1.118342\n","Train Epoch: 1 [134400/400000 (34%)]\tLoss: 1.066705\n","Train Epoch: 1 [140800/400000 (35%)]\tLoss: 1.104395\n","Train Epoch: 1 [147200/400000 (37%)]\tLoss: 1.134498\n","Train Epoch: 1 [153600/400000 (38%)]\tLoss: 1.093000\n","Train Epoch: 1 [160000/400000 (40%)]\tLoss: 1.078468\n","Train Epoch: 1 [166400/400000 (42%)]\tLoss: 1.017504\n","Train Epoch: 1 [172800/400000 (43%)]\tLoss: 1.082135\n","Train Epoch: 1 [179200/400000 (45%)]\tLoss: 1.105724\n","Train Epoch: 1 [185600/400000 (46%)]\tLoss: 1.063622\n","Train Epoch: 1 [192000/400000 (48%)]\tLoss: 1.023678\n","Train Epoch: 1 [198400/400000 (50%)]\tLoss: 1.025085\n","Train Epoch: 1 [204800/400000 (51%)]\tLoss: 1.066005\n","Train Epoch: 1 [211200/400000 (53%)]\tLoss: 1.006735\n","Train Epoch: 1 [217600/400000 (54%)]\tLoss: 1.032997\n","Train Epoch: 1 [224000/400000 (56%)]\tLoss: 1.066806\n","Train Epoch: 1 [230400/400000 (58%)]\tLoss: 1.016046\n","Train Epoch: 1 [236800/400000 (59%)]\tLoss: 1.064338\n","Train Epoch: 1 [243200/400000 (61%)]\tLoss: 1.027746\n","Train Epoch: 1 [249600/400000 (62%)]\tLoss: 1.038761\n","Train Epoch: 1 [256000/400000 (64%)]\tLoss: 1.016202\n","Train Epoch: 1 [262400/400000 (66%)]\tLoss: 1.021824\n","Train Epoch: 1 [268800/400000 (67%)]\tLoss: 1.013040\n","Train Epoch: 1 [275200/400000 (69%)]\tLoss: 1.055268\n","Train Epoch: 1 [281600/400000 (70%)]\tLoss: 1.011352\n","Train Epoch: 1 [288000/400000 (72%)]\tLoss: 1.078195\n","Train Epoch: 1 [294400/400000 (74%)]\tLoss: 0.991497\n","Train Epoch: 1 [300800/400000 (75%)]\tLoss: 1.020085\n","Train Epoch: 1 [307200/400000 (77%)]\tLoss: 1.022811\n","Train Epoch: 1 [313600/400000 (78%)]\tLoss: 1.042052\n","Train Epoch: 1 [320000/400000 (80%)]\tLoss: 1.013440\n","Train Epoch: 1 [326400/400000 (82%)]\tLoss: 0.993541\n","Train Epoch: 1 [332800/400000 (83%)]\tLoss: 1.007596\n","Train Epoch: 1 [339200/400000 (85%)]\tLoss: 0.968779\n","Train Epoch: 1 [345600/400000 (86%)]\tLoss: 1.043043\n","Train Epoch: 1 [352000/400000 (88%)]\tLoss: 1.010798\n","Train Epoch: 1 [358400/400000 (90%)]\tLoss: 1.039171\n","Train Epoch: 1 [364800/400000 (91%)]\tLoss: 1.035019\n","Train Epoch: 1 [371200/400000 (93%)]\tLoss: 1.048483\n","Train Epoch: 1 [377600/400000 (94%)]\tLoss: 1.025770\n","Train Epoch: 1 [384000/400000 (96%)]\tLoss: 1.024961\n","Train Epoch: 1 [390400/400000 (98%)]\tLoss: 0.997839\n","Train Epoch: 1 [396800/400000 (99%)]\tLoss: 0.977656\n","\n","Test set: Average loss: 0.0000, Accuracy: 55371/100000 (55%)\n","\n","Train Epoch: 2 [0/400000 (0%)]\tLoss: 0.986062\n","Train Epoch: 2 [6400/400000 (2%)]\tLoss: 1.052993\n","Train Epoch: 2 [12800/400000 (3%)]\tLoss: 0.995260\n","Train Epoch: 2 [19200/400000 (5%)]\tLoss: 0.980997\n","Train Epoch: 2 [25600/400000 (6%)]\tLoss: 1.064431\n","Train Epoch: 2 [32000/400000 (8%)]\tLoss: 1.088598\n","Train Epoch: 2 [38400/400000 (10%)]\tLoss: 0.969158\n","Train Epoch: 2 [44800/400000 (11%)]\tLoss: 1.006108\n","Train Epoch: 2 [51200/400000 (13%)]\tLoss: 1.064299\n","Train Epoch: 2 [57600/400000 (14%)]\tLoss: 0.968697\n","Train Epoch: 2 [64000/400000 (16%)]\tLoss: 1.010056\n","Train Epoch: 2 [70400/400000 (18%)]\tLoss: 1.050820\n","Train Epoch: 2 [76800/400000 (19%)]\tLoss: 1.004937\n","Train Epoch: 2 [83200/400000 (21%)]\tLoss: 1.036257\n","Train Epoch: 2 [89600/400000 (22%)]\tLoss: 0.977079\n","Train Epoch: 2 [96000/400000 (24%)]\tLoss: 0.970230\n","Train Epoch: 2 [102400/400000 (26%)]\tLoss: 1.015488\n","Train Epoch: 2 [108800/400000 (27%)]\tLoss: 0.992863\n","Train Epoch: 2 [115200/400000 (29%)]\tLoss: 1.000396\n","Train Epoch: 2 [121600/400000 (30%)]\tLoss: 1.002847\n","Train Epoch: 2 [128000/400000 (32%)]\tLoss: 1.038623\n","Train Epoch: 2 [134400/400000 (34%)]\tLoss: 1.031503\n","Train Epoch: 2 [140800/400000 (35%)]\tLoss: 1.051228\n","Train Epoch: 2 [147200/400000 (37%)]\tLoss: 1.039204\n","Train Epoch: 2 [153600/400000 (38%)]\tLoss: 1.025054\n","Train Epoch: 2 [160000/400000 (40%)]\tLoss: 0.965587\n","Train Epoch: 2 [166400/400000 (42%)]\tLoss: 1.105025\n","Train Epoch: 2 [172800/400000 (43%)]\tLoss: 0.952553\n","Train Epoch: 2 [179200/400000 (45%)]\tLoss: 1.025525\n","Train Epoch: 2 [185600/400000 (46%)]\tLoss: 0.996422\n","Train Epoch: 2 [192000/400000 (48%)]\tLoss: 1.024215\n","Train Epoch: 2 [198400/400000 (50%)]\tLoss: 1.070711\n","Train Epoch: 2 [204800/400000 (51%)]\tLoss: 1.003644\n","Train Epoch: 2 [211200/400000 (53%)]\tLoss: 0.952566\n","Train Epoch: 2 [217600/400000 (54%)]\tLoss: 1.069048\n","Train Epoch: 2 [224000/400000 (56%)]\tLoss: 1.041352\n","Train Epoch: 2 [230400/400000 (58%)]\tLoss: 1.018171\n","Train Epoch: 2 [236800/400000 (59%)]\tLoss: 0.989855\n","Train Epoch: 2 [243200/400000 (61%)]\tLoss: 0.970984\n","Train Epoch: 2 [249600/400000 (62%)]\tLoss: 1.016676\n","Train Epoch: 2 [256000/400000 (64%)]\tLoss: 1.020330\n","Train Epoch: 2 [262400/400000 (66%)]\tLoss: 1.002551\n","Train Epoch: 2 [268800/400000 (67%)]\tLoss: 1.015263\n","Train Epoch: 2 [275200/400000 (69%)]\tLoss: 1.011323\n","Train Epoch: 2 [281600/400000 (70%)]\tLoss: 1.010317\n","Train Epoch: 2 [288000/400000 (72%)]\tLoss: 0.982860\n","Train Epoch: 2 [294400/400000 (74%)]\tLoss: 1.059880\n","Train Epoch: 2 [300800/400000 (75%)]\tLoss: 1.028821\n","Train Epoch: 2 [307200/400000 (77%)]\tLoss: 1.032069\n","Train Epoch: 2 [313600/400000 (78%)]\tLoss: 1.013814\n","Train Epoch: 2 [320000/400000 (80%)]\tLoss: 1.037386\n","Train Epoch: 2 [326400/400000 (82%)]\tLoss: 1.014628\n","Train Epoch: 2 [332800/400000 (83%)]\tLoss: 0.992293\n","Train Epoch: 2 [339200/400000 (85%)]\tLoss: 1.089748\n","Train Epoch: 2 [345600/400000 (86%)]\tLoss: 1.028043\n","Train Epoch: 2 [352000/400000 (88%)]\tLoss: 1.025931\n","Train Epoch: 2 [358400/400000 (90%)]\tLoss: 1.027675\n","Train Epoch: 2 [364800/400000 (91%)]\tLoss: 0.953206\n","Train Epoch: 2 [371200/400000 (93%)]\tLoss: 1.009061\n","Train Epoch: 2 [377600/400000 (94%)]\tLoss: 1.042372\n","Train Epoch: 2 [384000/400000 (96%)]\tLoss: 0.984071\n","Train Epoch: 2 [390400/400000 (98%)]\tLoss: 1.001082\n","Train Epoch: 2 [396800/400000 (99%)]\tLoss: 0.997803\n","\n","Test set: Average loss: 0.0000, Accuracy: 55809/100000 (56%)\n","\n","Train Epoch: 3 [0/400000 (0%)]\tLoss: 1.025522\n","Train Epoch: 3 [6400/400000 (2%)]\tLoss: 1.015444\n","Train Epoch: 3 [12800/400000 (3%)]\tLoss: 1.000388\n","Train Epoch: 3 [19200/400000 (5%)]\tLoss: 1.000572\n","Train Epoch: 3 [25600/400000 (6%)]\tLoss: 0.994510\n","Train Epoch: 3 [32000/400000 (8%)]\tLoss: 1.037000\n","Train Epoch: 3 [38400/400000 (10%)]\tLoss: 0.983553\n","Train Epoch: 3 [44800/400000 (11%)]\tLoss: 1.061398\n","Train Epoch: 3 [51200/400000 (13%)]\tLoss: 1.023763\n","Train Epoch: 3 [57600/400000 (14%)]\tLoss: 1.016827\n","Train Epoch: 3 [64000/400000 (16%)]\tLoss: 1.088332\n","Train Epoch: 3 [70400/400000 (18%)]\tLoss: 1.067621\n","Train Epoch: 3 [76800/400000 (19%)]\tLoss: 1.024449\n","Train Epoch: 3 [83200/400000 (21%)]\tLoss: 1.024955\n","Train Epoch: 3 [89600/400000 (22%)]\tLoss: 1.038248\n","Train Epoch: 3 [96000/400000 (24%)]\tLoss: 1.055568\n","Train Epoch: 3 [102400/400000 (26%)]\tLoss: 1.028949\n","Train Epoch: 3 [108800/400000 (27%)]\tLoss: 0.993889\n","Train Epoch: 3 [115200/400000 (29%)]\tLoss: 0.970902\n","Train Epoch: 3 [121600/400000 (30%)]\tLoss: 1.003064\n","Train Epoch: 3 [128000/400000 (32%)]\tLoss: 1.031720\n","Train Epoch: 3 [134400/400000 (34%)]\tLoss: 1.023352\n","Train Epoch: 3 [140800/400000 (35%)]\tLoss: 1.019687\n","Train Epoch: 3 [147200/400000 (37%)]\tLoss: 1.035532\n","Train Epoch: 3 [153600/400000 (38%)]\tLoss: 1.070483\n","Train Epoch: 3 [160000/400000 (40%)]\tLoss: 0.968694\n","Train Epoch: 3 [166400/400000 (42%)]\tLoss: 1.067237\n","Train Epoch: 3 [172800/400000 (43%)]\tLoss: 1.019740\n","Train Epoch: 3 [179200/400000 (45%)]\tLoss: 0.999622\n","Train Epoch: 3 [185600/400000 (46%)]\tLoss: 0.975095\n","Train Epoch: 3 [192000/400000 (48%)]\tLoss: 1.011557\n","Train Epoch: 3 [198400/400000 (50%)]\tLoss: 1.027994\n","Train Epoch: 3 [204800/400000 (51%)]\tLoss: 1.043070\n","Train Epoch: 3 [211200/400000 (53%)]\tLoss: 0.991453\n","Train Epoch: 3 [217600/400000 (54%)]\tLoss: 1.008645\n","Train Epoch: 3 [224000/400000 (56%)]\tLoss: 1.092454\n","Train Epoch: 3 [230400/400000 (58%)]\tLoss: 1.029234\n","Train Epoch: 3 [236800/400000 (59%)]\tLoss: 1.068968\n","Train Epoch: 3 [243200/400000 (61%)]\tLoss: 1.019863\n","Train Epoch: 3 [249600/400000 (62%)]\tLoss: 1.054182\n","Train Epoch: 3 [256000/400000 (64%)]\tLoss: 0.970069\n","Train Epoch: 3 [262400/400000 (66%)]\tLoss: 1.042465\n","Train Epoch: 3 [268800/400000 (67%)]\tLoss: 1.009739\n","Train Epoch: 3 [275200/400000 (69%)]\tLoss: 1.017668\n","Train Epoch: 3 [281600/400000 (70%)]\tLoss: 1.053654\n","Train Epoch: 3 [288000/400000 (72%)]\tLoss: 1.057826\n","Train Epoch: 3 [294400/400000 (74%)]\tLoss: 0.954391\n","Train Epoch: 3 [300800/400000 (75%)]\tLoss: 1.070652\n","Train Epoch: 3 [307200/400000 (77%)]\tLoss: 1.029140\n","Train Epoch: 3 [313600/400000 (78%)]\tLoss: 0.996980\n","Train Epoch: 3 [320000/400000 (80%)]\tLoss: 1.023107\n","Train Epoch: 3 [326400/400000 (82%)]\tLoss: 1.011565\n","Train Epoch: 3 [332800/400000 (83%)]\tLoss: 1.049066\n","Train Epoch: 3 [339200/400000 (85%)]\tLoss: 1.033000\n","Train Epoch: 3 [345600/400000 (86%)]\tLoss: 0.978659\n","Train Epoch: 3 [352000/400000 (88%)]\tLoss: 1.028179\n","Train Epoch: 3 [358400/400000 (90%)]\tLoss: 1.018202\n","Train Epoch: 3 [364800/400000 (91%)]\tLoss: 1.001668\n","Train Epoch: 3 [371200/400000 (93%)]\tLoss: 1.010955\n","Train Epoch: 3 [377600/400000 (94%)]\tLoss: 1.002864\n","Train Epoch: 3 [384000/400000 (96%)]\tLoss: 1.020503\n","Train Epoch: 3 [390400/400000 (98%)]\tLoss: 1.012376\n","Train Epoch: 3 [396800/400000 (99%)]\tLoss: 1.007624\n","\n","Test set: Average loss: 0.0000, Accuracy: 55749/100000 (56%)\n","\n","Train Epoch: 4 [0/400000 (0%)]\tLoss: 0.997719\n","Train Epoch: 4 [6400/400000 (2%)]\tLoss: 0.999490\n","Train Epoch: 4 [12800/400000 (3%)]\tLoss: 1.048516\n","Train Epoch: 4 [19200/400000 (5%)]\tLoss: 1.082858\n","Train Epoch: 4 [25600/400000 (6%)]\tLoss: 1.058336\n","Train Epoch: 4 [32000/400000 (8%)]\tLoss: 1.039577\n","Train Epoch: 4 [38400/400000 (10%)]\tLoss: 1.004671\n","Train Epoch: 4 [44800/400000 (11%)]\tLoss: 0.983067\n","Train Epoch: 4 [51200/400000 (13%)]\tLoss: 1.033666\n","Train Epoch: 4 [57600/400000 (14%)]\tLoss: 0.984843\n","Train Epoch: 4 [64000/400000 (16%)]\tLoss: 1.021000\n","Train Epoch: 4 [70400/400000 (18%)]\tLoss: 0.989717\n","Train Epoch: 4 [76800/400000 (19%)]\tLoss: 1.055849\n","Train Epoch: 4 [83200/400000 (21%)]\tLoss: 1.036369\n","Train Epoch: 4 [89600/400000 (22%)]\tLoss: 1.016916\n","Train Epoch: 4 [96000/400000 (24%)]\tLoss: 0.975221\n","Train Epoch: 4 [102400/400000 (26%)]\tLoss: 0.987631\n","Train Epoch: 4 [108800/400000 (27%)]\tLoss: 0.969488\n","Train Epoch: 4 [115200/400000 (29%)]\tLoss: 0.977649\n","Train Epoch: 4 [121600/400000 (30%)]\tLoss: 0.991110\n","Train Epoch: 4 [128000/400000 (32%)]\tLoss: 0.979427\n","Train Epoch: 4 [134400/400000 (34%)]\tLoss: 1.014360\n","Train Epoch: 4 [140800/400000 (35%)]\tLoss: 0.960246\n","Train Epoch: 4 [147200/400000 (37%)]\tLoss: 0.976169\n","Train Epoch: 4 [153600/400000 (38%)]\tLoss: 1.034315\n","Train Epoch: 4 [160000/400000 (40%)]\tLoss: 0.974653\n","Train Epoch: 4 [166400/400000 (42%)]\tLoss: 0.959946\n","Train Epoch: 4 [172800/400000 (43%)]\tLoss: 1.002605\n","Train Epoch: 4 [179200/400000 (45%)]\tLoss: 1.004525\n","Train Epoch: 4 [185600/400000 (46%)]\tLoss: 0.966867\n","Train Epoch: 4 [192000/400000 (48%)]\tLoss: 0.994760\n","Train Epoch: 4 [198400/400000 (50%)]\tLoss: 1.014459\n","Train Epoch: 4 [204800/400000 (51%)]\tLoss: 1.009780\n","Train Epoch: 4 [211200/400000 (53%)]\tLoss: 1.078304\n","Train Epoch: 4 [217600/400000 (54%)]\tLoss: 1.029135\n","Train Epoch: 4 [224000/400000 (56%)]\tLoss: 0.956887\n","Train Epoch: 4 [230400/400000 (58%)]\tLoss: 0.988463\n","Train Epoch: 4 [236800/400000 (59%)]\tLoss: 1.011392\n","Train Epoch: 4 [243200/400000 (61%)]\tLoss: 1.003259\n","Train Epoch: 4 [249600/400000 (62%)]\tLoss: 1.041508\n","Train Epoch: 4 [256000/400000 (64%)]\tLoss: 1.043134\n","Train Epoch: 4 [262400/400000 (66%)]\tLoss: 1.051838\n","Train Epoch: 4 [268800/400000 (67%)]\tLoss: 1.048667\n","Train Epoch: 4 [275200/400000 (69%)]\tLoss: 1.025536\n","Train Epoch: 4 [281600/400000 (70%)]\tLoss: 1.057459\n","Train Epoch: 4 [288000/400000 (72%)]\tLoss: 1.007550\n","Train Epoch: 4 [294400/400000 (74%)]\tLoss: 1.067505\n","Train Epoch: 4 [300800/400000 (75%)]\tLoss: 1.015403\n","Train Epoch: 4 [307200/400000 (77%)]\tLoss: 0.994271\n","Train Epoch: 4 [313600/400000 (78%)]\tLoss: 0.953775\n","Train Epoch: 4 [320000/400000 (80%)]\tLoss: 0.994377\n","Train Epoch: 4 [326400/400000 (82%)]\tLoss: 1.033089\n","Train Epoch: 4 [332800/400000 (83%)]\tLoss: 1.038056\n","Train Epoch: 4 [339200/400000 (85%)]\tLoss: 0.972236\n","Train Epoch: 4 [345600/400000 (86%)]\tLoss: 1.000057\n","Train Epoch: 4 [352000/400000 (88%)]\tLoss: 1.026999\n","Train Epoch: 4 [358400/400000 (90%)]\tLoss: 1.021301\n","Train Epoch: 4 [364800/400000 (91%)]\tLoss: 1.013019\n","Train Epoch: 4 [371200/400000 (93%)]\tLoss: 1.043747\n","Train Epoch: 4 [377600/400000 (94%)]\tLoss: 1.008475\n","Train Epoch: 4 [384000/400000 (96%)]\tLoss: 1.021388\n","Train Epoch: 4 [390400/400000 (98%)]\tLoss: 1.000826\n","Train Epoch: 4 [396800/400000 (99%)]\tLoss: 0.959936\n","\n","Test set: Average loss: 0.0000, Accuracy: 56111/100000 (56%)\n","\n","Train Epoch: 5 [0/400000 (0%)]\tLoss: 0.992313\n","Train Epoch: 5 [6400/400000 (2%)]\tLoss: 1.034603\n","Train Epoch: 5 [12800/400000 (3%)]\tLoss: 1.077464\n","Train Epoch: 5 [19200/400000 (5%)]\tLoss: 1.025904\n","Train Epoch: 5 [25600/400000 (6%)]\tLoss: 1.012255\n","Train Epoch: 5 [32000/400000 (8%)]\tLoss: 1.022235\n","Train Epoch: 5 [38400/400000 (10%)]\tLoss: 1.003068\n","Train Epoch: 5 [44800/400000 (11%)]\tLoss: 0.960239\n","Train Epoch: 5 [51200/400000 (13%)]\tLoss: 0.993081\n","Train Epoch: 5 [57600/400000 (14%)]\tLoss: 0.968319\n","Train Epoch: 5 [64000/400000 (16%)]\tLoss: 0.990687\n","Train Epoch: 5 [70400/400000 (18%)]\tLoss: 0.989469\n","Train Epoch: 5 [76800/400000 (19%)]\tLoss: 1.015498\n","Train Epoch: 5 [83200/400000 (21%)]\tLoss: 0.975641\n","Train Epoch: 5 [89600/400000 (22%)]\tLoss: 0.949293\n","Train Epoch: 5 [96000/400000 (24%)]\tLoss: 1.009366\n","Train Epoch: 5 [102400/400000 (26%)]\tLoss: 0.960301\n","Train Epoch: 5 [108800/400000 (27%)]\tLoss: 1.012220\n","Train Epoch: 5 [115200/400000 (29%)]\tLoss: 0.998315\n","Train Epoch: 5 [121600/400000 (30%)]\tLoss: 1.031165\n","Train Epoch: 5 [128000/400000 (32%)]\tLoss: 1.033665\n","Train Epoch: 5 [134400/400000 (34%)]\tLoss: 1.005128\n","Train Epoch: 5 [140800/400000 (35%)]\tLoss: 1.031014\n","Train Epoch: 5 [147200/400000 (37%)]\tLoss: 1.032273\n","Train Epoch: 5 [153600/400000 (38%)]\tLoss: 1.021531\n","Train Epoch: 5 [160000/400000 (40%)]\tLoss: 1.011833\n","Train Epoch: 5 [166400/400000 (42%)]\tLoss: 1.017895\n","Train Epoch: 5 [172800/400000 (43%)]\tLoss: 0.948305\n","Train Epoch: 5 [179200/400000 (45%)]\tLoss: 1.045621\n","Train Epoch: 5 [185600/400000 (46%)]\tLoss: 0.961272\n","Train Epoch: 5 [192000/400000 (48%)]\tLoss: 1.028522\n","Train Epoch: 5 [198400/400000 (50%)]\tLoss: 0.954842\n","Train Epoch: 5 [204800/400000 (51%)]\tLoss: 1.030168\n","Train Epoch: 5 [211200/400000 (53%)]\tLoss: 1.039931\n","Train Epoch: 5 [217600/400000 (54%)]\tLoss: 0.984460\n","Train Epoch: 5 [224000/400000 (56%)]\tLoss: 0.991770\n","Train Epoch: 5 [230400/400000 (58%)]\tLoss: 1.024195\n","Train Epoch: 5 [236800/400000 (59%)]\tLoss: 1.040864\n","Train Epoch: 5 [243200/400000 (61%)]\tLoss: 1.004957\n","Train Epoch: 5 [249600/400000 (62%)]\tLoss: 0.942501\n","Train Epoch: 5 [256000/400000 (64%)]\tLoss: 1.005723\n","Train Epoch: 5 [262400/400000 (66%)]\tLoss: 1.067944\n","Train Epoch: 5 [268800/400000 (67%)]\tLoss: 0.993388\n","Train Epoch: 5 [275200/400000 (69%)]\tLoss: 1.016961\n","Train Epoch: 5 [281600/400000 (70%)]\tLoss: 1.014349\n","Train Epoch: 5 [288000/400000 (72%)]\tLoss: 1.024053\n","Train Epoch: 5 [294400/400000 (74%)]\tLoss: 1.079396\n","Train Epoch: 5 [300800/400000 (75%)]\tLoss: 1.059810\n","Train Epoch: 5 [307200/400000 (77%)]\tLoss: 1.009549\n","Train Epoch: 5 [313600/400000 (78%)]\tLoss: 0.997990\n","Train Epoch: 5 [320000/400000 (80%)]\tLoss: 0.983811\n","Train Epoch: 5 [326400/400000 (82%)]\tLoss: 1.011436\n","Train Epoch: 5 [332800/400000 (83%)]\tLoss: 0.992440\n","Train Epoch: 5 [339200/400000 (85%)]\tLoss: 0.968628\n","Train Epoch: 5 [345600/400000 (86%)]\tLoss: 0.970690\n","Train Epoch: 5 [352000/400000 (88%)]\tLoss: 1.032759\n","Train Epoch: 5 [358400/400000 (90%)]\tLoss: 1.015139\n","Train Epoch: 5 [364800/400000 (91%)]\tLoss: 1.058421\n","Train Epoch: 5 [371200/400000 (93%)]\tLoss: 1.037719\n","Train Epoch: 5 [377600/400000 (94%)]\tLoss: 1.049776\n","Train Epoch: 5 [384000/400000 (96%)]\tLoss: 1.049114\n","Train Epoch: 5 [390400/400000 (98%)]\tLoss: 0.974806\n","Train Epoch: 5 [396800/400000 (99%)]\tLoss: 0.969943\n","\n","Test set: Average loss: 0.0000, Accuracy: 56204/100000 (56%)\n","\n","Train Epoch: 6 [0/400000 (0%)]\tLoss: 0.955445\n","Train Epoch: 6 [6400/400000 (2%)]\tLoss: 1.024703\n","Train Epoch: 6 [12800/400000 (3%)]\tLoss: 0.939397\n","Train Epoch: 6 [19200/400000 (5%)]\tLoss: 1.038505\n","Train Epoch: 6 [25600/400000 (6%)]\tLoss: 0.999898\n","Train Epoch: 6 [32000/400000 (8%)]\tLoss: 1.024319\n","Train Epoch: 6 [38400/400000 (10%)]\tLoss: 1.011544\n","Train Epoch: 6 [44800/400000 (11%)]\tLoss: 1.016417\n","Train Epoch: 6 [51200/400000 (13%)]\tLoss: 0.994829\n","Train Epoch: 6 [57600/400000 (14%)]\tLoss: 0.967618\n","Train Epoch: 6 [64000/400000 (16%)]\tLoss: 1.022712\n","Train Epoch: 6 [70400/400000 (18%)]\tLoss: 1.065862\n","Train Epoch: 6 [76800/400000 (19%)]\tLoss: 1.031086\n","Train Epoch: 6 [83200/400000 (21%)]\tLoss: 1.008447\n","Train Epoch: 6 [89600/400000 (22%)]\tLoss: 0.960257\n","Train Epoch: 6 [96000/400000 (24%)]\tLoss: 0.957483\n","Train Epoch: 6 [102400/400000 (26%)]\tLoss: 1.050061\n","Train Epoch: 6 [108800/400000 (27%)]\tLoss: 0.978536\n","Train Epoch: 6 [115200/400000 (29%)]\tLoss: 1.011725\n","Train Epoch: 6 [121600/400000 (30%)]\tLoss: 1.023728\n","Train Epoch: 6 [128000/400000 (32%)]\tLoss: 0.995108\n","Train Epoch: 6 [134400/400000 (34%)]\tLoss: 0.988009\n","Train Epoch: 6 [140800/400000 (35%)]\tLoss: 0.996137\n","Train Epoch: 6 [147200/400000 (37%)]\tLoss: 0.997886\n","Train Epoch: 6 [153600/400000 (38%)]\tLoss: 1.008948\n","Train Epoch: 6 [160000/400000 (40%)]\tLoss: 0.988905\n","Train Epoch: 6 [166400/400000 (42%)]\tLoss: 0.970741\n","Train Epoch: 6 [172800/400000 (43%)]\tLoss: 0.999799\n","Train Epoch: 6 [179200/400000 (45%)]\tLoss: 1.025777\n","Train Epoch: 6 [185600/400000 (46%)]\tLoss: 0.934700\n","Train Epoch: 6 [192000/400000 (48%)]\tLoss: 1.010279\n","Train Epoch: 6 [198400/400000 (50%)]\tLoss: 0.985481\n","Train Epoch: 6 [204800/400000 (51%)]\tLoss: 0.999656\n","Train Epoch: 6 [211200/400000 (53%)]\tLoss: 0.969461\n","Train Epoch: 6 [217600/400000 (54%)]\tLoss: 1.017071\n","Train Epoch: 6 [224000/400000 (56%)]\tLoss: 0.974698\n","Train Epoch: 6 [230400/400000 (58%)]\tLoss: 0.941427\n","Train Epoch: 6 [236800/400000 (59%)]\tLoss: 1.005145\n","Train Epoch: 6 [243200/400000 (61%)]\tLoss: 1.023775\n","Train Epoch: 6 [249600/400000 (62%)]\tLoss: 1.065614\n","Train Epoch: 6 [256000/400000 (64%)]\tLoss: 0.998158\n","Train Epoch: 6 [262400/400000 (66%)]\tLoss: 1.031456\n","Train Epoch: 6 [268800/400000 (67%)]\tLoss: 1.010863\n","Train Epoch: 6 [275200/400000 (69%)]\tLoss: 1.001653\n","Train Epoch: 6 [281600/400000 (70%)]\tLoss: 1.015964\n","Train Epoch: 6 [288000/400000 (72%)]\tLoss: 0.999602\n","Train Epoch: 6 [294400/400000 (74%)]\tLoss: 1.012491\n","Train Epoch: 6 [300800/400000 (75%)]\tLoss: 1.021987\n","Train Epoch: 6 [307200/400000 (77%)]\tLoss: 1.007030\n","Train Epoch: 6 [313600/400000 (78%)]\tLoss: 0.970476\n","Train Epoch: 6 [320000/400000 (80%)]\tLoss: 1.015554\n","Train Epoch: 6 [326400/400000 (82%)]\tLoss: 0.997956\n","Train Epoch: 6 [332800/400000 (83%)]\tLoss: 0.964023\n","Train Epoch: 6 [339200/400000 (85%)]\tLoss: 1.083759\n","Train Epoch: 6 [345600/400000 (86%)]\tLoss: 0.972954\n","Train Epoch: 6 [352000/400000 (88%)]\tLoss: 0.967208\n","Train Epoch: 6 [358400/400000 (90%)]\tLoss: 1.022481\n","Train Epoch: 6 [364800/400000 (91%)]\tLoss: 1.024028\n","Train Epoch: 6 [371200/400000 (93%)]\tLoss: 1.040865\n","Train Epoch: 6 [377600/400000 (94%)]\tLoss: 1.065406\n","Train Epoch: 6 [384000/400000 (96%)]\tLoss: 1.017972\n","Train Epoch: 6 [390400/400000 (98%)]\tLoss: 1.013197\n","Train Epoch: 6 [396800/400000 (99%)]\tLoss: 1.021834\n","\n","Test set: Average loss: 0.0000, Accuracy: 56209/100000 (56%)\n","\n","Train Epoch: 7 [0/400000 (0%)]\tLoss: 1.021864\n","Train Epoch: 7 [6400/400000 (2%)]\tLoss: 0.966068\n","Train Epoch: 7 [12800/400000 (3%)]\tLoss: 1.010450\n","Train Epoch: 7 [19200/400000 (5%)]\tLoss: 1.033910\n","Train Epoch: 7 [25600/400000 (6%)]\tLoss: 1.069784\n","Train Epoch: 7 [32000/400000 (8%)]\tLoss: 1.047619\n","Train Epoch: 7 [38400/400000 (10%)]\tLoss: 1.036554\n","Train Epoch: 7 [44800/400000 (11%)]\tLoss: 1.005964\n","Train Epoch: 7 [51200/400000 (13%)]\tLoss: 0.971025\n","Train Epoch: 7 [57600/400000 (14%)]\tLoss: 1.032338\n","Train Epoch: 7 [64000/400000 (16%)]\tLoss: 0.974610\n","Train Epoch: 7 [70400/400000 (18%)]\tLoss: 1.001238\n","Train Epoch: 7 [76800/400000 (19%)]\tLoss: 1.049648\n","Train Epoch: 7 [83200/400000 (21%)]\tLoss: 1.030930\n","Train Epoch: 7 [89600/400000 (22%)]\tLoss: 1.065970\n","Train Epoch: 7 [96000/400000 (24%)]\tLoss: 1.009308\n","Train Epoch: 7 [102400/400000 (26%)]\tLoss: 1.005809\n","Train Epoch: 7 [108800/400000 (27%)]\tLoss: 0.990233\n","Train Epoch: 7 [115200/400000 (29%)]\tLoss: 0.980060\n","Train Epoch: 7 [121600/400000 (30%)]\tLoss: 1.016260\n","Train Epoch: 7 [128000/400000 (32%)]\tLoss: 1.029787\n","Train Epoch: 7 [134400/400000 (34%)]\tLoss: 1.024529\n","Train Epoch: 7 [140800/400000 (35%)]\tLoss: 1.031891\n","Train Epoch: 7 [147200/400000 (37%)]\tLoss: 1.001236\n","Train Epoch: 7 [153600/400000 (38%)]\tLoss: 1.023470\n","Train Epoch: 7 [160000/400000 (40%)]\tLoss: 1.062119\n","Train Epoch: 7 [166400/400000 (42%)]\tLoss: 1.036499\n","Train Epoch: 7 [172800/400000 (43%)]\tLoss: 0.941848\n","Train Epoch: 7 [179200/400000 (45%)]\tLoss: 1.009866\n","Train Epoch: 7 [185600/400000 (46%)]\tLoss: 1.038870\n","Train Epoch: 7 [192000/400000 (48%)]\tLoss: 1.002692\n","Train Epoch: 7 [198400/400000 (50%)]\tLoss: 0.977467\n","Train Epoch: 7 [204800/400000 (51%)]\tLoss: 1.049784\n","Train Epoch: 7 [211200/400000 (53%)]\tLoss: 0.993290\n","Train Epoch: 7 [217600/400000 (54%)]\tLoss: 1.020537\n","Train Epoch: 7 [224000/400000 (56%)]\tLoss: 1.011472\n","Train Epoch: 7 [230400/400000 (58%)]\tLoss: 1.019192\n","Train Epoch: 7 [236800/400000 (59%)]\tLoss: 1.018225\n","Train Epoch: 7 [243200/400000 (61%)]\tLoss: 1.025527\n","Train Epoch: 7 [249600/400000 (62%)]\tLoss: 0.980866\n","Train Epoch: 7 [256000/400000 (64%)]\tLoss: 1.069926\n","Train Epoch: 7 [262400/400000 (66%)]\tLoss: 0.948388\n","Train Epoch: 7 [268800/400000 (67%)]\tLoss: 1.007479\n","Train Epoch: 7 [275200/400000 (69%)]\tLoss: 1.059228\n","Train Epoch: 7 [281600/400000 (70%)]\tLoss: 1.034148\n","Train Epoch: 7 [288000/400000 (72%)]\tLoss: 0.953763\n","Train Epoch: 7 [294400/400000 (74%)]\tLoss: 0.978769\n","Train Epoch: 7 [300800/400000 (75%)]\tLoss: 0.936448\n","Train Epoch: 7 [307200/400000 (77%)]\tLoss: 1.033095\n","Train Epoch: 7 [313600/400000 (78%)]\tLoss: 0.984141\n","Train Epoch: 7 [320000/400000 (80%)]\tLoss: 0.998462\n","Train Epoch: 7 [326400/400000 (82%)]\tLoss: 1.005615\n","Train Epoch: 7 [332800/400000 (83%)]\tLoss: 0.972714\n","Train Epoch: 7 [339200/400000 (85%)]\tLoss: 1.019312\n","Train Epoch: 7 [345600/400000 (86%)]\tLoss: 1.030543\n","Train Epoch: 7 [352000/400000 (88%)]\tLoss: 1.000804\n","Train Epoch: 7 [358400/400000 (90%)]\tLoss: 0.991710\n","Train Epoch: 7 [364800/400000 (91%)]\tLoss: 0.977408\n","Train Epoch: 7 [371200/400000 (93%)]\tLoss: 1.036317\n","Train Epoch: 7 [377600/400000 (94%)]\tLoss: 0.984987\n","Train Epoch: 7 [384000/400000 (96%)]\tLoss: 1.007499\n","Train Epoch: 7 [390400/400000 (98%)]\tLoss: 0.960399\n","Train Epoch: 7 [396800/400000 (99%)]\tLoss: 0.990098\n","\n","Test set: Average loss: 0.0000, Accuracy: 56183/100000 (56%)\n","\n","Train Epoch: 8 [0/400000 (0%)]\tLoss: 0.999364\n","Train Epoch: 8 [6400/400000 (2%)]\tLoss: 0.972889\n","Train Epoch: 8 [12800/400000 (3%)]\tLoss: 0.992833\n","Train Epoch: 8 [19200/400000 (5%)]\tLoss: 1.019937\n","Train Epoch: 8 [25600/400000 (6%)]\tLoss: 1.065555\n","Train Epoch: 8 [32000/400000 (8%)]\tLoss: 1.009466\n","Train Epoch: 8 [38400/400000 (10%)]\tLoss: 1.016695\n","Train Epoch: 8 [44800/400000 (11%)]\tLoss: 1.024431\n","Train Epoch: 8 [51200/400000 (13%)]\tLoss: 0.990081\n","Train Epoch: 8 [57600/400000 (14%)]\tLoss: 1.020162\n","Train Epoch: 8 [64000/400000 (16%)]\tLoss: 0.923879\n","Train Epoch: 8 [70400/400000 (18%)]\tLoss: 0.994586\n","Train Epoch: 8 [76800/400000 (19%)]\tLoss: 0.985822\n","Train Epoch: 8 [83200/400000 (21%)]\tLoss: 1.006451\n","Train Epoch: 8 [89600/400000 (22%)]\tLoss: 0.982105\n","Train Epoch: 8 [96000/400000 (24%)]\tLoss: 0.995964\n","Train Epoch: 8 [102400/400000 (26%)]\tLoss: 0.985442\n","Train Epoch: 8 [108800/400000 (27%)]\tLoss: 0.985073\n","Train Epoch: 8 [115200/400000 (29%)]\tLoss: 0.990255\n","Train Epoch: 8 [121600/400000 (30%)]\tLoss: 1.000853\n","Train Epoch: 8 [128000/400000 (32%)]\tLoss: 0.993603\n","Train Epoch: 8 [134400/400000 (34%)]\tLoss: 0.984370\n","Train Epoch: 8 [140800/400000 (35%)]\tLoss: 0.994947\n","Train Epoch: 8 [147200/400000 (37%)]\tLoss: 1.002744\n","Train Epoch: 8 [153600/400000 (38%)]\tLoss: 0.984882\n","Train Epoch: 8 [160000/400000 (40%)]\tLoss: 0.955981\n","Train Epoch: 8 [166400/400000 (42%)]\tLoss: 0.998181\n","Train Epoch: 8 [172800/400000 (43%)]\tLoss: 1.012118\n","Train Epoch: 8 [179200/400000 (45%)]\tLoss: 0.965040\n","Train Epoch: 8 [185600/400000 (46%)]\tLoss: 0.967612\n","Train Epoch: 8 [192000/400000 (48%)]\tLoss: 1.006160\n","Train Epoch: 8 [198400/400000 (50%)]\tLoss: 0.979438\n","Train Epoch: 8 [204800/400000 (51%)]\tLoss: 0.996968\n","Train Epoch: 8 [211200/400000 (53%)]\tLoss: 1.005960\n","Train Epoch: 8 [217600/400000 (54%)]\tLoss: 0.995576\n","Train Epoch: 8 [224000/400000 (56%)]\tLoss: 1.033177\n","Train Epoch: 8 [230400/400000 (58%)]\tLoss: 1.029822\n","Train Epoch: 8 [236800/400000 (59%)]\tLoss: 1.015990\n","Train Epoch: 8 [243200/400000 (61%)]\tLoss: 0.979390\n","Train Epoch: 8 [249600/400000 (62%)]\tLoss: 1.010154\n","Train Epoch: 8 [256000/400000 (64%)]\tLoss: 0.973375\n","Train Epoch: 8 [262400/400000 (66%)]\tLoss: 1.007692\n","Train Epoch: 8 [268800/400000 (67%)]\tLoss: 1.015083\n","Train Epoch: 8 [275200/400000 (69%)]\tLoss: 0.995530\n","Train Epoch: 8 [281600/400000 (70%)]\tLoss: 1.003108\n","Train Epoch: 8 [288000/400000 (72%)]\tLoss: 1.000100\n","Train Epoch: 8 [294400/400000 (74%)]\tLoss: 1.006240\n","Train Epoch: 8 [300800/400000 (75%)]\tLoss: 1.016691\n","Train Epoch: 8 [307200/400000 (77%)]\tLoss: 0.971860\n","Train Epoch: 8 [313600/400000 (78%)]\tLoss: 0.972403\n","Train Epoch: 8 [320000/400000 (80%)]\tLoss: 1.013606\n","Train Epoch: 8 [326400/400000 (82%)]\tLoss: 0.959745\n","Train Epoch: 8 [332800/400000 (83%)]\tLoss: 0.995062\n","Train Epoch: 8 [339200/400000 (85%)]\tLoss: 1.016357\n","Train Epoch: 8 [345600/400000 (86%)]\tLoss: 1.060737\n","Train Epoch: 8 [352000/400000 (88%)]\tLoss: 1.000006\n","Train Epoch: 8 [358400/400000 (90%)]\tLoss: 1.010714\n","Train Epoch: 8 [364800/400000 (91%)]\tLoss: 1.028154\n","Train Epoch: 8 [371200/400000 (93%)]\tLoss: 1.069530\n","Train Epoch: 8 [377600/400000 (94%)]\tLoss: 0.986713\n","Train Epoch: 8 [384000/400000 (96%)]\tLoss: 1.026466\n","Train Epoch: 8 [390400/400000 (98%)]\tLoss: 0.979153\n","Train Epoch: 8 [396800/400000 (99%)]\tLoss: 0.934392\n","\n","Test set: Average loss: 0.0000, Accuracy: 56285/100000 (56%)\n","\n","Train Epoch: 9 [0/400000 (0%)]\tLoss: 0.961963\n","Train Epoch: 9 [6400/400000 (2%)]\tLoss: 0.996809\n","Train Epoch: 9 [12800/400000 (3%)]\tLoss: 1.060715\n","Train Epoch: 9 [19200/400000 (5%)]\tLoss: 1.007996\n","Train Epoch: 9 [25600/400000 (6%)]\tLoss: 1.017831\n","Train Epoch: 9 [32000/400000 (8%)]\tLoss: 0.951391\n","Train Epoch: 9 [38400/400000 (10%)]\tLoss: 1.037109\n","Train Epoch: 9 [44800/400000 (11%)]\tLoss: 1.050367\n","Train Epoch: 9 [51200/400000 (13%)]\tLoss: 0.999752\n","Train Epoch: 9 [57600/400000 (14%)]\tLoss: 1.014930\n","Train Epoch: 9 [64000/400000 (16%)]\tLoss: 0.967903\n","Train Epoch: 9 [70400/400000 (18%)]\tLoss: 0.992582\n","Train Epoch: 9 [76800/400000 (19%)]\tLoss: 1.003181\n","Train Epoch: 9 [83200/400000 (21%)]\tLoss: 1.015193\n","Train Epoch: 9 [89600/400000 (22%)]\tLoss: 1.038669\n","Train Epoch: 9 [96000/400000 (24%)]\tLoss: 1.000940\n","Train Epoch: 9 [102400/400000 (26%)]\tLoss: 1.048567\n","Train Epoch: 9 [108800/400000 (27%)]\tLoss: 1.063724\n","Train Epoch: 9 [115200/400000 (29%)]\tLoss: 1.067038\n","Train Epoch: 9 [121600/400000 (30%)]\tLoss: 0.999470\n","Train Epoch: 9 [128000/400000 (32%)]\tLoss: 0.983622\n","Train Epoch: 9 [134400/400000 (34%)]\tLoss: 0.954943\n","Train Epoch: 9 [140800/400000 (35%)]\tLoss: 0.964487\n","Train Epoch: 9 [147200/400000 (37%)]\tLoss: 1.012994\n","Train Epoch: 9 [153600/400000 (38%)]\tLoss: 0.995627\n","Train Epoch: 9 [160000/400000 (40%)]\tLoss: 1.001459\n","Train Epoch: 9 [166400/400000 (42%)]\tLoss: 1.025055\n","Train Epoch: 9 [172800/400000 (43%)]\tLoss: 0.958100\n","Train Epoch: 9 [179200/400000 (45%)]\tLoss: 0.991541\n","Train Epoch: 9 [185600/400000 (46%)]\tLoss: 0.971226\n","Train Epoch: 9 [192000/400000 (48%)]\tLoss: 0.943639\n","Train Epoch: 9 [198400/400000 (50%)]\tLoss: 1.014066\n","Train Epoch: 9 [204800/400000 (51%)]\tLoss: 0.968703\n","Train Epoch: 9 [211200/400000 (53%)]\tLoss: 0.991331\n","Train Epoch: 9 [217600/400000 (54%)]\tLoss: 0.999181\n","Train Epoch: 9 [224000/400000 (56%)]\tLoss: 0.927593\n","Train Epoch: 9 [230400/400000 (58%)]\tLoss: 1.019967\n","Train Epoch: 9 [236800/400000 (59%)]\tLoss: 1.046622\n","Train Epoch: 9 [243200/400000 (61%)]\tLoss: 0.966937\n","Train Epoch: 9 [249600/400000 (62%)]\tLoss: 1.004321\n","Train Epoch: 9 [256000/400000 (64%)]\tLoss: 1.051987\n","Train Epoch: 9 [262400/400000 (66%)]\tLoss: 0.989388\n","Train Epoch: 9 [268800/400000 (67%)]\tLoss: 0.982261\n","Train Epoch: 9 [275200/400000 (69%)]\tLoss: 0.988805\n","Train Epoch: 9 [281600/400000 (70%)]\tLoss: 0.968750\n","Train Epoch: 9 [288000/400000 (72%)]\tLoss: 0.958286\n","Train Epoch: 9 [294400/400000 (74%)]\tLoss: 0.958812\n","Train Epoch: 9 [300800/400000 (75%)]\tLoss: 1.002304\n","Train Epoch: 9 [307200/400000 (77%)]\tLoss: 1.015766\n","Train Epoch: 9 [313600/400000 (78%)]\tLoss: 1.022991\n","Train Epoch: 9 [320000/400000 (80%)]\tLoss: 0.992630\n","Train Epoch: 9 [326400/400000 (82%)]\tLoss: 0.952699\n","Train Epoch: 9 [332800/400000 (83%)]\tLoss: 1.026732\n","Train Epoch: 9 [339200/400000 (85%)]\tLoss: 1.041714\n","Train Epoch: 9 [345600/400000 (86%)]\tLoss: 1.019429\n","Train Epoch: 9 [352000/400000 (88%)]\tLoss: 0.947665\n","Train Epoch: 9 [358400/400000 (90%)]\tLoss: 0.983670\n","Train Epoch: 9 [364800/400000 (91%)]\tLoss: 0.992144\n","Train Epoch: 9 [371200/400000 (93%)]\tLoss: 0.985748\n","Train Epoch: 9 [377600/400000 (94%)]\tLoss: 1.024885\n","Train Epoch: 9 [384000/400000 (96%)]\tLoss: 1.019334\n","Train Epoch: 9 [390400/400000 (98%)]\tLoss: 1.015900\n","Train Epoch: 9 [396800/400000 (99%)]\tLoss: 1.023941\n","\n","Test set: Average loss: 0.0000, Accuracy: 56298/100000 (56%)\n","\n","Train Epoch: 10 [0/400000 (0%)]\tLoss: 1.041353\n","Train Epoch: 10 [6400/400000 (2%)]\tLoss: 0.986183\n","Train Epoch: 10 [12800/400000 (3%)]\tLoss: 1.032849\n","Train Epoch: 10 [19200/400000 (5%)]\tLoss: 0.966875\n","Train Epoch: 10 [25600/400000 (6%)]\tLoss: 1.029367\n","Train Epoch: 10 [32000/400000 (8%)]\tLoss: 1.012622\n","Train Epoch: 10 [38400/400000 (10%)]\tLoss: 0.924054\n","Train Epoch: 10 [44800/400000 (11%)]\tLoss: 1.010041\n","Train Epoch: 10 [51200/400000 (13%)]\tLoss: 1.042886\n","Train Epoch: 10 [57600/400000 (14%)]\tLoss: 0.967091\n","Train Epoch: 10 [64000/400000 (16%)]\tLoss: 0.983024\n","Train Epoch: 10 [70400/400000 (18%)]\tLoss: 1.071501\n","Train Epoch: 10 [76800/400000 (19%)]\tLoss: 0.987415\n","Train Epoch: 10 [83200/400000 (21%)]\tLoss: 0.999406\n","Train Epoch: 10 [89600/400000 (22%)]\tLoss: 0.987880\n","Train Epoch: 10 [96000/400000 (24%)]\tLoss: 0.984948\n","Train Epoch: 10 [102400/400000 (26%)]\tLoss: 0.991603\n","Train Epoch: 10 [108800/400000 (27%)]\tLoss: 1.059563\n","Train Epoch: 10 [115200/400000 (29%)]\tLoss: 0.962059\n","Train Epoch: 10 [121600/400000 (30%)]\tLoss: 1.003248\n","Train Epoch: 10 [128000/400000 (32%)]\tLoss: 1.028330\n","Train Epoch: 10 [134400/400000 (34%)]\tLoss: 0.989483\n","Train Epoch: 10 [140800/400000 (35%)]\tLoss: 0.982036\n","Train Epoch: 10 [147200/400000 (37%)]\tLoss: 1.011392\n","Train Epoch: 10 [153600/400000 (38%)]\tLoss: 0.983049\n","Train Epoch: 10 [160000/400000 (40%)]\tLoss: 0.993645\n","Train Epoch: 10 [166400/400000 (42%)]\tLoss: 1.053183\n","Train Epoch: 10 [172800/400000 (43%)]\tLoss: 0.925295\n","Train Epoch: 10 [179200/400000 (45%)]\tLoss: 0.995022\n","Train Epoch: 10 [185600/400000 (46%)]\tLoss: 1.021729\n","Train Epoch: 10 [192000/400000 (48%)]\tLoss: 0.993820\n","Train Epoch: 10 [198400/400000 (50%)]\tLoss: 1.015116\n","Train Epoch: 10 [204800/400000 (51%)]\tLoss: 1.025207\n","Train Epoch: 10 [211200/400000 (53%)]\tLoss: 1.044124\n","Train Epoch: 10 [217600/400000 (54%)]\tLoss: 0.951199\n","Train Epoch: 10 [224000/400000 (56%)]\tLoss: 0.954553\n","Train Epoch: 10 [230400/400000 (58%)]\tLoss: 0.999345\n","Train Epoch: 10 [236800/400000 (59%)]\tLoss: 0.984426\n","Train Epoch: 10 [243200/400000 (61%)]\tLoss: 1.053781\n","Train Epoch: 10 [249600/400000 (62%)]\tLoss: 0.960565\n","Train Epoch: 10 [256000/400000 (64%)]\tLoss: 1.037442\n","Train Epoch: 10 [262400/400000 (66%)]\tLoss: 0.993064\n","Train Epoch: 10 [268800/400000 (67%)]\tLoss: 1.042311\n","Train Epoch: 10 [275200/400000 (69%)]\tLoss: 1.002434\n","Train Epoch: 10 [281600/400000 (70%)]\tLoss: 0.932158\n","Train Epoch: 10 [288000/400000 (72%)]\tLoss: 1.036752\n","Train Epoch: 10 [294400/400000 (74%)]\tLoss: 0.960216\n","Train Epoch: 10 [300800/400000 (75%)]\tLoss: 0.998407\n","Train Epoch: 10 [307200/400000 (77%)]\tLoss: 1.023499\n","Train Epoch: 10 [313600/400000 (78%)]\tLoss: 0.996219\n","Train Epoch: 10 [320000/400000 (80%)]\tLoss: 1.008787\n","Train Epoch: 10 [326400/400000 (82%)]\tLoss: 1.044225\n","Train Epoch: 10 [332800/400000 (83%)]\tLoss: 1.034288\n","Train Epoch: 10 [339200/400000 (85%)]\tLoss: 1.017210\n","Train Epoch: 10 [345600/400000 (86%)]\tLoss: 1.052523\n","Train Epoch: 10 [352000/400000 (88%)]\tLoss: 0.999426\n","Train Epoch: 10 [358400/400000 (90%)]\tLoss: 1.044596\n","Train Epoch: 10 [364800/400000 (91%)]\tLoss: 0.996254\n","Train Epoch: 10 [371200/400000 (93%)]\tLoss: 1.006499\n","Train Epoch: 10 [377600/400000 (94%)]\tLoss: 0.941611\n","Train Epoch: 10 [384000/400000 (96%)]\tLoss: 1.029164\n","Train Epoch: 10 [390400/400000 (98%)]\tLoss: 0.972922\n","Train Epoch: 10 [396800/400000 (99%)]\tLoss: 1.018144\n","\n","Test set: Average loss: 0.0000, Accuracy: 56294/100000 (56%)\n","\n","56298\n"]}],"source":["torch.manual_seed(args.seed)\n","\n","device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n","model = Net().to(device)\n","\n","for param_tensor in model.state_dict():\n","        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n","\n","#Form training and testing dataset\n","optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n","\n","train_dataset = torch.utils.data.TensorDataset(train_vectors, train_labels)\n","test_dataset = torch.utils.data.TensorDataset(test_vectors, test_labels)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=640, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=640, shuffle=False)\n","scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n","\n","#Model training\n","ACC = 0\n","for epoch in range(1, args.epochs + 1):\n","    train(args, model, device, train_loader, optimizer, epoch)\n","    ACC_ = test(model, device, test_loader)\n","    if ACC_>ACC or ACC_ == ACC:\n","        ACC = ACC_\n","        torch.save(model.state_dict(), \"Baseline_lstm.pt\")\n","\n","    scheduler.step()\n","\n","print(ACC)\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1IsU3G6rxR5Av7doUpPuti9HJsoBq3hik","timestamp":1691994852365}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}