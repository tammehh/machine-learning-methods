{"cells":[{"cell_type":"markdown","source":["# Note:\n","- This notebook file may contain methods or algorithms that are NOT covered by the teaching content of BT4222 and hence will not be assessed in your midterm exam.\n","- It serves to increase your exposure in depth and breath to the practical methods in addressing the specific project topic. We believe it will be helpful for your current project and also your future internship endeavors."],"metadata":{"id":"IPTSDnKrlasY"}},{"cell_type":"markdown","metadata":{"id":"UXYWxqcLTrsT"},"source":["# **Import Library**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STdFlfuXL6Xz"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR"]},{"cell_type":"markdown","metadata":{"id":"-98wEEiETkWO"},"source":["# **Define Network Structure**\n"," The network structure was from\n","\"Hierarchical Attentional Hybrid Neural Networks for Document Classification\"\n","Figure1. For fair compasrsion, we encoding the whole document as a vector of 50 and add 3 CNN layers to extract the information\n","Then, follow by one block mention in paper\n","```\n","conv1.weight     torch.Size([32, 1, 3])\n","conv1.bias       torch.Size([32])\n","Bn1.weight       torch.Size([32])\n","Bn1.bias         torch.Size([32])\n","Bn1.running_mean         torch.Size([32])\n","Bn1.running_var          torch.Size([32])\n","Bn1.num_batches_tracked          torch.Size([])\n","conv2.weight     torch.Size([32, 1, 3])\n","conv2.bias       torch.Size([32])\n","Bn2.weight       torch.Size([32])\n","Bn2.bias         torch.Size([32])\n","Bn2.running_mean         torch.Size([32])\n","Bn2.running_var          torch.Size([32])\n","Bn2.num_batches_tracked          torch.Size([])\n","conv3.weight     torch.Size([32, 1, 3])\n","conv3.bias       torch.Size([32])\n","Bn3.weight       torch.Size([32])\n","Bn3.bias         torch.Size([32])\n","Bn3.running_mean         torch.Size([32])\n","Bn3.running_var          torch.Size([32])\n","Bn3.num_batches_tracked          torch.Size([])\n","bi_lstm1.weight_ih_l0    torch.Size([400, 960])\n","bi_lstm1.weight_hh_l0    torch.Size([400, 100])\n","bi_lstm1.bias_ih_l0      torch.Size([400])\n","bi_lstm1.bias_hh_l0      torch.Size([400])\n","fc1.weight       torch.Size([100, 100])\n","fc1.bias         torch.Size([100])\n","self_attn_1.in_proj_weight       torch.Size([300, 100])\n","self_attn_1.in_proj_bias         torch.Size([300])\n","self_attn_1.out_proj.weight      torch.Size([100, 100])\n","self_attn_1.out_proj.bias        torch.Size([100])\n","fc2.weight       torch.Size([5, 100])\n","fc2.bias         torch.Size([5])\n","```\n","self.self_attn_1: This is a multi-head self-attention mechanism that is a core component of the Transformer model. The nn.MultiheadAttention module expects input of size (seq_len, batch, embed_dim), where seq_len is the sequence length, batch is the batch size, and embed_dim is the embedding dimension (must be divisible by the number of attention heads). It applies multiple \"heads\" of attention to the input, and then concatenates the output of these heads and linearly transforms it. This allows the network to focus on different parts of the input for each head.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcaxvGyjMLef"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # Define the first convolutional layer\n","        self.conv1 = nn.Conv1d(1, 32, 3, 1,1, bias=True)\n","        # Define the batch normalization layer for the first conv layer\n","        self.Bn1 = nn.BatchNorm1d(32)\n","        # Define the max pooling layer for the first conv layer\n","        self.pool1=nn.MaxPool1d(kernel_size=5, stride=5)\n","\n","        self.conv2 = nn.Conv1d(1, 32, 3, 1,1, bias=True)\n","        self.Bn2 = nn.BatchNorm1d(32)\n","        self.pool2=nn.MaxPool1d(kernel_size=5, stride=5)\n","\n","        self.conv3 = nn.Conv1d(1, 32, 3, 1,1, bias=True)\n","        self.Bn3 = nn.BatchNorm1d(32)\n","        self.pool3=nn.MaxPool1d(kernel_size=5, stride=5)\n","\n","        # Define LSTM layer with input size of 960 and hidden size of 100\n","        self.bi_lstm1 = nn.LSTM(input_size=960, hidden_size=100, num_layers=1, batch_first=True, bidirectional=False)\n","        # Define the first fully connected layer after LSTM\n","        self.fc1 = nn.Linear(100, 100, bias=True)\n","        # Define self-attention layer\n","        self.self_attn_1 = nn.MultiheadAttention(embed_dim=100, num_heads=4)\n","        # Define the final fully connected layer for classification\n","        self.fc2 = nn.Linear(100, 5, bias=True)\n","\n","    def forward(self, x):\n","        # Pass input through the first convolutional layer, then through the ReLU activation function, then through max pooling\n","        x_layer1 = self.pool1(F.relu(self.Bn1(self.conv1(x))))\n","        x_layer2 = self.pool1(F.relu(self.Bn2(self.conv2(x))))\n","        x_layer3 = self.pool1(F.relu(self.Bn3(self.conv3(x))))\n","        # Concatenate the outputs of the three layers along the channel dimension\n","        x = torch.cat((x_layer1, x_layer2,x_layer3), 1)\n","\n","        # Flatten the tensor for the fully connected layers\n","        x = torch.flatten(x, 1)\n","\n","        # Pass input through the LSTM layer\n","        x, _ = self.bi_lstm1(x)\n","        # Pass output of LSTM layer through the first fully connected layer, then through the ReLU activation function\n","        x = F.relu(self.fc1(x))\n","        # Reshape the tensor for the self-attention layer\n","        x = x.view(-1, 1, 100)\n","        # Pass tensor through the self-attention layer\n","        x, _ = self.self_attn_1(x.permute(1, 0, 2), x.permute(1, 0, 2), x.permute(1, 0, 2))\n","        # Reshape tensor back to original shape\n","        x = x.permute(1, 0, 2)\n","        x = x.view(-1, 100)\n","        # Pass the output through the final fully connected layer for classification\n","        x = self.fc2(x)\n","\n","        return x\n","\n"]},{"cell_type":"markdown","metadata":{"id":"y2tLMOBDYdT7"},"source":["# **Training and Testing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRYHsopyMYYU"},"outputs":[],"source":["def train(args, model, device, train_loader, optimizer, epoch):\n","    model.train()  # Set the model to training mode\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):  # Loop over each batch from the training set\n","        data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n","\n","        target = target-1  # Adjust the target values (Moving 1-5 to 0-4  for easy training)\n","        target = target.long()  # Make sure that target data is long type (necessary for loss function)\n","\n","        optimizer.zero_grad()  # Clear gradients from the previous training step\n","        output = model(data)  # Run forward pass (model predictions)\n","\n","        loss = F.cross_entropy(output, target)  # Calculate the loss between the output and target\n","        loss.backward()  # Perform backpropagation (calculate gradients of loss w.r.t. parameters)\n","        optimizer.step()  # Update the model parameters\n","\n","        if batch_idx % args.log_interval == 0:  # Print log info for specified interval\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.item()))\n","\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()  # Set the model to evaluation mode\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n","        for data, target in test_loader:  # Loop over each batch from the testing set\n","            data, target = data.to(device), target.to(device)  # Move the data to the device that is used\n","            target = target-1  # Adjust the target values\n","            output = model(data)  # Run forward pass (model predictions)\n","            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability as the predicted output\n","            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n","\n","    test_loss /= len(test_loader.dataset)  # Calculate the average loss\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n","    return correct  # Return the number of correctly classified samples\n"]},{"cell_type":"markdown","metadata":{"id":"vafhcsv9Yiql"},"source":["# **Hyperparameter**\n","\n","We use only cpu here as an example. learning rate is set as 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wlKYHWEMdwi"},"outputs":[],"source":["class Args:\n","  epochs = 10\n","  lr = 1.0\n","  use_cuda=False\n","  gamma = 0.7\n","  log_interval = 10\n","  no_cuda = False\n","  seed = 1\n","\n","args = Args()"]},{"cell_type":"markdown","metadata":{"id":"HpgnI9XQR8x5"},"source":["# **Load Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"niXJRCXUR9Uh","outputId":"0a7fdc81-f664-4d24-fb22-9280fe724345","executionInfo":{"status":"ok","timestamp":1691931106265,"user_tz":-480,"elapsed":15531,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1CCIfElCaURQbuYvHZiL445UQIRzmmuM7\n","To: /content/train_vectors.pt\n","100%|██████████| 80.0M/80.0M [00:03<00:00, 25.6MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1bwkg7XdmH6Mkp_tkAakCbxJMWNAXJU43\n","To: /content/train_labels.pt\n","100%|██████████| 3.20M/3.20M [00:00<00:00, 16.9MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1fprUkqC9Qb-y1eDRZt0gA4-4gS941TUo\n","To: /content/test_vectors.pt\n","100%|██████████| 20.0M/20.0M [00:00<00:00, 45.7MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1VwOqpW7DZPhqAGDrreVwhtzCB2lUc_LD\n","To: /content/test_labels.pt\n","100%|██████████| 801k/801k [00:00<00:00, 5.89MB/s]\n"]}],"source":["from google.colab import drive\n","\n","import gdown\n","\n","file_id = '1CCIfElCaURQbuYvHZiL445UQIRzmmuM7'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'train_vectors.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1bwkg7XdmH6Mkp_tkAakCbxJMWNAXJU43'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'train_labels.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1fprUkqC9Qb-y1eDRZt0gA4-4gS941TUo'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'test_vectors.pt'\n","gdown.download(url, output, quiet=False)\n","\n","file_id = '1VwOqpW7DZPhqAGDrreVwhtzCB2lUc_LD'\n","url = f'https://drive.google.com/uc?id={file_id}'\n","output = 'test_labels.pt'\n","gdown.download(url, output, quiet=False)\n","\n","train_vectors = torch.load('train_vectors.pt')\n","train_labels = torch.load('train_labels.pt')\n","test_vectors = torch.load('test_vectors.pt')\n","test_labels = torch.load('test_labels.pt')\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x_jwrzWbbENR"},"source":["# **Start training and testing**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPYD-LvcMi6w","outputId":"3f09a511-ff9f-40f7-caa5-b6bb0410105c","executionInfo":{"status":"ok","timestamp":1691931987065,"user_tz":-480,"elapsed":875091,"user":{"displayName":"Zl Y","userId":"17409724740339786761"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["conv1.weight \t torch.Size([32, 1, 3])\n","conv1.bias \t torch.Size([32])\n","Bn1.weight \t torch.Size([32])\n","Bn1.bias \t torch.Size([32])\n","Bn1.running_mean \t torch.Size([32])\n","Bn1.running_var \t torch.Size([32])\n","Bn1.num_batches_tracked \t torch.Size([])\n","conv2.weight \t torch.Size([32, 1, 3])\n","conv2.bias \t torch.Size([32])\n","Bn2.weight \t torch.Size([32])\n","Bn2.bias \t torch.Size([32])\n","Bn2.running_mean \t torch.Size([32])\n","Bn2.running_var \t torch.Size([32])\n","Bn2.num_batches_tracked \t torch.Size([])\n","conv3.weight \t torch.Size([32, 1, 3])\n","conv3.bias \t torch.Size([32])\n","Bn3.weight \t torch.Size([32])\n","Bn3.bias \t torch.Size([32])\n","Bn3.running_mean \t torch.Size([32])\n","Bn3.running_var \t torch.Size([32])\n","Bn3.num_batches_tracked \t torch.Size([])\n","bi_lstm1.weight_ih_l0 \t torch.Size([400, 960])\n","bi_lstm1.weight_hh_l0 \t torch.Size([400, 100])\n","bi_lstm1.bias_ih_l0 \t torch.Size([400])\n","bi_lstm1.bias_hh_l0 \t torch.Size([400])\n","fc1.weight \t torch.Size([100, 100])\n","fc1.bias \t torch.Size([100])\n","self_attn_1.in_proj_weight \t torch.Size([300, 100])\n","self_attn_1.in_proj_bias \t torch.Size([300])\n","self_attn_1.out_proj.weight \t torch.Size([100, 100])\n","self_attn_1.out_proj.bias \t torch.Size([100])\n","fc2.weight \t torch.Size([5, 100])\n","fc2.bias \t torch.Size([5])\n","Train Epoch: 1 [0/400000 (0%)]\tLoss: 1.613145\n","Train Epoch: 1 [6400/400000 (2%)]\tLoss: 1.548903\n","Train Epoch: 1 [12800/400000 (3%)]\tLoss: 1.718163\n","Train Epoch: 1 [19200/400000 (5%)]\tLoss: 1.558329\n","Train Epoch: 1 [25600/400000 (6%)]\tLoss: 1.725064\n","Train Epoch: 1 [32000/400000 (8%)]\tLoss: 1.734213\n","Train Epoch: 1 [38400/400000 (10%)]\tLoss: 1.386108\n","Train Epoch: 1 [44800/400000 (11%)]\tLoss: 1.478534\n","Train Epoch: 1 [51200/400000 (13%)]\tLoss: 1.428638\n","Train Epoch: 1 [57600/400000 (14%)]\tLoss: 1.271994\n","Train Epoch: 1 [64000/400000 (16%)]\tLoss: 1.331311\n","Train Epoch: 1 [70400/400000 (18%)]\tLoss: 1.483674\n","Train Epoch: 1 [76800/400000 (19%)]\tLoss: 1.396280\n","Train Epoch: 1 [83200/400000 (21%)]\tLoss: 1.428215\n","Train Epoch: 1 [89600/400000 (22%)]\tLoss: 1.209748\n","Train Epoch: 1 [96000/400000 (24%)]\tLoss: 1.217964\n","Train Epoch: 1 [102400/400000 (26%)]\tLoss: 1.288226\n","Train Epoch: 1 [108800/400000 (27%)]\tLoss: 1.190915\n","Train Epoch: 1 [115200/400000 (29%)]\tLoss: 1.188606\n","Train Epoch: 1 [121600/400000 (30%)]\tLoss: 1.218551\n","Train Epoch: 1 [128000/400000 (32%)]\tLoss: 1.201905\n","Train Epoch: 1 [134400/400000 (34%)]\tLoss: 1.306980\n","Train Epoch: 1 [140800/400000 (35%)]\tLoss: 1.204735\n","Train Epoch: 1 [147200/400000 (37%)]\tLoss: 1.199990\n","Train Epoch: 1 [153600/400000 (38%)]\tLoss: 1.221047\n","Train Epoch: 1 [160000/400000 (40%)]\tLoss: 1.167951\n","Train Epoch: 1 [166400/400000 (42%)]\tLoss: 1.191399\n","Train Epoch: 1 [172800/400000 (43%)]\tLoss: 1.323338\n","Train Epoch: 1 [179200/400000 (45%)]\tLoss: 1.161601\n","Train Epoch: 1 [185600/400000 (46%)]\tLoss: 1.243313\n","Train Epoch: 1 [192000/400000 (48%)]\tLoss: 1.227198\n","Train Epoch: 1 [198400/400000 (50%)]\tLoss: 1.080174\n","Train Epoch: 1 [204800/400000 (51%)]\tLoss: 1.217089\n","Train Epoch: 1 [211200/400000 (53%)]\tLoss: 1.124242\n","Train Epoch: 1 [217600/400000 (54%)]\tLoss: 1.177899\n","Train Epoch: 1 [224000/400000 (56%)]\tLoss: 1.240869\n","Train Epoch: 1 [230400/400000 (58%)]\tLoss: 1.268336\n","Train Epoch: 1 [236800/400000 (59%)]\tLoss: 1.121474\n","Train Epoch: 1 [243200/400000 (61%)]\tLoss: 1.147088\n","Train Epoch: 1 [249600/400000 (62%)]\tLoss: 1.142177\n","Train Epoch: 1 [256000/400000 (64%)]\tLoss: 1.149491\n","Train Epoch: 1 [262400/400000 (66%)]\tLoss: 1.201835\n","Train Epoch: 1 [268800/400000 (67%)]\tLoss: 1.126332\n","Train Epoch: 1 [275200/400000 (69%)]\tLoss: 1.106000\n","Train Epoch: 1 [281600/400000 (70%)]\tLoss: 1.199101\n","Train Epoch: 1 [288000/400000 (72%)]\tLoss: 1.148482\n","Train Epoch: 1 [294400/400000 (74%)]\tLoss: 1.104860\n","Train Epoch: 1 [300800/400000 (75%)]\tLoss: 1.123132\n","Train Epoch: 1 [307200/400000 (77%)]\tLoss: 1.150474\n","Train Epoch: 1 [313600/400000 (78%)]\tLoss: 1.141769\n","Train Epoch: 1 [320000/400000 (80%)]\tLoss: 1.117476\n","Train Epoch: 1 [326400/400000 (82%)]\tLoss: 1.105317\n","Train Epoch: 1 [332800/400000 (83%)]\tLoss: 1.078671\n","Train Epoch: 1 [339200/400000 (85%)]\tLoss: 1.114302\n","Train Epoch: 1 [345600/400000 (86%)]\tLoss: 1.129378\n","Train Epoch: 1 [352000/400000 (88%)]\tLoss: 1.099197\n","Train Epoch: 1 [358400/400000 (90%)]\tLoss: 1.100518\n","Train Epoch: 1 [364800/400000 (91%)]\tLoss: 1.230891\n","Train Epoch: 1 [371200/400000 (93%)]\tLoss: 1.089672\n","Train Epoch: 1 [377600/400000 (94%)]\tLoss: 1.102679\n","Train Epoch: 1 [384000/400000 (96%)]\tLoss: 1.086008\n","Train Epoch: 1 [390400/400000 (98%)]\tLoss: 1.113637\n","Train Epoch: 1 [396800/400000 (99%)]\tLoss: 1.103300\n","\n","Test set: Average loss: 0.0000, Accuracy: 49060/100000 (49%)\n","\n","Train Epoch: 2 [0/400000 (0%)]\tLoss: 1.115356\n","Train Epoch: 2 [6400/400000 (2%)]\tLoss: 1.063572\n","Train Epoch: 2 [12800/400000 (3%)]\tLoss: 1.122555\n","Train Epoch: 2 [19200/400000 (5%)]\tLoss: 1.150661\n","Train Epoch: 2 [25600/400000 (6%)]\tLoss: 1.024243\n","Train Epoch: 2 [32000/400000 (8%)]\tLoss: 1.059829\n","Train Epoch: 2 [38400/400000 (10%)]\tLoss: 1.033555\n","Train Epoch: 2 [44800/400000 (11%)]\tLoss: 1.088921\n","Train Epoch: 2 [51200/400000 (13%)]\tLoss: 1.072295\n","Train Epoch: 2 [57600/400000 (14%)]\tLoss: 1.035151\n","Train Epoch: 2 [64000/400000 (16%)]\tLoss: 1.138456\n","Train Epoch: 2 [70400/400000 (18%)]\tLoss: 1.009244\n","Train Epoch: 2 [76800/400000 (19%)]\tLoss: 1.004575\n","Train Epoch: 2 [83200/400000 (21%)]\tLoss: 1.067531\n","Train Epoch: 2 [89600/400000 (22%)]\tLoss: 1.041376\n","Train Epoch: 2 [96000/400000 (24%)]\tLoss: 1.133742\n","Train Epoch: 2 [102400/400000 (26%)]\tLoss: 1.082608\n","Train Epoch: 2 [108800/400000 (27%)]\tLoss: 1.138181\n","Train Epoch: 2 [115200/400000 (29%)]\tLoss: 1.067822\n","Train Epoch: 2 [121600/400000 (30%)]\tLoss: 1.110171\n","Train Epoch: 2 [128000/400000 (32%)]\tLoss: 1.025495\n","Train Epoch: 2 [134400/400000 (34%)]\tLoss: 1.078045\n","Train Epoch: 2 [140800/400000 (35%)]\tLoss: 1.083822\n","Train Epoch: 2 [147200/400000 (37%)]\tLoss: 1.092114\n","Train Epoch: 2 [153600/400000 (38%)]\tLoss: 1.083860\n","Train Epoch: 2 [160000/400000 (40%)]\tLoss: 1.176291\n","Train Epoch: 2 [166400/400000 (42%)]\tLoss: 1.032965\n","Train Epoch: 2 [172800/400000 (43%)]\tLoss: 1.033899\n","Train Epoch: 2 [179200/400000 (45%)]\tLoss: 1.124853\n","Train Epoch: 2 [185600/400000 (46%)]\tLoss: 1.068265\n","Train Epoch: 2 [192000/400000 (48%)]\tLoss: 1.041214\n","Train Epoch: 2 [198400/400000 (50%)]\tLoss: 1.096505\n","Train Epoch: 2 [204800/400000 (51%)]\tLoss: 1.070534\n","Train Epoch: 2 [211200/400000 (53%)]\tLoss: 1.082618\n","Train Epoch: 2 [217600/400000 (54%)]\tLoss: 1.038179\n","Train Epoch: 2 [224000/400000 (56%)]\tLoss: 1.011259\n","Train Epoch: 2 [230400/400000 (58%)]\tLoss: 1.063700\n","Train Epoch: 2 [236800/400000 (59%)]\tLoss: 1.045951\n","Train Epoch: 2 [243200/400000 (61%)]\tLoss: 1.082484\n","Train Epoch: 2 [249600/400000 (62%)]\tLoss: 1.073920\n","Train Epoch: 2 [256000/400000 (64%)]\tLoss: 1.062448\n","Train Epoch: 2 [262400/400000 (66%)]\tLoss: 1.120238\n","Train Epoch: 2 [268800/400000 (67%)]\tLoss: 1.045936\n","Train Epoch: 2 [275200/400000 (69%)]\tLoss: 1.034478\n","Train Epoch: 2 [281600/400000 (70%)]\tLoss: 1.060052\n","Train Epoch: 2 [288000/400000 (72%)]\tLoss: 1.047020\n","Train Epoch: 2 [294400/400000 (74%)]\tLoss: 1.083827\n","Train Epoch: 2 [300800/400000 (75%)]\tLoss: 1.073764\n","Train Epoch: 2 [307200/400000 (77%)]\tLoss: 1.088489\n","Train Epoch: 2 [313600/400000 (78%)]\tLoss: 1.021459\n","Train Epoch: 2 [320000/400000 (80%)]\tLoss: 1.107372\n","Train Epoch: 2 [326400/400000 (82%)]\tLoss: 1.027678\n","Train Epoch: 2 [332800/400000 (83%)]\tLoss: 1.130083\n","Train Epoch: 2 [339200/400000 (85%)]\tLoss: 1.108892\n","Train Epoch: 2 [345600/400000 (86%)]\tLoss: 1.044582\n","Train Epoch: 2 [352000/400000 (88%)]\tLoss: 1.115849\n","Train Epoch: 2 [358400/400000 (90%)]\tLoss: 1.025075\n","Train Epoch: 2 [364800/400000 (91%)]\tLoss: 1.026449\n","Train Epoch: 2 [371200/400000 (93%)]\tLoss: 1.045220\n","Train Epoch: 2 [377600/400000 (94%)]\tLoss: 1.040038\n","Train Epoch: 2 [384000/400000 (96%)]\tLoss: 1.058683\n","Train Epoch: 2 [390400/400000 (98%)]\tLoss: 1.027764\n","Train Epoch: 2 [396800/400000 (99%)]\tLoss: 1.025697\n","\n","Test set: Average loss: 0.0000, Accuracy: 53455/100000 (53%)\n","\n","Train Epoch: 3 [0/400000 (0%)]\tLoss: 1.066615\n","Train Epoch: 3 [6400/400000 (2%)]\tLoss: 1.003811\n","Train Epoch: 3 [12800/400000 (3%)]\tLoss: 1.088612\n","Train Epoch: 3 [19200/400000 (5%)]\tLoss: 1.015237\n","Train Epoch: 3 [25600/400000 (6%)]\tLoss: 1.090189\n","Train Epoch: 3 [32000/400000 (8%)]\tLoss: 1.024438\n","Train Epoch: 3 [38400/400000 (10%)]\tLoss: 1.080826\n","Train Epoch: 3 [44800/400000 (11%)]\tLoss: 1.054802\n","Train Epoch: 3 [51200/400000 (13%)]\tLoss: 1.106339\n","Train Epoch: 3 [57600/400000 (14%)]\tLoss: 1.052222\n","Train Epoch: 3 [64000/400000 (16%)]\tLoss: 1.048243\n","Train Epoch: 3 [70400/400000 (18%)]\tLoss: 1.020748\n","Train Epoch: 3 [76800/400000 (19%)]\tLoss: 1.039002\n","Train Epoch: 3 [83200/400000 (21%)]\tLoss: 0.999118\n","Train Epoch: 3 [89600/400000 (22%)]\tLoss: 1.000617\n","Train Epoch: 3 [96000/400000 (24%)]\tLoss: 1.059980\n","Train Epoch: 3 [102400/400000 (26%)]\tLoss: 1.057807\n","Train Epoch: 3 [108800/400000 (27%)]\tLoss: 1.003431\n","Train Epoch: 3 [115200/400000 (29%)]\tLoss: 1.033546\n","Train Epoch: 3 [121600/400000 (30%)]\tLoss: 1.019001\n","Train Epoch: 3 [128000/400000 (32%)]\tLoss: 1.123697\n","Train Epoch: 3 [134400/400000 (34%)]\tLoss: 1.033395\n","Train Epoch: 3 [140800/400000 (35%)]\tLoss: 1.039029\n","Train Epoch: 3 [147200/400000 (37%)]\tLoss: 1.074319\n","Train Epoch: 3 [153600/400000 (38%)]\tLoss: 1.063734\n","Train Epoch: 3 [160000/400000 (40%)]\tLoss: 1.049608\n","Train Epoch: 3 [166400/400000 (42%)]\tLoss: 1.064050\n","Train Epoch: 3 [172800/400000 (43%)]\tLoss: 1.001590\n","Train Epoch: 3 [179200/400000 (45%)]\tLoss: 1.064841\n","Train Epoch: 3 [185600/400000 (46%)]\tLoss: 1.021698\n","Train Epoch: 3 [192000/400000 (48%)]\tLoss: 1.034722\n","Train Epoch: 3 [198400/400000 (50%)]\tLoss: 1.024854\n","Train Epoch: 3 [204800/400000 (51%)]\tLoss: 1.060657\n","Train Epoch: 3 [211200/400000 (53%)]\tLoss: 1.003423\n","Train Epoch: 3 [217600/400000 (54%)]\tLoss: 1.042608\n","Train Epoch: 3 [224000/400000 (56%)]\tLoss: 1.039394\n","Train Epoch: 3 [230400/400000 (58%)]\tLoss: 1.072711\n","Train Epoch: 3 [236800/400000 (59%)]\tLoss: 1.044500\n","Train Epoch: 3 [243200/400000 (61%)]\tLoss: 1.086947\n","Train Epoch: 3 [249600/400000 (62%)]\tLoss: 1.075141\n","Train Epoch: 3 [256000/400000 (64%)]\tLoss: 1.095230\n","Train Epoch: 3 [262400/400000 (66%)]\tLoss: 1.067958\n","Train Epoch: 3 [268800/400000 (67%)]\tLoss: 1.021095\n","Train Epoch: 3 [275200/400000 (69%)]\tLoss: 1.116108\n","Train Epoch: 3 [281600/400000 (70%)]\tLoss: 1.070657\n","Train Epoch: 3 [288000/400000 (72%)]\tLoss: 1.005716\n","Train Epoch: 3 [294400/400000 (74%)]\tLoss: 1.040593\n","Train Epoch: 3 [300800/400000 (75%)]\tLoss: 1.026038\n","Train Epoch: 3 [307200/400000 (77%)]\tLoss: 1.034410\n","Train Epoch: 3 [313600/400000 (78%)]\tLoss: 1.066191\n","Train Epoch: 3 [320000/400000 (80%)]\tLoss: 1.060345\n","Train Epoch: 3 [326400/400000 (82%)]\tLoss: 1.049745\n","Train Epoch: 3 [332800/400000 (83%)]\tLoss: 1.032972\n","Train Epoch: 3 [339200/400000 (85%)]\tLoss: 1.066451\n","Train Epoch: 3 [345600/400000 (86%)]\tLoss: 1.032686\n","Train Epoch: 3 [352000/400000 (88%)]\tLoss: 1.050518\n","Train Epoch: 3 [358400/400000 (90%)]\tLoss: 1.050357\n","Train Epoch: 3 [364800/400000 (91%)]\tLoss: 1.017889\n","Train Epoch: 3 [371200/400000 (93%)]\tLoss: 1.098536\n","Train Epoch: 3 [377600/400000 (94%)]\tLoss: 1.106689\n","Train Epoch: 3 [384000/400000 (96%)]\tLoss: 1.071750\n","Train Epoch: 3 [390400/400000 (98%)]\tLoss: 1.108796\n","Train Epoch: 3 [396800/400000 (99%)]\tLoss: 1.057723\n","\n","Test set: Average loss: 0.0000, Accuracy: 54305/100000 (54%)\n","\n","Train Epoch: 4 [0/400000 (0%)]\tLoss: 1.060450\n","Train Epoch: 4 [6400/400000 (2%)]\tLoss: 1.042045\n","Train Epoch: 4 [12800/400000 (3%)]\tLoss: 1.046866\n","Train Epoch: 4 [19200/400000 (5%)]\tLoss: 1.049163\n","Train Epoch: 4 [25600/400000 (6%)]\tLoss: 1.010639\n","Train Epoch: 4 [32000/400000 (8%)]\tLoss: 1.041081\n","Train Epoch: 4 [38400/400000 (10%)]\tLoss: 1.041815\n","Train Epoch: 4 [44800/400000 (11%)]\tLoss: 0.996763\n","Train Epoch: 4 [51200/400000 (13%)]\tLoss: 1.064572\n","Train Epoch: 4 [57600/400000 (14%)]\tLoss: 1.064505\n","Train Epoch: 4 [64000/400000 (16%)]\tLoss: 1.018245\n","Train Epoch: 4 [70400/400000 (18%)]\tLoss: 1.009640\n","Train Epoch: 4 [76800/400000 (19%)]\tLoss: 1.029169\n","Train Epoch: 4 [83200/400000 (21%)]\tLoss: 1.040233\n","Train Epoch: 4 [89600/400000 (22%)]\tLoss: 0.989778\n","Train Epoch: 4 [96000/400000 (24%)]\tLoss: 1.081019\n","Train Epoch: 4 [102400/400000 (26%)]\tLoss: 1.074598\n","Train Epoch: 4 [108800/400000 (27%)]\tLoss: 1.041450\n","Train Epoch: 4 [115200/400000 (29%)]\tLoss: 1.055056\n","Train Epoch: 4 [121600/400000 (30%)]\tLoss: 1.047577\n","Train Epoch: 4 [128000/400000 (32%)]\tLoss: 1.030832\n","Train Epoch: 4 [134400/400000 (34%)]\tLoss: 1.059406\n","Train Epoch: 4 [140800/400000 (35%)]\tLoss: 1.006815\n","Train Epoch: 4 [147200/400000 (37%)]\tLoss: 1.043980\n","Train Epoch: 4 [153600/400000 (38%)]\tLoss: 1.027697\n","Train Epoch: 4 [160000/400000 (40%)]\tLoss: 1.011296\n","Train Epoch: 4 [166400/400000 (42%)]\tLoss: 1.031639\n","Train Epoch: 4 [172800/400000 (43%)]\tLoss: 1.018691\n","Train Epoch: 4 [179200/400000 (45%)]\tLoss: 0.990396\n","Train Epoch: 4 [185600/400000 (46%)]\tLoss: 1.026381\n","Train Epoch: 4 [192000/400000 (48%)]\tLoss: 1.081472\n","Train Epoch: 4 [198400/400000 (50%)]\tLoss: 1.017058\n","Train Epoch: 4 [204800/400000 (51%)]\tLoss: 1.043374\n","Train Epoch: 4 [211200/400000 (53%)]\tLoss: 1.022569\n","Train Epoch: 4 [217600/400000 (54%)]\tLoss: 1.066397\n","Train Epoch: 4 [224000/400000 (56%)]\tLoss: 1.056849\n","Train Epoch: 4 [230400/400000 (58%)]\tLoss: 0.989203\n","Train Epoch: 4 [236800/400000 (59%)]\tLoss: 1.019037\n","Train Epoch: 4 [243200/400000 (61%)]\tLoss: 1.022841\n","Train Epoch: 4 [249600/400000 (62%)]\tLoss: 1.026891\n","Train Epoch: 4 [256000/400000 (64%)]\tLoss: 1.073299\n","Train Epoch: 4 [262400/400000 (66%)]\tLoss: 0.960531\n","Train Epoch: 4 [268800/400000 (67%)]\tLoss: 1.066282\n","Train Epoch: 4 [275200/400000 (69%)]\tLoss: 1.066018\n","Train Epoch: 4 [281600/400000 (70%)]\tLoss: 1.003482\n","Train Epoch: 4 [288000/400000 (72%)]\tLoss: 1.044325\n","Train Epoch: 4 [294400/400000 (74%)]\tLoss: 1.053192\n","Train Epoch: 4 [300800/400000 (75%)]\tLoss: 1.025410\n","Train Epoch: 4 [307200/400000 (77%)]\tLoss: 1.059913\n","Train Epoch: 4 [313600/400000 (78%)]\tLoss: 1.079948\n","Train Epoch: 4 [320000/400000 (80%)]\tLoss: 1.031016\n","Train Epoch: 4 [326400/400000 (82%)]\tLoss: 1.054934\n","Train Epoch: 4 [332800/400000 (83%)]\tLoss: 1.009068\n","Train Epoch: 4 [339200/400000 (85%)]\tLoss: 1.007046\n","Train Epoch: 4 [345600/400000 (86%)]\tLoss: 1.048926\n","Train Epoch: 4 [352000/400000 (88%)]\tLoss: 1.017882\n","Train Epoch: 4 [358400/400000 (90%)]\tLoss: 1.015696\n","Train Epoch: 4 [364800/400000 (91%)]\tLoss: 1.082552\n","Train Epoch: 4 [371200/400000 (93%)]\tLoss: 1.037533\n","Train Epoch: 4 [377600/400000 (94%)]\tLoss: 1.057287\n","Train Epoch: 4 [384000/400000 (96%)]\tLoss: 1.001524\n","Train Epoch: 4 [390400/400000 (98%)]\tLoss: 1.013487\n","Train Epoch: 4 [396800/400000 (99%)]\tLoss: 1.032488\n","\n","Test set: Average loss: 0.0000, Accuracy: 54421/100000 (54%)\n","\n","Train Epoch: 5 [0/400000 (0%)]\tLoss: 1.011492\n","Train Epoch: 5 [6400/400000 (2%)]\tLoss: 1.049036\n","Train Epoch: 5 [12800/400000 (3%)]\tLoss: 1.067021\n","Train Epoch: 5 [19200/400000 (5%)]\tLoss: 1.078329\n","Train Epoch: 5 [25600/400000 (6%)]\tLoss: 0.996586\n","Train Epoch: 5 [32000/400000 (8%)]\tLoss: 1.075886\n","Train Epoch: 5 [38400/400000 (10%)]\tLoss: 0.999718\n","Train Epoch: 5 [44800/400000 (11%)]\tLoss: 0.996919\n","Train Epoch: 5 [51200/400000 (13%)]\tLoss: 1.011703\n","Train Epoch: 5 [57600/400000 (14%)]\tLoss: 1.029318\n","Train Epoch: 5 [64000/400000 (16%)]\tLoss: 1.039526\n","Train Epoch: 5 [70400/400000 (18%)]\tLoss: 1.047452\n","Train Epoch: 5 [76800/400000 (19%)]\tLoss: 1.043934\n","Train Epoch: 5 [83200/400000 (21%)]\tLoss: 1.019691\n","Train Epoch: 5 [89600/400000 (22%)]\tLoss: 1.005111\n","Train Epoch: 5 [96000/400000 (24%)]\tLoss: 1.054541\n","Train Epoch: 5 [102400/400000 (26%)]\tLoss: 0.990563\n","Train Epoch: 5 [108800/400000 (27%)]\tLoss: 1.031817\n","Train Epoch: 5 [115200/400000 (29%)]\tLoss: 0.984689\n","Train Epoch: 5 [121600/400000 (30%)]\tLoss: 1.020828\n","Train Epoch: 5 [128000/400000 (32%)]\tLoss: 1.031960\n","Train Epoch: 5 [134400/400000 (34%)]\tLoss: 1.007935\n","Train Epoch: 5 [140800/400000 (35%)]\tLoss: 1.051627\n","Train Epoch: 5 [147200/400000 (37%)]\tLoss: 1.017606\n","Train Epoch: 5 [153600/400000 (38%)]\tLoss: 0.989704\n","Train Epoch: 5 [160000/400000 (40%)]\tLoss: 1.049503\n","Train Epoch: 5 [166400/400000 (42%)]\tLoss: 1.013011\n","Train Epoch: 5 [172800/400000 (43%)]\tLoss: 1.044698\n","Train Epoch: 5 [179200/400000 (45%)]\tLoss: 1.059259\n","Train Epoch: 5 [185600/400000 (46%)]\tLoss: 1.066267\n","Train Epoch: 5 [192000/400000 (48%)]\tLoss: 1.017028\n","Train Epoch: 5 [198400/400000 (50%)]\tLoss: 1.046446\n","Train Epoch: 5 [204800/400000 (51%)]\tLoss: 0.991454\n","Train Epoch: 5 [211200/400000 (53%)]\tLoss: 1.022859\n","Train Epoch: 5 [217600/400000 (54%)]\tLoss: 1.009950\n","Train Epoch: 5 [224000/400000 (56%)]\tLoss: 1.021508\n","Train Epoch: 5 [230400/400000 (58%)]\tLoss: 0.993108\n","Train Epoch: 5 [236800/400000 (59%)]\tLoss: 1.038342\n","Train Epoch: 5 [243200/400000 (61%)]\tLoss: 1.034897\n","Train Epoch: 5 [249600/400000 (62%)]\tLoss: 1.055225\n","Train Epoch: 5 [256000/400000 (64%)]\tLoss: 1.092307\n","Train Epoch: 5 [262400/400000 (66%)]\tLoss: 0.999088\n","Train Epoch: 5 [268800/400000 (67%)]\tLoss: 1.025956\n","Train Epoch: 5 [275200/400000 (69%)]\tLoss: 1.105887\n","Train Epoch: 5 [281600/400000 (70%)]\tLoss: 1.039748\n","Train Epoch: 5 [288000/400000 (72%)]\tLoss: 1.084786\n","Train Epoch: 5 [294400/400000 (74%)]\tLoss: 1.029432\n","Train Epoch: 5 [300800/400000 (75%)]\tLoss: 1.063433\n","Train Epoch: 5 [307200/400000 (77%)]\tLoss: 1.008095\n","Train Epoch: 5 [313600/400000 (78%)]\tLoss: 1.017685\n","Train Epoch: 5 [320000/400000 (80%)]\tLoss: 1.047376\n","Train Epoch: 5 [326400/400000 (82%)]\tLoss: 1.068566\n","Train Epoch: 5 [332800/400000 (83%)]\tLoss: 1.062064\n","Train Epoch: 5 [339200/400000 (85%)]\tLoss: 0.950803\n","Train Epoch: 5 [345600/400000 (86%)]\tLoss: 1.047012\n","Train Epoch: 5 [352000/400000 (88%)]\tLoss: 1.045463\n","Train Epoch: 5 [358400/400000 (90%)]\tLoss: 1.060638\n","Train Epoch: 5 [364800/400000 (91%)]\tLoss: 1.000671\n","Train Epoch: 5 [371200/400000 (93%)]\tLoss: 1.058565\n","Train Epoch: 5 [377600/400000 (94%)]\tLoss: 1.084905\n","Train Epoch: 5 [384000/400000 (96%)]\tLoss: 0.977977\n","Train Epoch: 5 [390400/400000 (98%)]\tLoss: 0.993106\n","Train Epoch: 5 [396800/400000 (99%)]\tLoss: 1.047832\n","\n","Test set: Average loss: 0.0000, Accuracy: 54598/100000 (55%)\n","\n","Train Epoch: 6 [0/400000 (0%)]\tLoss: 1.104232\n","Train Epoch: 6 [6400/400000 (2%)]\tLoss: 1.061809\n","Train Epoch: 6 [12800/400000 (3%)]\tLoss: 1.058042\n","Train Epoch: 6 [19200/400000 (5%)]\tLoss: 1.015552\n","Train Epoch: 6 [25600/400000 (6%)]\tLoss: 1.018215\n","Train Epoch: 6 [32000/400000 (8%)]\tLoss: 0.996510\n","Train Epoch: 6 [38400/400000 (10%)]\tLoss: 1.045440\n","Train Epoch: 6 [44800/400000 (11%)]\tLoss: 1.038571\n","Train Epoch: 6 [51200/400000 (13%)]\tLoss: 0.977557\n","Train Epoch: 6 [57600/400000 (14%)]\tLoss: 0.989620\n","Train Epoch: 6 [64000/400000 (16%)]\tLoss: 1.017345\n","Train Epoch: 6 [70400/400000 (18%)]\tLoss: 1.021662\n","Train Epoch: 6 [76800/400000 (19%)]\tLoss: 1.029202\n","Train Epoch: 6 [83200/400000 (21%)]\tLoss: 0.993774\n","Train Epoch: 6 [89600/400000 (22%)]\tLoss: 1.018881\n","Train Epoch: 6 [96000/400000 (24%)]\tLoss: 1.013430\n","Train Epoch: 6 [102400/400000 (26%)]\tLoss: 0.982428\n","Train Epoch: 6 [108800/400000 (27%)]\tLoss: 1.017096\n","Train Epoch: 6 [115200/400000 (29%)]\tLoss: 1.006339\n","Train Epoch: 6 [121600/400000 (30%)]\tLoss: 1.032065\n","Train Epoch: 6 [128000/400000 (32%)]\tLoss: 1.032833\n","Train Epoch: 6 [134400/400000 (34%)]\tLoss: 1.009808\n","Train Epoch: 6 [140800/400000 (35%)]\tLoss: 1.035922\n","Train Epoch: 6 [147200/400000 (37%)]\tLoss: 0.986077\n","Train Epoch: 6 [153600/400000 (38%)]\tLoss: 1.022651\n","Train Epoch: 6 [160000/400000 (40%)]\tLoss: 1.064089\n","Train Epoch: 6 [166400/400000 (42%)]\tLoss: 1.023443\n","Train Epoch: 6 [172800/400000 (43%)]\tLoss: 1.043623\n","Train Epoch: 6 [179200/400000 (45%)]\tLoss: 1.021817\n","Train Epoch: 6 [185600/400000 (46%)]\tLoss: 1.044819\n","Train Epoch: 6 [192000/400000 (48%)]\tLoss: 1.034916\n","Train Epoch: 6 [198400/400000 (50%)]\tLoss: 1.100620\n","Train Epoch: 6 [204800/400000 (51%)]\tLoss: 1.034277\n","Train Epoch: 6 [211200/400000 (53%)]\tLoss: 1.039122\n","Train Epoch: 6 [217600/400000 (54%)]\tLoss: 1.011933\n","Train Epoch: 6 [224000/400000 (56%)]\tLoss: 1.042610\n","Train Epoch: 6 [230400/400000 (58%)]\tLoss: 1.036649\n","Train Epoch: 6 [236800/400000 (59%)]\tLoss: 0.981685\n","Train Epoch: 6 [243200/400000 (61%)]\tLoss: 0.993342\n","Train Epoch: 6 [249600/400000 (62%)]\tLoss: 1.008280\n","Train Epoch: 6 [256000/400000 (64%)]\tLoss: 1.029112\n","Train Epoch: 6 [262400/400000 (66%)]\tLoss: 0.991270\n","Train Epoch: 6 [268800/400000 (67%)]\tLoss: 1.057260\n","Train Epoch: 6 [275200/400000 (69%)]\tLoss: 1.017258\n","Train Epoch: 6 [281600/400000 (70%)]\tLoss: 0.972548\n","Train Epoch: 6 [288000/400000 (72%)]\tLoss: 0.998635\n","Train Epoch: 6 [294400/400000 (74%)]\tLoss: 1.005400\n","Train Epoch: 6 [300800/400000 (75%)]\tLoss: 1.017324\n","Train Epoch: 6 [307200/400000 (77%)]\tLoss: 0.998803\n","Train Epoch: 6 [313600/400000 (78%)]\tLoss: 1.043279\n","Train Epoch: 6 [320000/400000 (80%)]\tLoss: 1.025406\n","Train Epoch: 6 [326400/400000 (82%)]\tLoss: 1.053439\n","Train Epoch: 6 [332800/400000 (83%)]\tLoss: 1.091796\n","Train Epoch: 6 [339200/400000 (85%)]\tLoss: 1.033085\n","Train Epoch: 6 [345600/400000 (86%)]\tLoss: 1.008069\n","Train Epoch: 6 [352000/400000 (88%)]\tLoss: 1.028977\n","Train Epoch: 6 [358400/400000 (90%)]\tLoss: 1.044279\n","Train Epoch: 6 [364800/400000 (91%)]\tLoss: 1.053950\n","Train Epoch: 6 [371200/400000 (93%)]\tLoss: 1.055377\n","Train Epoch: 6 [377600/400000 (94%)]\tLoss: 1.070161\n","Train Epoch: 6 [384000/400000 (96%)]\tLoss: 1.029170\n","Train Epoch: 6 [390400/400000 (98%)]\tLoss: 1.011653\n","Train Epoch: 6 [396800/400000 (99%)]\tLoss: 1.003913\n","\n","Test set: Average loss: 0.0000, Accuracy: 54766/100000 (55%)\n","\n","Train Epoch: 7 [0/400000 (0%)]\tLoss: 1.036294\n","Train Epoch: 7 [6400/400000 (2%)]\tLoss: 1.005668\n","Train Epoch: 7 [12800/400000 (3%)]\tLoss: 1.007223\n","Train Epoch: 7 [19200/400000 (5%)]\tLoss: 1.084735\n","Train Epoch: 7 [25600/400000 (6%)]\tLoss: 1.047353\n","Train Epoch: 7 [32000/400000 (8%)]\tLoss: 1.026218\n","Train Epoch: 7 [38400/400000 (10%)]\tLoss: 1.034519\n","Train Epoch: 7 [44800/400000 (11%)]\tLoss: 1.025176\n","Train Epoch: 7 [51200/400000 (13%)]\tLoss: 1.029461\n","Train Epoch: 7 [57600/400000 (14%)]\tLoss: 0.988003\n","Train Epoch: 7 [64000/400000 (16%)]\tLoss: 1.009838\n","Train Epoch: 7 [70400/400000 (18%)]\tLoss: 1.035581\n","Train Epoch: 7 [76800/400000 (19%)]\tLoss: 1.059307\n","Train Epoch: 7 [83200/400000 (21%)]\tLoss: 1.095929\n","Train Epoch: 7 [89600/400000 (22%)]\tLoss: 1.027694\n","Train Epoch: 7 [96000/400000 (24%)]\tLoss: 1.001414\n","Train Epoch: 7 [102400/400000 (26%)]\tLoss: 1.021983\n","Train Epoch: 7 [108800/400000 (27%)]\tLoss: 1.038417\n","Train Epoch: 7 [115200/400000 (29%)]\tLoss: 1.058195\n","Train Epoch: 7 [121600/400000 (30%)]\tLoss: 1.031342\n","Train Epoch: 7 [128000/400000 (32%)]\tLoss: 1.017077\n","Train Epoch: 7 [134400/400000 (34%)]\tLoss: 1.066256\n","Train Epoch: 7 [140800/400000 (35%)]\tLoss: 1.021259\n","Train Epoch: 7 [147200/400000 (37%)]\tLoss: 0.987346\n","Train Epoch: 7 [153600/400000 (38%)]\tLoss: 1.016327\n","Train Epoch: 7 [160000/400000 (40%)]\tLoss: 1.006143\n","Train Epoch: 7 [166400/400000 (42%)]\tLoss: 1.001272\n","Train Epoch: 7 [172800/400000 (43%)]\tLoss: 1.065382\n","Train Epoch: 7 [179200/400000 (45%)]\tLoss: 1.057106\n","Train Epoch: 7 [185600/400000 (46%)]\tLoss: 1.063409\n","Train Epoch: 7 [192000/400000 (48%)]\tLoss: 1.024505\n","Train Epoch: 7 [198400/400000 (50%)]\tLoss: 0.971848\n","Train Epoch: 7 [204800/400000 (51%)]\tLoss: 1.011694\n","Train Epoch: 7 [211200/400000 (53%)]\tLoss: 1.061731\n","Train Epoch: 7 [217600/400000 (54%)]\tLoss: 0.983210\n","Train Epoch: 7 [224000/400000 (56%)]\tLoss: 1.039536\n","Train Epoch: 7 [230400/400000 (58%)]\tLoss: 1.080461\n","Train Epoch: 7 [236800/400000 (59%)]\tLoss: 1.022754\n","Train Epoch: 7 [243200/400000 (61%)]\tLoss: 1.059252\n","Train Epoch: 7 [249600/400000 (62%)]\tLoss: 1.020108\n","Train Epoch: 7 [256000/400000 (64%)]\tLoss: 1.000822\n","Train Epoch: 7 [262400/400000 (66%)]\tLoss: 1.022219\n","Train Epoch: 7 [268800/400000 (67%)]\tLoss: 1.059293\n","Train Epoch: 7 [275200/400000 (69%)]\tLoss: 1.000139\n","Train Epoch: 7 [281600/400000 (70%)]\tLoss: 1.047972\n","Train Epoch: 7 [288000/400000 (72%)]\tLoss: 0.992740\n","Train Epoch: 7 [294400/400000 (74%)]\tLoss: 1.051677\n","Train Epoch: 7 [300800/400000 (75%)]\tLoss: 1.033626\n","Train Epoch: 7 [307200/400000 (77%)]\tLoss: 0.966711\n","Train Epoch: 7 [313600/400000 (78%)]\tLoss: 1.029469\n","Train Epoch: 7 [320000/400000 (80%)]\tLoss: 1.044911\n","Train Epoch: 7 [326400/400000 (82%)]\tLoss: 0.988115\n","Train Epoch: 7 [332800/400000 (83%)]\tLoss: 1.069442\n","Train Epoch: 7 [339200/400000 (85%)]\tLoss: 0.990699\n","Train Epoch: 7 [345600/400000 (86%)]\tLoss: 0.996918\n","Train Epoch: 7 [352000/400000 (88%)]\tLoss: 1.027521\n","Train Epoch: 7 [358400/400000 (90%)]\tLoss: 1.025614\n","Train Epoch: 7 [364800/400000 (91%)]\tLoss: 0.997900\n","Train Epoch: 7 [371200/400000 (93%)]\tLoss: 1.087198\n","Train Epoch: 7 [377600/400000 (94%)]\tLoss: 1.010228\n","Train Epoch: 7 [384000/400000 (96%)]\tLoss: 0.981491\n","Train Epoch: 7 [390400/400000 (98%)]\tLoss: 1.041982\n","Train Epoch: 7 [396800/400000 (99%)]\tLoss: 0.971632\n","\n","Test set: Average loss: 0.0000, Accuracy: 54939/100000 (55%)\n","\n","Train Epoch: 8 [0/400000 (0%)]\tLoss: 1.025172\n","Train Epoch: 8 [6400/400000 (2%)]\tLoss: 1.024716\n","Train Epoch: 8 [12800/400000 (3%)]\tLoss: 1.023837\n","Train Epoch: 8 [19200/400000 (5%)]\tLoss: 1.021062\n","Train Epoch: 8 [25600/400000 (6%)]\tLoss: 1.050942\n","Train Epoch: 8 [32000/400000 (8%)]\tLoss: 1.017393\n","Train Epoch: 8 [38400/400000 (10%)]\tLoss: 0.981274\n","Train Epoch: 8 [44800/400000 (11%)]\tLoss: 1.018747\n","Train Epoch: 8 [51200/400000 (13%)]\tLoss: 0.994549\n","Train Epoch: 8 [57600/400000 (14%)]\tLoss: 1.012818\n","Train Epoch: 8 [64000/400000 (16%)]\tLoss: 1.026237\n","Train Epoch: 8 [70400/400000 (18%)]\tLoss: 0.987303\n","Train Epoch: 8 [76800/400000 (19%)]\tLoss: 1.014737\n","Train Epoch: 8 [83200/400000 (21%)]\tLoss: 1.063451\n","Train Epoch: 8 [89600/400000 (22%)]\tLoss: 0.998702\n","Train Epoch: 8 [96000/400000 (24%)]\tLoss: 1.065259\n","Train Epoch: 8 [102400/400000 (26%)]\tLoss: 1.009152\n","Train Epoch: 8 [108800/400000 (27%)]\tLoss: 1.033382\n","Train Epoch: 8 [115200/400000 (29%)]\tLoss: 1.044795\n","Train Epoch: 8 [121600/400000 (30%)]\tLoss: 1.030081\n","Train Epoch: 8 [128000/400000 (32%)]\tLoss: 1.001283\n","Train Epoch: 8 [134400/400000 (34%)]\tLoss: 1.040842\n","Train Epoch: 8 [140800/400000 (35%)]\tLoss: 1.028689\n","Train Epoch: 8 [147200/400000 (37%)]\tLoss: 1.024832\n","Train Epoch: 8 [153600/400000 (38%)]\tLoss: 1.043055\n","Train Epoch: 8 [160000/400000 (40%)]\tLoss: 0.969566\n","Train Epoch: 8 [166400/400000 (42%)]\tLoss: 1.021899\n","Train Epoch: 8 [172800/400000 (43%)]\tLoss: 0.993222\n","Train Epoch: 8 [179200/400000 (45%)]\tLoss: 1.040920\n","Train Epoch: 8 [185600/400000 (46%)]\tLoss: 1.012299\n","Train Epoch: 8 [192000/400000 (48%)]\tLoss: 1.070894\n","Train Epoch: 8 [198400/400000 (50%)]\tLoss: 1.115033\n","Train Epoch: 8 [204800/400000 (51%)]\tLoss: 0.953187\n","Train Epoch: 8 [211200/400000 (53%)]\tLoss: 1.076640\n","Train Epoch: 8 [217600/400000 (54%)]\tLoss: 1.064969\n","Train Epoch: 8 [224000/400000 (56%)]\tLoss: 1.015624\n","Train Epoch: 8 [230400/400000 (58%)]\tLoss: 1.028933\n","Train Epoch: 8 [236800/400000 (59%)]\tLoss: 1.000474\n","Train Epoch: 8 [243200/400000 (61%)]\tLoss: 0.948942\n","Train Epoch: 8 [249600/400000 (62%)]\tLoss: 1.013458\n","Train Epoch: 8 [256000/400000 (64%)]\tLoss: 0.982645\n","Train Epoch: 8 [262400/400000 (66%)]\tLoss: 1.016239\n","Train Epoch: 8 [268800/400000 (67%)]\tLoss: 1.015228\n","Train Epoch: 8 [275200/400000 (69%)]\tLoss: 0.981143\n","Train Epoch: 8 [281600/400000 (70%)]\tLoss: 1.005643\n","Train Epoch: 8 [288000/400000 (72%)]\tLoss: 1.010283\n","Train Epoch: 8 [294400/400000 (74%)]\tLoss: 0.986395\n","Train Epoch: 8 [300800/400000 (75%)]\tLoss: 1.032677\n","Train Epoch: 8 [307200/400000 (77%)]\tLoss: 1.045914\n","Train Epoch: 8 [313600/400000 (78%)]\tLoss: 1.027472\n","Train Epoch: 8 [320000/400000 (80%)]\tLoss: 0.988808\n","Train Epoch: 8 [326400/400000 (82%)]\tLoss: 1.059366\n","Train Epoch: 8 [332800/400000 (83%)]\tLoss: 0.995580\n","Train Epoch: 8 [339200/400000 (85%)]\tLoss: 1.043815\n","Train Epoch: 8 [345600/400000 (86%)]\tLoss: 0.988327\n","Train Epoch: 8 [352000/400000 (88%)]\tLoss: 1.064208\n","Train Epoch: 8 [358400/400000 (90%)]\tLoss: 0.985886\n","Train Epoch: 8 [364800/400000 (91%)]\tLoss: 1.006079\n","Train Epoch: 8 [371200/400000 (93%)]\tLoss: 0.981651\n","Train Epoch: 8 [377600/400000 (94%)]\tLoss: 1.033834\n","Train Epoch: 8 [384000/400000 (96%)]\tLoss: 1.042914\n","Train Epoch: 8 [390400/400000 (98%)]\tLoss: 1.029588\n","Train Epoch: 8 [396800/400000 (99%)]\tLoss: 1.061803\n","\n","Test set: Average loss: 0.0000, Accuracy: 54941/100000 (55%)\n","\n","Train Epoch: 9 [0/400000 (0%)]\tLoss: 1.002083\n","Train Epoch: 9 [6400/400000 (2%)]\tLoss: 0.971338\n","Train Epoch: 9 [12800/400000 (3%)]\tLoss: 1.022855\n","Train Epoch: 9 [19200/400000 (5%)]\tLoss: 1.020958\n","Train Epoch: 9 [25600/400000 (6%)]\tLoss: 1.002174\n","Train Epoch: 9 [32000/400000 (8%)]\tLoss: 1.021334\n","Train Epoch: 9 [38400/400000 (10%)]\tLoss: 1.026904\n","Train Epoch: 9 [44800/400000 (11%)]\tLoss: 1.017064\n","Train Epoch: 9 [51200/400000 (13%)]\tLoss: 1.052672\n","Train Epoch: 9 [57600/400000 (14%)]\tLoss: 1.011582\n","Train Epoch: 9 [64000/400000 (16%)]\tLoss: 1.005553\n","Train Epoch: 9 [70400/400000 (18%)]\tLoss: 1.005079\n","Train Epoch: 9 [76800/400000 (19%)]\tLoss: 0.980389\n","Train Epoch: 9 [83200/400000 (21%)]\tLoss: 1.002792\n","Train Epoch: 9 [89600/400000 (22%)]\tLoss: 1.000068\n","Train Epoch: 9 [96000/400000 (24%)]\tLoss: 1.082420\n","Train Epoch: 9 [102400/400000 (26%)]\tLoss: 1.021212\n","Train Epoch: 9 [108800/400000 (27%)]\tLoss: 1.006099\n","Train Epoch: 9 [115200/400000 (29%)]\tLoss: 1.084231\n","Train Epoch: 9 [121600/400000 (30%)]\tLoss: 1.003758\n","Train Epoch: 9 [128000/400000 (32%)]\tLoss: 1.043266\n","Train Epoch: 9 [134400/400000 (34%)]\tLoss: 1.014398\n","Train Epoch: 9 [140800/400000 (35%)]\tLoss: 0.978099\n","Train Epoch: 9 [147200/400000 (37%)]\tLoss: 1.007901\n","Train Epoch: 9 [153600/400000 (38%)]\tLoss: 1.047260\n","Train Epoch: 9 [160000/400000 (40%)]\tLoss: 0.977070\n","Train Epoch: 9 [166400/400000 (42%)]\tLoss: 0.978552\n","Train Epoch: 9 [172800/400000 (43%)]\tLoss: 0.994211\n","Train Epoch: 9 [179200/400000 (45%)]\tLoss: 1.027789\n","Train Epoch: 9 [185600/400000 (46%)]\tLoss: 1.037704\n","Train Epoch: 9 [192000/400000 (48%)]\tLoss: 1.024169\n","Train Epoch: 9 [198400/400000 (50%)]\tLoss: 1.006944\n","Train Epoch: 9 [204800/400000 (51%)]\tLoss: 1.071659\n","Train Epoch: 9 [211200/400000 (53%)]\tLoss: 0.993481\n","Train Epoch: 9 [217600/400000 (54%)]\tLoss: 1.022966\n","Train Epoch: 9 [224000/400000 (56%)]\tLoss: 0.989525\n","Train Epoch: 9 [230400/400000 (58%)]\tLoss: 1.038923\n","Train Epoch: 9 [236800/400000 (59%)]\tLoss: 0.980762\n","Train Epoch: 9 [243200/400000 (61%)]\tLoss: 1.067190\n","Train Epoch: 9 [249600/400000 (62%)]\tLoss: 1.001683\n","Train Epoch: 9 [256000/400000 (64%)]\tLoss: 0.993761\n","Train Epoch: 9 [262400/400000 (66%)]\tLoss: 0.968205\n","Train Epoch: 9 [268800/400000 (67%)]\tLoss: 1.057040\n","Train Epoch: 9 [275200/400000 (69%)]\tLoss: 1.026052\n","Train Epoch: 9 [281600/400000 (70%)]\tLoss: 1.018154\n","Train Epoch: 9 [288000/400000 (72%)]\tLoss: 0.989886\n","Train Epoch: 9 [294400/400000 (74%)]\tLoss: 1.036927\n","Train Epoch: 9 [300800/400000 (75%)]\tLoss: 0.995870\n","Train Epoch: 9 [307200/400000 (77%)]\tLoss: 1.022356\n","Train Epoch: 9 [313600/400000 (78%)]\tLoss: 1.026150\n","Train Epoch: 9 [320000/400000 (80%)]\tLoss: 1.007606\n","Train Epoch: 9 [326400/400000 (82%)]\tLoss: 0.988958\n","Train Epoch: 9 [332800/400000 (83%)]\tLoss: 0.999596\n","Train Epoch: 9 [339200/400000 (85%)]\tLoss: 0.991655\n","Train Epoch: 9 [345600/400000 (86%)]\tLoss: 1.045688\n","Train Epoch: 9 [352000/400000 (88%)]\tLoss: 1.028878\n","Train Epoch: 9 [358400/400000 (90%)]\tLoss: 0.994182\n","Train Epoch: 9 [364800/400000 (91%)]\tLoss: 0.973598\n","Train Epoch: 9 [371200/400000 (93%)]\tLoss: 1.060266\n","Train Epoch: 9 [377600/400000 (94%)]\tLoss: 0.989641\n","Train Epoch: 9 [384000/400000 (96%)]\tLoss: 1.023386\n","Train Epoch: 9 [390400/400000 (98%)]\tLoss: 1.019415\n","Train Epoch: 9 [396800/400000 (99%)]\tLoss: 1.062855\n","\n","Test set: Average loss: 0.0000, Accuracy: 54965/100000 (55%)\n","\n","Train Epoch: 10 [0/400000 (0%)]\tLoss: 1.077303\n","Train Epoch: 10 [6400/400000 (2%)]\tLoss: 1.013705\n","Train Epoch: 10 [12800/400000 (3%)]\tLoss: 1.011003\n","Train Epoch: 10 [19200/400000 (5%)]\tLoss: 1.022175\n","Train Epoch: 10 [25600/400000 (6%)]\tLoss: 1.081615\n","Train Epoch: 10 [32000/400000 (8%)]\tLoss: 0.986121\n","Train Epoch: 10 [38400/400000 (10%)]\tLoss: 1.009660\n","Train Epoch: 10 [44800/400000 (11%)]\tLoss: 1.019004\n","Train Epoch: 10 [51200/400000 (13%)]\tLoss: 0.940080\n","Train Epoch: 10 [57600/400000 (14%)]\tLoss: 0.994462\n","Train Epoch: 10 [64000/400000 (16%)]\tLoss: 1.014355\n","Train Epoch: 10 [70400/400000 (18%)]\tLoss: 0.981935\n","Train Epoch: 10 [76800/400000 (19%)]\tLoss: 1.025281\n","Train Epoch: 10 [83200/400000 (21%)]\tLoss: 1.021852\n","Train Epoch: 10 [89600/400000 (22%)]\tLoss: 1.049487\n","Train Epoch: 10 [96000/400000 (24%)]\tLoss: 0.980111\n","Train Epoch: 10 [102400/400000 (26%)]\tLoss: 0.978901\n","Train Epoch: 10 [108800/400000 (27%)]\tLoss: 1.053250\n","Train Epoch: 10 [115200/400000 (29%)]\tLoss: 1.023358\n","Train Epoch: 10 [121600/400000 (30%)]\tLoss: 0.952148\n","Train Epoch: 10 [128000/400000 (32%)]\tLoss: 1.005733\n","Train Epoch: 10 [134400/400000 (34%)]\tLoss: 1.001746\n","Train Epoch: 10 [140800/400000 (35%)]\tLoss: 1.037943\n","Train Epoch: 10 [147200/400000 (37%)]\tLoss: 1.003593\n","Train Epoch: 10 [153600/400000 (38%)]\tLoss: 0.987760\n","Train Epoch: 10 [160000/400000 (40%)]\tLoss: 1.021902\n","Train Epoch: 10 [166400/400000 (42%)]\tLoss: 1.011630\n","Train Epoch: 10 [172800/400000 (43%)]\tLoss: 1.019035\n","Train Epoch: 10 [179200/400000 (45%)]\tLoss: 1.027600\n","Train Epoch: 10 [185600/400000 (46%)]\tLoss: 1.041698\n","Train Epoch: 10 [192000/400000 (48%)]\tLoss: 1.040530\n","Train Epoch: 10 [198400/400000 (50%)]\tLoss: 1.029330\n","Train Epoch: 10 [204800/400000 (51%)]\tLoss: 1.020364\n","Train Epoch: 10 [211200/400000 (53%)]\tLoss: 0.964301\n","Train Epoch: 10 [217600/400000 (54%)]\tLoss: 0.985743\n","Train Epoch: 10 [224000/400000 (56%)]\tLoss: 0.980372\n","Train Epoch: 10 [230400/400000 (58%)]\tLoss: 1.042632\n","Train Epoch: 10 [236800/400000 (59%)]\tLoss: 0.990846\n","Train Epoch: 10 [243200/400000 (61%)]\tLoss: 1.023638\n","Train Epoch: 10 [249600/400000 (62%)]\tLoss: 1.055242\n","Train Epoch: 10 [256000/400000 (64%)]\tLoss: 1.081629\n","Train Epoch: 10 [262400/400000 (66%)]\tLoss: 0.994359\n","Train Epoch: 10 [268800/400000 (67%)]\tLoss: 1.003181\n","Train Epoch: 10 [275200/400000 (69%)]\tLoss: 1.071030\n","Train Epoch: 10 [281600/400000 (70%)]\tLoss: 1.001169\n","Train Epoch: 10 [288000/400000 (72%)]\tLoss: 1.001974\n","Train Epoch: 10 [294400/400000 (74%)]\tLoss: 0.984838\n","Train Epoch: 10 [300800/400000 (75%)]\tLoss: 1.017157\n","Train Epoch: 10 [307200/400000 (77%)]\tLoss: 1.026974\n","Train Epoch: 10 [313600/400000 (78%)]\tLoss: 1.032060\n","Train Epoch: 10 [320000/400000 (80%)]\tLoss: 0.990404\n","Train Epoch: 10 [326400/400000 (82%)]\tLoss: 1.037490\n","Train Epoch: 10 [332800/400000 (83%)]\tLoss: 1.053768\n","Train Epoch: 10 [339200/400000 (85%)]\tLoss: 0.992875\n","Train Epoch: 10 [345600/400000 (86%)]\tLoss: 0.991242\n","Train Epoch: 10 [352000/400000 (88%)]\tLoss: 1.014261\n","Train Epoch: 10 [358400/400000 (90%)]\tLoss: 1.014361\n","Train Epoch: 10 [364800/400000 (91%)]\tLoss: 0.983040\n","Train Epoch: 10 [371200/400000 (93%)]\tLoss: 1.000412\n","Train Epoch: 10 [377600/400000 (94%)]\tLoss: 1.058532\n","Train Epoch: 10 [384000/400000 (96%)]\tLoss: 1.045612\n","Train Epoch: 10 [390400/400000 (98%)]\tLoss: 1.024437\n","Train Epoch: 10 [396800/400000 (99%)]\tLoss: 1.000924\n","\n","Test set: Average loss: 0.0000, Accuracy: 55034/100000 (55%)\n","\n","55034\n"]}],"source":["torch.manual_seed(args.seed)\n","\n","device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n","model = Net().to(device)\n","\n","for param_tensor in model.state_dict():\n","        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n","\n","#Form training and testing dataset\n","optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n","\n","train_dataset = torch.utils.data.TensorDataset(train_vectors, train_labels)\n","test_dataset = torch.utils.data.TensorDataset(test_vectors, test_labels)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=640, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=640, shuffle=False)\n","scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n","\n","#Model training\n","ACC = 0\n","for epoch in range(1, args.epochs + 1):\n","    train(args, model, device, train_loader, optimizer, epoch)\n","    ACC_ = test(model, device, test_loader)\n","    if ACC_>ACC or ACC_ == ACC:\n","        ACC = ACC_\n","        torch.save(model.state_dict(), \"cnn_lstm_att.pt\")\n","\n","    scheduler.step()\n","\n","print(ACC)\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1GmsHCEwS-N86FrQnez-JTYG1GN20xJPl","timestamp":1691995094506}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}